{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version is [1.4.1].\n"
     ]
    }
   ],
   "source": [
    "import nbloader,os,warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from sklearn.utils import shuffle\n",
    "from util import gpusession,create_gradient_clipping,load_mnist_with_noise,print_n_txt,mixup\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "if __name__ == \"__main__\":\n",
    "    print (\"TensorFlow version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_cls_class defined.\n"
     ]
    }
   ],
   "source": [
    "class cnn_cls_class(object):\n",
    "    def __init__(self,_name='basic_cnn',_xdim=[28,28,1],_ydim=10,_hdims=[64,64],_filterSizes=[3,3],_max_pools=[2,2]\n",
    "                 ,_feat_dim=128,_actv=tf.nn.relu,_bn=slim.batch_norm\n",
    "                 ,_l2_reg_coef=1e-5\n",
    "                 ,_momentum = 0.5\n",
    "                 ,_USE_INPUT_BN=False,_USE_RESNET=False,_USE_GAP=False,_USE_SGD=False\n",
    "                 ,_USE_MIXUP=False\n",
    "                 ,_GPU_ID=0,_VERBOSE=True):\n",
    "        self.name = _name \n",
    "        self.xdim = _xdim\n",
    "        self.ydim = _ydim\n",
    "        self.hdims = _hdims\n",
    "        self.filterSizes = _filterSizes\n",
    "        self.max_pools = _max_pools\n",
    "        self.feat_dim = _feat_dim\n",
    "        self.actv = _actv\n",
    "        self.bn = _bn\n",
    "        self.l2_reg_coef = _l2_reg_coef\n",
    "        self.momentum = _momentum\n",
    "        self.USE_INPUT_BN = _USE_INPUT_BN\n",
    "        self.USE_RESNET = _USE_RESNET\n",
    "        self.USE_GAP = _USE_GAP\n",
    "        self.USE_SGD = _USE_SGD\n",
    "        self.USE_MIXUP = _USE_MIXUP\n",
    "        self.GPU_ID = (int)(_GPU_ID)\n",
    "        self.VERBOSE = _VERBOSE\n",
    "        with tf.device('/device:GPU:%d'%(self.GPU_ID)):\n",
    "            # Build model\n",
    "            self.build_model()\n",
    "            # Build graph\n",
    "            self.build_graph()\n",
    "            # Check parameters\n",
    "            self.check_params()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # Set placeholders\n",
    "        _xdim = self.xdim[0]*self.xdim[1]*self.xdim[2] # Total dimension\n",
    "        self.x = tf.placeholder(dtype=tf.float32,shape=[None,_xdim]) # Input [N x xdim]\n",
    "        self.t = tf.placeholder(dtype=tf.float32,shape=[None,self.ydim]) # Output [N x D]\n",
    "        self.kp = tf.placeholder(dtype=tf.float32,shape=[]) # []\n",
    "        self.is_training = tf.placeholder(dtype=tf.bool,shape=[]) # []\n",
    "        self.lr = tf.placeholder(dtype=tf.float32,shape=[]) # []\n",
    "        self.bn_init     = {'beta': tf.constant_initializer(0.),\n",
    "                           'gamma': tf.random_normal_initializer(1., 0.01)}\n",
    "        batch_norm_params = {'is_training':self.is_training,'decay':0.9,'updates_collections': None}\n",
    "        \n",
    "        with tf.variable_scope(self.name,reuse=False) as scope:\n",
    "            \n",
    "            # List of features\n",
    "            self.layers = []\n",
    "            self.layers.append(self.x)\n",
    "\n",
    "            # Reshape input \n",
    "            _net = tf.reshape(self.x,[-1]+self.xdim) \n",
    "            self.layers.append(_net) \n",
    "            \n",
    "            # Input normalization \n",
    "            if self.USE_INPUT_BN:\n",
    "                _net = slim.batch_norm(_net,param_initializers=self.bn_init,is_training=self.is_training,updates_collections=None)\n",
    "                \n",
    "            # Convolution layers\n",
    "            for hidx,hdim in enumerate(self.hdims):\n",
    "                fs = self.filterSizes[hidx]\n",
    "                if self.USE_RESNET: # Use residual connection \n",
    "                    cChannelSize = _net.get_shape()[3] # Current channel size\n",
    "                    if cChannelSize == hdim:\n",
    "                        _identity = _net\n",
    "                    else: # Expand dimension if required \n",
    "                        _identity = slim.conv2d(_net,hdim,[1,1],padding='SAME',activation_fn=None\n",
    "                                              , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                              , normalizer_fn       = self.bn\n",
    "                                              , normalizer_params   = batch_norm_params\n",
    "                                              , scope='identity_%d'%(hidx))\n",
    "                    # First conv\n",
    "                    _net = slim.conv2d(_net,hdim,[fs,fs],padding='SAME'\n",
    "                                     , activation_fn       = None\n",
    "                                     , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                     , normalizer_fn       = self.bn\n",
    "                                     , normalizer_params   = batch_norm_params\n",
    "                                     , scope='res_a_%d'%(hidx))\n",
    "                    # Relu\n",
    "                    _net = self.actv(_net)\n",
    "                    self.layers.append(_net) # Append to list\n",
    "                    # Second conv\n",
    "                    _net = slim.conv2d(_net,hdim,[fs,fs],padding='SAME'\n",
    "                                     , activation_fn       = None\n",
    "                                     , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                     , normalizer_fn       = self.bn\n",
    "                                     , normalizer_params   = batch_norm_params\n",
    "                                     , scope='res_b_%d'%(hidx))\n",
    "                    # Skip connection\n",
    "                    _net = _net + _identity\n",
    "                    # Relu\n",
    "                    _net = self.actv(_net)\n",
    "                    self.layers.append(_net) # Append to list\n",
    "                else:\n",
    "                    _net = slim.conv2d(_net,hdim,[fs,fs],padding='SAME'\n",
    "                                     , activation_fn       = self.actv\n",
    "                                     , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                     , normalizer_fn       = self.bn\n",
    "                                     , normalizer_params   = batch_norm_params\n",
    "                                     , scope='conv_%d'%(hidx))\n",
    "                    self.layers.append(_net) # Append to list\n",
    "                # Max pooling (if required)\n",
    "                max_pool = self.max_pools[hidx]\n",
    "                if max_pool > 1:\n",
    "                    _net = slim.max_pool2d(_net,[max_pool,max_pool],scope='pool_%d'%(hidx))\n",
    "                    self.layers.append(_net) # Append to list\n",
    "                \n",
    "            # Global average pooling \n",
    "            if self.USE_GAP: \n",
    "                _net = tf.reduce_mean(_net,[1,2])\n",
    "                self.layers.append(_net) # Append to list\n",
    "                # Feature\n",
    "                self.feat = _net # [N x Q]\n",
    "            else:\n",
    "                # Flatten and output\n",
    "                _net = slim.flatten(_net, scope='flatten')\n",
    "                self.layers.append(_net) # Append to list\n",
    "                # Dense\n",
    "                _net = slim.fully_connected(_net,self.feat_dim,scope='fc')\n",
    "                self.layers.append(_net) # Append to list\n",
    "                # Feature\n",
    "                self.feat = _net # [N x Q]\n",
    "            \n",
    "            # Dropout at the last layer \n",
    "            _net = slim.dropout(_net, keep_prob=self.kp,is_training=self.is_training,scope='dropout')  \n",
    "            _out = slim.fully_connected(_net,self.ydim,activation_fn=None,normalizer_fn=None, scope='out')# [N x D]\n",
    "            self.layers.append(_out) # Append to list\n",
    "            self.out = _out\n",
    "            \n",
    "    # Build graph\n",
    "    def build_graph(self):\n",
    "        # Cross-entropy loss\n",
    "        self._loss_ce = tf.nn.softmax_cross_entropy_with_logits(labels=self.t,logits=self.out) # [N]\n",
    "        self.loss_ce = tf.reduce_mean(self._loss_ce) # []\n",
    "        # Weight decay regularizer\n",
    "        _g_vars = tf.global_variables()\n",
    "        _c_vars = [var for var in _g_vars if '%s/'%(self.name) in var.name]\n",
    "        self.l2_reg = self.l2_reg_coef*tf.reduce_sum(tf.stack([tf.nn.l2_loss(v) for v in _c_vars])) # []\n",
    "        # Total loss\n",
    "        self.loss_total = self.loss_ce + self.l2_reg\n",
    "        if self.USE_SGD:\n",
    "            # self.optm = tf.train.GradientDescentOptimizer(learning_rate=self.lr).minimize(self.loss_total)\n",
    "            self.optm = tf.train.MomentumOptimizer(learning_rate=self.lr,momentum=self.momentum).minimize(self.loss_total)\n",
    "        else:\n",
    "            self.optm = tf.train.AdamOptimizer(learning_rate=self.lr\n",
    "                                               ,beta1=0.9,beta2=0.999,epsilon=1e-6).minimize(self.loss_total)\n",
    "        # Accuracy\n",
    "        _corr = tf.equal(tf.argmax(self.out, 1), tf.argmax(self.t, 1))    \n",
    "        self.accr = tf.reduce_mean(tf.cast(_corr,tf.float32)) \n",
    "        \n",
    "    # Check parameters\n",
    "    def check_params(self):\n",
    "        _g_vars = tf.global_variables()\n",
    "        self.g_vars = [var for var in _g_vars if '%s/'%(self.name) in var.name]\n",
    "        if self.VERBOSE:\n",
    "            print (\"==== Global Variables ====\")\n",
    "        for i in range(len(self.g_vars)):\n",
    "            w_name  = self.g_vars[i].name \n",
    "            w_shape = self.g_vars[i].get_shape().as_list()\n",
    "            if self.VERBOSE:\n",
    "                print (\" [%02d] Name:[%s] Shape:[%s]\" % (i,w_name,w_shape))\n",
    "        # Print layers\n",
    "        if self.VERBOSE:\n",
    "            print (\"====== Layers ======\")\n",
    "            nLayers = len(self.layers)\n",
    "            for i in range(nLayers):\n",
    "                print (\"[%02d/%d] %s %s\"%(i,nLayers,self.layers[i].name,self.layers[i].shape))\n",
    "    \n",
    "    # Saver\n",
    "    def save(self,_sess,_savename=None):\n",
    "        if _savename==None:\n",
    "            _savename='../net/net_%s.npz'%(self.name)\n",
    "        # Get global variables \n",
    "        self.g_wnames,self.g_wvals,self.g_wshapes = [],[],[]\n",
    "        for i in range(len(self.g_vars)):\n",
    "            curr_wname = self.g_vars[i].name\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==curr_wname][0]\n",
    "            curr_wval  = _sess.run(curr_wvar)\n",
    "            \n",
    "            curr_wval_sqz = curr_wval\n",
    "            # curr_wval_sqz  = curr_wval.squeeze() # ???\n",
    "            curr_wval_sqz = np.asanyarray(curr_wval_sqz,order=(1,-1))\n",
    "            \n",
    "            self.g_wnames.append(curr_wname)\n",
    "            self.g_wvals.append(curr_wval_sqz)\n",
    "            self.g_wshapes.append(curr_wval.shape)\n",
    "        # Save \n",
    "        np.savez(_savename,g_wnames=self.g_wnames,g_wvals=self.g_wvals,g_wshapes=self.g_wshapes)\n",
    "        if self.VERBOSE: \n",
    "            print (\"[%s] Saved. Size is [%.4f]MB\" % \n",
    "                   (_savename,os.path.getsize(_savename)/1000./1000.))\n",
    "    \n",
    "    # Restore \n",
    "    def restore(self,_sess,_loadname=None):\n",
    "        if _loadname==None:\n",
    "            _loadname='../net/net_%s.npz'%(self.name)\n",
    "        l = np.load(_loadname)\n",
    "        g_wnames = l['g_wnames']\n",
    "        g_wvals  = l['g_wvals']\n",
    "        g_wshapes = l['g_wshapes']\n",
    "        for widx,wname in enumerate(g_wnames):\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==wname][0]\n",
    "            _sess.run(tf.assign(curr_wvar,g_wvals[widx].reshape(g_wshapes[widx])))\n",
    "        if self.VERBOSE:\n",
    "            print (\"Weight restored from [%s] Size is [%.4f]MB\" % \n",
    "                   (_loadname,os.path.getsize(_loadname)/1000./1000.))\n",
    "    \n",
    "    # Train \n",
    "    def train(self,_sess,_trainimg,_trainlabel,_testimg,_testlabel,_valimg,_vallabel\n",
    "              ,_maxEpoch=10,_batchSize=256,_lr=1e-3,_kp=0.9\n",
    "              ,_LR_SCHEDULE=False,_PRINT_EVERY=10,_SAVE_BEST=True,_DO_AUGMENTATION=False,_VERBOSE_TRAIN=True):\n",
    "        tf.set_random_seed(0)\n",
    "        nTrain,nVal,nTest = _trainimg.shape[0],_valimg.shape[0],_testimg.shape[0]\n",
    "        txtName = ('../res/res_%s.txt'%(self.name))\n",
    "        f = open(txtName,'w') # Open txt file\n",
    "        print_n_txt(_f=f,_chars='Text name: '+txtName)\n",
    "        print_period=max(1,_maxEpoch//_PRINT_EVERY)\n",
    "        maxIter,maxValAccr,maxTestAccr = max(nTrain//_batchSize,1),0.0,0.0\n",
    "        for epoch in range(_maxEpoch+1): # For every epoch \n",
    "            _trainimg,_trainlabel = shuffle(_trainimg,_trainlabel) \n",
    "            for iter in range(maxIter): # For every iteration in one epoch\n",
    "                start,end = iter*_batchSize,(iter+1)*_batchSize\n",
    "                # Learning rate scheduling\n",
    "                if _LR_SCHEDULE:\n",
    "                    if epoch < 0.5*_maxEpoch:\n",
    "                        _lr_use = _lr\n",
    "                    elif epoch < 0.75*_maxEpoch:\n",
    "                        _lr_use = _lr/10.0\n",
    "                    else:\n",
    "                        _lr_use = _lr/100.0\n",
    "                else:\n",
    "                    _lr_use = _lr\n",
    "                if _DO_AUGMENTATION:\n",
    "                    trainImgBatch = augment_img(_trainimg[start:end,:],self.xdim) \n",
    "                else:\n",
    "                    trainImgBatch = _trainimg[start:end,:]\n",
    "                if self.USE_MIXUP:\n",
    "                    xBatch = trainImgBatch\n",
    "                    tBatch = _trainlabel[start:end,:]\n",
    "                    xBatch,tBatch = mixup(xBatch,tBatch,32)\n",
    "                else:\n",
    "                    xBatch = trainImgBatch\n",
    "                    tBatch = _trainlabel[start:end,:]\n",
    "                feeds = {self.x:xBatch,self.t:tBatch\n",
    "                         ,self.kp:_kp,self.lr:_lr_use,self.is_training:True}\n",
    "                _sess.run(self.optm,feed_dict=feeds)\n",
    "            # Print training losses, training accuracy, validation accuracy, and test accuracy\n",
    "            if (epoch%print_period)==0 or (epoch==(_maxEpoch)):\n",
    "                batchSize4print = 512 \n",
    "                # Compute train loss and accuracy\n",
    "                maxIter4print = max(nTrain//batchSize4print,1)\n",
    "                trainLoss,trainAccr,nTemp = 0,0,0\n",
    "                for iter in range(maxIter4print):\n",
    "                    start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "                    feeds_train = {self.x:_trainimg[start:end,:],self.t:_trainlabel[start:end,:]\n",
    "                             ,self.kp:1.0,self.is_training:False}\n",
    "                    _trainLoss,_trainAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_train) \n",
    "                    _nTemp = end-start; nTemp+=_nTemp\n",
    "                    trainLoss+=(_nTemp*_trainLoss); trainAccr+=(_nTemp*_trainAccr)\n",
    "                trainLoss/=nTemp;trainAccr/=nTemp\n",
    "                # Compute validation loss and accuracy\n",
    "                maxIter4print = max(nVal//batchSize4print,1)\n",
    "                valLoss,valAccr,nTemp = 0,0,0\n",
    "                for iter in range(maxIter4print):\n",
    "                    start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "                    feeds_val = {self.x:_valimg[start:end,:],self.t:_vallabel[start:end,:]\n",
    "                             ,self.kp:1.0,self.is_training:False}\n",
    "                    _valLoss,_valAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_val) \n",
    "                    _nTemp = end-start; nTemp+=_nTemp\n",
    "                    valLoss+=(_nTemp*_valLoss); valAccr+=(_nTemp*_valAccr)\n",
    "                valLoss/=nTemp;valAccr/=nTemp\n",
    "                # Compute test loss and accuracy\n",
    "                maxIter4print = max(nTest//batchSize4print,1)\n",
    "                testLoss,testAccr,nTemp = 0,0,0\n",
    "                for iter in range(maxIter4print):\n",
    "                    start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "                    feeds_test = {self.x:_testimg[start:end,:],self.t:_testlabel[start:end,:]\n",
    "                             ,self.kp:1.0,self.is_training:False}\n",
    "                    _testLoss,_testAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_test) \n",
    "                    _nTemp = end-start; nTemp+=_nTemp\n",
    "                    testLoss+=(_nTemp*_testLoss); testAccr+=(_nTemp*_testAccr)\n",
    "                testLoss/=nTemp;testAccr/=nTemp\n",
    "                # Compute max val accr\n",
    "                if valAccr > maxValAccr:\n",
    "                    maxValAccr = valAccr\n",
    "                    maxTestAccr = testAccr\n",
    "                    if _SAVE_BEST: self.save(_sess) \n",
    "                strTemp = ((\"[%02d/%d] [Loss] train:%.3f val:%.3f test:%.3f\"\n",
    "                            +\" [Accr] train:%.1f%% val:%.1f%% test:%.1f%% maxVal:%.1f%% maxTest:%.1f%%\")\n",
    "                       %(epoch,_maxEpoch,trainLoss,valLoss,testLoss\n",
    "                         ,trainAccr*100,valAccr*100,testAccr*100,maxValAccr*100,maxTestAccr*100))\n",
    "                print_n_txt(_f=f,_chars=strTemp,_DO_PRINT=_VERBOSE_TRAIN)\n",
    "        # Done \n",
    "        print (\"Training finished.\")\n",
    "    \n",
    "    # Test\n",
    "    def test(self,_sess,_trainimg,_trainlabel,_testimg,_testlabel,_valimg,_vallabel):\n",
    "        nTrain,nVal,nTest = _trainimg.shape[0],_valimg.shape[0],_testimg.shape[0]\n",
    "        # Check accuracies (train, val, and test)\n",
    "        batchSize4print = 512 \n",
    "        # Compute train loss and accuracy\n",
    "        maxIter4print = max(nTrain//batchSize4print,1)\n",
    "        trainLoss,trainAccr,nTemp = 0,0,0\n",
    "        for iter in range(maxIter4print):\n",
    "            start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "            feeds_train = {self.x:_trainimg[start:end,:],self.t:_trainlabel[start:end,:]\n",
    "                     ,self.kp:1.0,self.is_training:False}\n",
    "            _trainLoss,_trainAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_train) \n",
    "            _nTemp = end-start; nTemp+=_nTemp\n",
    "            trainLoss+=(_nTemp*_trainLoss); trainAccr+=(_nTemp*_trainAccr)\n",
    "        trainLoss/=nTemp;trainAccr/=nTemp\n",
    "        # Compute validation loss and accuracy\n",
    "        maxIter4print = max(nVal//batchSize4print,1)\n",
    "        valLoss,valAccr,nTemp = 0,0,0\n",
    "        for iter in range(maxIter4print):\n",
    "            start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "            feeds_val = {self.x:_valimg[start:end,:],self.t:_vallabel[start:end,:]\n",
    "                     ,self.kp:1.0,self.is_training:False}\n",
    "            _valLoss,_valAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_val) \n",
    "            _nTemp = end-start; nTemp+=_nTemp\n",
    "            valLoss+=(_nTemp*_valLoss); valAccr+=(_nTemp*_valAccr)\n",
    "        valLoss/=nTemp;valAccr/=nTemp\n",
    "        # Compute test loss and accuracy\n",
    "        maxIter4print = max(nTest//batchSize4print,1)\n",
    "        testLoss,testAccr,nTemp = 0,0,0\n",
    "        for iter in range(maxIter4print):\n",
    "            start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "            feeds_test = {self.x:_testimg[start:end,:],self.t:_testlabel[start:end,:]\n",
    "                     ,self.kp:1.0,self.is_training:False}\n",
    "            _testLoss,_testAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_test) \n",
    "            _nTemp = end-start; nTemp+=_nTemp\n",
    "            testLoss+=(_nTemp*_testLoss); testAccr+=(_nTemp*_testAccr)\n",
    "        testLoss/=nTemp;testAccr/=nTemp\n",
    "        strTemp = ((\"[%s] [Loss] train:%.3f val:%.3f test:%.3f\"\n",
    "                    +\" [Accr] train:%.3f%% val:%.3f%% test:%.3f%%\")\n",
    "               %(self.name,trainLoss,valLoss,testLoss,trainAccr*100,valAccr*100,testAccr*100))\n",
    "        print(strTemp)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print (\"cnn_cls_class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_config():\n",
    "    trainimg,trainlabel,testimg,testlabel,valimg,vallabel \\\n",
    "        = load_mnist_with_noise(_errType='rp',_outlierRatio=0.9,_seed=0)\n",
    "    xdim,ydim,hdims,filterSizes,max_pools,feat_dim = [28,28,1],10,[64,64],[3,3],[2,2],256\n",
    "    actv,bn,VERBOSE = tf.nn.relu,slim.batch_norm,True \n",
    "    USE_INPUT_BN,USE_RESNET,USE_GAP,USE_MIXUP = False,True,False,False\n",
    "    return trainimg,trainlabel,testimg,testlabel,valimg,vallabel \\\n",
    "        ,xdim,ydim,hdims,filterSizes,max_pools,feat_dim \\\n",
    "        ,actv,bn,VERBOSE \\\n",
    "        ,USE_INPUT_BN,USE_RESNET,USE_GAP,USE_MIXUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "==== Global Variables ====\n",
      " [00] Name:[basic_cnn_mnist/identity_0/weights:0] Shape:[[1, 1, 1, 64]]\n",
      " [01] Name:[basic_cnn_mnist/identity_0/BatchNorm/beta:0] Shape:[[64]]\n",
      " [02] Name:[basic_cnn_mnist/identity_0/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [03] Name:[basic_cnn_mnist/identity_0/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [04] Name:[basic_cnn_mnist/res_a_0/weights:0] Shape:[[3, 3, 1, 64]]\n",
      " [05] Name:[basic_cnn_mnist/res_a_0/BatchNorm/beta:0] Shape:[[64]]\n",
      " [06] Name:[basic_cnn_mnist/res_a_0/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [07] Name:[basic_cnn_mnist/res_a_0/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [08] Name:[basic_cnn_mnist/res_b_0/weights:0] Shape:[[3, 3, 64, 64]]\n",
      " [09] Name:[basic_cnn_mnist/res_b_0/BatchNorm/beta:0] Shape:[[64]]\n",
      " [10] Name:[basic_cnn_mnist/res_b_0/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [11] Name:[basic_cnn_mnist/res_b_0/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [12] Name:[basic_cnn_mnist/res_a_1/weights:0] Shape:[[3, 3, 64, 64]]\n",
      " [13] Name:[basic_cnn_mnist/res_a_1/BatchNorm/beta:0] Shape:[[64]]\n",
      " [14] Name:[basic_cnn_mnist/res_a_1/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [15] Name:[basic_cnn_mnist/res_a_1/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [16] Name:[basic_cnn_mnist/res_b_1/weights:0] Shape:[[3, 3, 64, 64]]\n",
      " [17] Name:[basic_cnn_mnist/res_b_1/BatchNorm/beta:0] Shape:[[64]]\n",
      " [18] Name:[basic_cnn_mnist/res_b_1/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [19] Name:[basic_cnn_mnist/res_b_1/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [20] Name:[basic_cnn_mnist/fc/weights:0] Shape:[[3136, 256]]\n",
      " [21] Name:[basic_cnn_mnist/fc/biases:0] Shape:[[256]]\n",
      " [22] Name:[basic_cnn_mnist/out/weights:0] Shape:[[256, 10]]\n",
      " [23] Name:[basic_cnn_mnist/out/biases:0] Shape:[[10]]\n",
      " [24] Name:[basic_cnn_mnist/identity_0/weights/Adam:0] Shape:[[1, 1, 1, 64]]\n",
      " [25] Name:[basic_cnn_mnist/identity_0/weights/Adam_1:0] Shape:[[1, 1, 1, 64]]\n",
      " [26] Name:[basic_cnn_mnist/identity_0/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [27] Name:[basic_cnn_mnist/identity_0/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [28] Name:[basic_cnn_mnist/res_a_0/weights/Adam:0] Shape:[[3, 3, 1, 64]]\n",
      " [29] Name:[basic_cnn_mnist/res_a_0/weights/Adam_1:0] Shape:[[3, 3, 1, 64]]\n",
      " [30] Name:[basic_cnn_mnist/res_a_0/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [31] Name:[basic_cnn_mnist/res_a_0/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [32] Name:[basic_cnn_mnist/res_b_0/weights/Adam:0] Shape:[[3, 3, 64, 64]]\n",
      " [33] Name:[basic_cnn_mnist/res_b_0/weights/Adam_1:0] Shape:[[3, 3, 64, 64]]\n",
      " [34] Name:[basic_cnn_mnist/res_b_0/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [35] Name:[basic_cnn_mnist/res_b_0/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [36] Name:[basic_cnn_mnist/res_a_1/weights/Adam:0] Shape:[[3, 3, 64, 64]]\n",
      " [37] Name:[basic_cnn_mnist/res_a_1/weights/Adam_1:0] Shape:[[3, 3, 64, 64]]\n",
      " [38] Name:[basic_cnn_mnist/res_a_1/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [39] Name:[basic_cnn_mnist/res_a_1/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [40] Name:[basic_cnn_mnist/res_b_1/weights/Adam:0] Shape:[[3, 3, 64, 64]]\n",
      " [41] Name:[basic_cnn_mnist/res_b_1/weights/Adam_1:0] Shape:[[3, 3, 64, 64]]\n",
      " [42] Name:[basic_cnn_mnist/res_b_1/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [43] Name:[basic_cnn_mnist/res_b_1/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [44] Name:[basic_cnn_mnist/fc/weights/Adam:0] Shape:[[3136, 256]]\n",
      " [45] Name:[basic_cnn_mnist/fc/weights/Adam_1:0] Shape:[[3136, 256]]\n",
      " [46] Name:[basic_cnn_mnist/fc/biases/Adam:0] Shape:[[256]]\n",
      " [47] Name:[basic_cnn_mnist/fc/biases/Adam_1:0] Shape:[[256]]\n",
      " [48] Name:[basic_cnn_mnist/out/weights/Adam:0] Shape:[[256, 10]]\n",
      " [49] Name:[basic_cnn_mnist/out/weights/Adam_1:0] Shape:[[256, 10]]\n",
      " [50] Name:[basic_cnn_mnist/out/biases/Adam:0] Shape:[[10]]\n",
      " [51] Name:[basic_cnn_mnist/out/biases/Adam_1:0] Shape:[[10]]\n",
      "====== Layers ======\n",
      "[00/11] Placeholder:0 (?, 784)\n",
      "[01/11] basic_cnn_mnist/Reshape:0 (?, 28, 28, 1)\n",
      "[02/11] basic_cnn_mnist/Relu:0 (?, 28, 28, 64)\n",
      "[03/11] basic_cnn_mnist/Relu_1:0 (?, 28, 28, 64)\n",
      "[04/11] basic_cnn_mnist/pool_0/MaxPool:0 (?, 14, 14, 64)\n",
      "[05/11] basic_cnn_mnist/Relu_2:0 (?, 14, 14, 64)\n",
      "[06/11] basic_cnn_mnist/Relu_3:0 (?, 14, 14, 64)\n",
      "[07/11] basic_cnn_mnist/pool_1/MaxPool:0 (?, 7, 7, 64)\n",
      "[08/11] basic_cnn_mnist/flatten/flatten/Reshape:0 (?, 3136)\n",
      "[09/11] basic_cnn_mnist/fc/Relu:0 (?, 256)\n",
      "[10/11] basic_cnn_mnist/out/BiasAdd:0 (?, 10)\n",
      "Text name: ../res/res_basic_cnn_mnist.txt\n",
      "[../net/net_basic_cnn_mnist.npz] Saved. Size is [16.0243]MB\n",
      "[00/50] [Loss] train:1.223 val:1.084 test:1.091 [Accr] train:46.0% val:57.6% test:58.5% maxVal:57.6% maxTest:58.5%\n",
      "[../net/net_basic_cnn_mnist.npz] Saved. Size is [16.0531]MB\n",
      "[05/50] [Loss] train:0.858 val:0.767 test:0.772 [Accr] train:53.8% val:65.3% test:65.2% maxVal:65.3% maxTest:65.2%\n",
      "[10/50] [Loss] train:0.780 val:0.746 test:0.745 [Accr] train:57.2% val:62.8% test:61.9% maxVal:65.3% maxTest:65.2%\n",
      "[15/50] [Loss] train:0.734 val:0.722 test:0.710 [Accr] train:60.1% val:62.8% test:63.2% maxVal:65.3% maxTest:65.2%\n",
      "[20/50] [Loss] train:0.700 val:0.710 test:0.697 [Accr] train:63.2% val:63.7% test:64.6% maxVal:65.3% maxTest:65.2%\n",
      "[25/50] [Loss] train:0.670 val:0.706 test:0.696 [Accr] train:66.2% val:63.0% test:64.0% maxVal:65.3% maxTest:65.2%\n",
      "[30/50] [Loss] train:0.656 val:0.701 test:0.689 [Accr] train:67.6% val:64.1% test:65.0% maxVal:65.3% maxTest:65.2%\n",
      "[35/50] [Loss] train:0.645 val:0.711 test:0.702 [Accr] train:68.7% val:62.6% test:62.2% maxVal:65.3% maxTest:65.2%\n",
      "[40/50] [Loss] train:0.634 val:0.705 test:0.694 [Accr] train:70.1% val:63.2% test:63.6% maxVal:65.3% maxTest:65.2%\n",
      "[45/50] [Loss] train:0.631 val:0.702 test:0.691 [Accr] train:70.5% val:63.0% test:63.4% maxVal:65.3% maxTest:65.2%\n",
      "[50/50] [Loss] train:0.628 val:0.707 test:0.695 [Accr] train:70.8% val:62.3% test:62.7% maxVal:65.3% maxTest:65.2%\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    trainimg,trainlabel,testimg,testlabel,valimg,vallabel \\\n",
    "        ,xdim,ydim,hdims,filterSizes,max_pools,feat_dim \\\n",
    "        ,actv,bn,VERBOSE \\\n",
    "        ,USE_INPUT_BN,USE_RESNET,USE_GAP,USE_MIXUP = get_mnist_config()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0) \n",
    "    CNN = cnn_cls_class(_name='basic_cnn_mnist',_xdim=xdim,_ydim=ydim,_hdims=hdims,_filterSizes=filterSizes,_max_pools=max_pools\n",
    "                        ,_feat_dim=feat_dim,_actv=actv,_bn=bn,_l2_reg_coef=1e-5 \n",
    "                        ,_USE_INPUT_BN=USE_INPUT_BN,_USE_RESNET=USE_RESNET,_USE_GAP=USE_GAP\n",
    "                        ,_USE_MIXUP=USE_MIXUP,_GPU_ID=0,_VERBOSE=VERBOSE)\n",
    "    sess = gpusession();sess.run(tf.global_variables_initializer()) \n",
    "    CNN.train(_sess=sess,_trainimg=trainimg,_trainlabel=trainlabel\n",
    "              ,_testimg=testimg,_testlabel=testlabel,_valimg=valimg,_vallabel=vallabel\n",
    "              ,_maxEpoch=50,_batchSize=256,_lr=1e-5,_LR_SCHEDULE=True,_PRINT_EVERY=10,_SAVE_BEST=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore and Re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "[basic_cnn_mnist] [Loss] train:0.858 val:0.767 test:0.772 [Accr] train:53.861% val:65.299% test:65.214%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    trainimg,trainlabel,testimg,testlabel,valimg,vallabel \\\n",
    "        ,xdim,ydim,hdims,filterSizes,max_pools,feat_dim \\\n",
    "        ,actv,bn,VERBOSE \\\n",
    "        ,USE_INPUT_BN,USE_RESNET,USE_GAP,USE_MIXUP = get_mnist_config()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0) \n",
    "    CNN2 = cnn_cls_class(_name='basic_cnn_mnist',_xdim=xdim,_ydim=ydim,_hdims=hdims,_filterSizes=filterSizes,_max_pools=max_pools\n",
    "                        ,_feat_dim=feat_dim,_actv=actv,_bn=bn,_l2_reg_coef=1e-5 \n",
    "                        ,_USE_INPUT_BN=USE_INPUT_BN,_USE_RESNET=USE_RESNET,_USE_GAP=USE_GAP\n",
    "                        ,_USE_MIXUP=USE_MIXUP,_GPU_ID=0,_VERBOSE=False)\n",
    "    sess = gpusession();sess.run(tf.global_variables_initializer()) \n",
    "    CNN2.restore(sess) # Restore weights\n",
    "    CNN2.test(sess,_trainimg=trainimg,_trainlabel=trainlabel\n",
    "             ,_testimg=testimg,_testlabel=testlabel,_valimg=valimg,_vallabel=vallabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
