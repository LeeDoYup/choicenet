{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Classification (MNIST) Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded.\n"
     ]
    }
   ],
   "source": [
    "import nbloader,time\n",
    "from nips_cls_config_2 import worker_class\n",
    "if __name__ == \"__main__\": \n",
    "    print (\"Packages loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Worker_00] Instantiated.\n",
      "[Worker_01] Instantiated.\n",
      "[Worker_02] Instantiated.\n",
      "[Worker_03] Instantiated.\n"
     ]
    }
   ],
   "source": [
    "nWorker = 4\n",
    "maxGPU  = 2\n",
    "WORKERS = ['']*nWorker\n",
    "for i in range(nWorker):\n",
    "    WORKERS[i] = worker_class(_idx=i,_maxProcessID=nWorker,_maxGPU=maxGPU,_name='Worker_%02d'%(i)\n",
    "                              ,_period=1.0,_maxTick=10,_VERBOSE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting [Worker_00]\n",
      "processID:[0/4] GPU_ID:[0] #Config:[2]\n",
      "WARNING:tensorflow:From <string>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Starting [Worker_01]\n",
      "processID:[1/4] GPU_ID:[1] #Config:[2]\n",
      "WARNING:tensorflow:From <string>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Starting [Worker_02]\n",
      "processID:[2/4] GPU_ID:[0] #Config:[1]\n",
      "WARNING:tensorflow:From <string>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From <string>:224: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Starting [Worker_03]\n",
      "processID:[3/4] GPU_ID:[1] #Config:[1]\n",
      "WARNING:tensorflow:From <string>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From <string>:224: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <string>:224: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From <string>:224: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "Text name: ../res/res_mnist_rs_err90_basic_kmix2_choiceNet.txt\n",
      "Text name: ../res/res_mnist_rs_err90_basic_kmix3_choiceNet.txt\n",
      "Text name: ../res/res_mnist_rs_err90_basic_kmix5_choiceNet.txt\n",
      "Text name: ../res/res_mnist_rs_err90_basic_kmix10_choiceNet.txt\n",
      "[00/40] [Loss] train:0.033(f:-0.123+r:0.041+k:0.000+l:0.116) val:-0.148 test:-0.161 [Accr] train:15.227% val:58.377% test:60.578% maxVal:58.377% maxTest:60.578%\n",
      "[00/40] [Loss] train:0.043(f:-0.116+r:0.043+k:0.000+l:0.116) val:-0.087 test:-0.098 [Accr] train:14.955% val:55.078% test:56.630% maxVal:55.078% maxTest:56.630%\n",
      "[00/40] [Loss] train:0.049(f:-0.114+r:0.046+k:0.000+l:0.116) val:-0.056 test:-0.066 [Accr] train:14.840% val:53.863% test:57.103% maxVal:53.863% maxTest:57.103%\n",
      "[00/40] [Loss] train:0.051(f:-0.114+r:0.049+k:0.001+l:0.116) val:-0.059 test:-0.069 [Accr] train:15.271% val:59.115% test:61.308% maxVal:59.115% maxTest:61.308%\n",
      "[01/40] [Loss] train:0.022(f:-0.137+r:0.043+k:0.000+l:0.115) val:-0.282 test:-0.292 [Accr] train:17.082% val:78.689% test:79.194% maxVal:78.689% maxTest:79.194%\n",
      "[01/40] [Loss] train:0.032(f:-0.130+r:0.046+k:0.000+l:0.115) val:-0.216 test:-0.222 [Accr] train:16.987% val:77.322% test:78.074% maxVal:77.322% maxTest:78.074%\n",
      "[01/40] [Loss] train:0.039(f:-0.125+r:0.048+k:0.000+l:0.115) val:-0.166 test:-0.173 [Accr] train:16.808% val:74.023% test:76.357% maxVal:74.023% maxTest:76.357%\n",
      "[01/40] [Loss] train:0.043(f:-0.123+r:0.050+k:0.001+l:0.115) val:-0.151 test:-0.156 [Accr] train:16.987% val:76.975% test:78.310% maxVal:76.975% maxTest:78.310%\n",
      "[02/40] [Loss] train:0.016(f:-0.144+r:0.045+k:0.000+l:0.115) val:-0.356 test:-0.361 [Accr] train:17.821% val:85.113% test:85.465% maxVal:85.113% maxTest:85.465%\n",
      "[02/40] [Loss] train:0.025(f:-0.137+r:0.047+k:0.000+l:0.115) val:-0.279 test:-0.288 [Accr] train:17.741% val:84.071% test:84.303% maxVal:84.071% maxTest:84.303%\n",
      "[02/40] [Loss] train:0.033(f:-0.132+r:0.049+k:0.000+l:0.115) val:-0.233 test:-0.236 [Accr] train:17.443% val:80.990% test:82.699% maxVal:80.990% maxTest:82.699%\n",
      "[03/40] [Loss] train:0.013(f:-0.148+r:0.046+k:0.000+l:0.115) val:-0.387 test:-0.394 [Accr] train:18.120% val:88.759% test:88.713% maxVal:88.759% maxTest:88.713%\n",
      "[03/40] [Loss] train:0.022(f:-0.141+r:0.048+k:0.000+l:0.115) val:-0.308 test:-0.320 [Accr] train:18.045% val:87.326% test:87.438% maxVal:87.326% maxTest:87.438%\n",
      "[02/40] [Loss] train:0.037(f:-0.130+r:0.051+k:0.001+l:0.115) val:-0.209 test:-0.215 [Accr] train:17.578% val:83.290% test:84.056% maxVal:83.290% maxTest:84.056%\n",
      "[03/40] [Loss] train:0.029(f:-0.136+r:0.049+k:0.000+l:0.115) val:-0.264 test:-0.265 [Accr] train:17.790% val:85.004% test:86.081% maxVal:85.004% maxTest:86.081%\n",
      "[04/40] [Loss] train:0.010(f:-0.151+r:0.046+k:0.000+l:0.115) val:-0.408 test:-0.416 [Accr] train:18.352% val:90.213% test:90.532% maxVal:90.213% maxTest:90.532%\n",
      "[04/40] [Loss] train:0.020(f:-0.143+r:0.048+k:0.000+l:0.115) val:-0.329 test:-0.337 [Accr] train:18.264% val:89.540% test:89.648% maxVal:89.540% maxTest:89.648%\n",
      "[03/40] [Loss] train:0.034(f:-0.133+r:0.051+k:0.001+l:0.115) val:-0.237 test:-0.246 [Accr] train:17.790% val:85.872% test:86.606% maxVal:85.872% maxTest:86.606%\n",
      "[04/40] [Loss] train:0.027(f:-0.138+r:0.050+k:0.000+l:0.115) val:-0.274 test:-0.283 [Accr] train:18.075% val:87.695% test:88.806% maxVal:87.695% maxTest:88.806%\n",
      "[05/40] [Loss] train:0.008(f:-0.152+r:0.045+k:0.000+l:0.115) val:-0.422 test:-0.427 [Accr] train:18.536% val:91.710% test:91.499% maxVal:91.710% maxTest:91.499%\n",
      "[05/40] [Loss] train:0.017(f:-0.145+r:0.047+k:0.000+l:0.115) val:-0.350 test:-0.357 [Accr] train:18.363% val:91.124% test:90.748% maxVal:91.124% maxTest:90.748%\n",
      "[04/40] [Loss] train:0.032(f:-0.135+r:0.051+k:0.001+l:0.115) val:-0.254 test:-0.265 [Accr] train:18.069% val:88.260% test:88.507% maxVal:88.260% maxTest:88.507%\n",
      "[06/40] [Loss] train:0.004(f:-0.155+r:0.044+k:0.000+l:0.115) val:-0.429 test:-0.437 [Accr] train:18.686% val:92.839% test:92.609% maxVal:92.839% maxTest:92.609%\n",
      "[05/40] [Loss] train:0.025(f:-0.139+r:0.049+k:0.000+l:0.115) val:-0.297 test:-0.299 [Accr] train:18.204% val:89.670% test:90.368% maxVal:89.670% maxTest:90.368%\n",
      "[06/40] [Loss] train:0.015(f:-0.147+r:0.047+k:0.000+l:0.115) val:-0.359 test:-0.367 [Accr] train:18.509% val:92.079% test:92.105% maxVal:92.079% maxTest:92.105%\n",
      "[07/40] [Loss] train:0.002(f:-0.157+r:0.044+k:0.000+l:0.115) val:-0.446 test:-0.452 [Accr] train:18.732% val:92.622% test:93.133% maxVal:92.839% maxTest:92.609%\n",
      "[06/40] [Loss] train:0.022(f:-0.142+r:0.049+k:0.000+l:0.115) val:-0.316 test:-0.320 [Accr] train:18.378% val:91.363% test:91.817% maxVal:91.363% maxTest:91.817%\n",
      "[05/40] [Loss] train:0.030(f:-0.137+r:0.051+k:0.001+l:0.115) val:-0.278 test:-0.282 [Accr] train:18.166% val:89.627% test:89.782% maxVal:89.627% maxTest:89.782%\n",
      "[07/40] [Loss] train:0.013(f:-0.148+r:0.047+k:0.000+l:0.115) val:-0.369 test:-0.379 [Accr] train:18.609% val:92.773% test:92.630% maxVal:92.773% maxTest:92.630%\n",
      "[08/40] [Loss] train:0.001(f:-0.157+r:0.043+k:0.000+l:0.114) val:-0.454 test:-0.451 [Accr] train:18.849% val:93.316% test:93.586% maxVal:93.316% maxTest:93.586%\n",
      "[07/40] [Loss] train:0.020(f:-0.143+r:0.049+k:0.000+l:0.115) val:-0.330 test:-0.332 [Accr] train:18.463% val:92.101% test:92.516% maxVal:92.101% maxTest:92.516%\n",
      "[08/40] [Loss] train:0.011(f:-0.149+r:0.046+k:0.000+l:0.114) val:-0.378 test:-0.384 [Accr] train:18.657% val:92.904% test:93.000% maxVal:92.904% maxTest:93.000%\n",
      "[06/40] [Loss] train:0.027(f:-0.139+r:0.050+k:0.001+l:0.115) val:-0.288 test:-0.295 [Accr] train:18.319% val:91.146% test:91.170% maxVal:91.146% maxTest:91.170%\n",
      "[09/40] [Loss] train:-0.001(f:-0.158+r:0.042+k:0.000+l:0.114) val:-0.452 test:-0.454 [Accr] train:18.912% val:93.707% test:93.945% maxVal:93.707% maxTest:93.945%\n",
      "[09/40] [Loss] train:0.009(f:-0.151+r:0.046+k:0.000+l:0.114) val:-0.379 test:-0.388 [Accr] train:18.706% val:93.555% test:93.411% maxVal:93.555% maxTest:93.411%\n",
      "[08/40] [Loss] train:0.019(f:-0.144+r:0.048+k:0.000+l:0.114) val:-0.343 test:-0.345 [Accr] train:18.480% val:92.687% test:92.794% maxVal:92.687% maxTest:92.794%\n",
      "[07/40] [Loss] train:0.025(f:-0.141+r:0.050+k:0.001+l:0.115) val:-0.300 test:-0.307 [Accr] train:18.429% val:91.753% test:91.817% maxVal:91.753% maxTest:91.817%\n",
      "[10/40] [Loss] train:-0.002(f:-0.158+r:0.042+k:0.000+l:0.114) val:-0.458 test:-0.460 [Accr] train:19.011% val:94.032% test:94.480% maxVal:94.032% maxTest:94.480%\n",
      "[10/40] [Loss] train:0.007(f:-0.152+r:0.045+k:0.000+l:0.114) val:-0.383 test:-0.394 [Accr] train:18.821% val:93.945% test:93.925% maxVal:93.945% maxTest:93.925%\n",
      "[09/40] [Loss] train:0.017(f:-0.145+r:0.047+k:0.000+l:0.114) val:-0.349 test:-0.343 [Accr] train:18.584% val:93.142% test:93.503% maxVal:93.142% maxTest:93.503%\n",
      "[11/40] [Loss] train:-0.004(f:-0.159+r:0.041+k:0.000+l:0.114) val:-0.450 test:-0.452 [Accr] train:19.029% val:94.162% test:94.500% maxVal:94.162% maxTest:94.500%\n",
      "[08/40] [Loss] train:0.023(f:-0.142+r:0.049+k:0.001+l:0.114) val:-0.317 test:-0.322 [Accr] train:18.513% val:92.470% test:92.331% maxVal:92.470% maxTest:92.331%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/40] [Loss] train:0.006(f:-0.152+r:0.044+k:0.000+l:0.114) val:-0.391 test:-0.400 [Accr] train:18.849% val:93.989% test:94.028% maxVal:93.989% maxTest:94.028%\n",
      "[10/40] [Loss] train:0.015(f:-0.146+r:0.046+k:0.000+l:0.114) val:-0.350 test:-0.352 [Accr] train:18.692% val:93.641% test:93.873% maxVal:93.641% maxTest:93.873%\n",
      "[12/40] [Loss] train:-0.007(f:-0.162+r:0.041+k:0.000+l:0.114) val:-0.467 test:-0.470 [Accr] train:19.150% val:94.293% test:94.809% maxVal:94.293% maxTest:94.809%\n",
      "[12/40] [Loss] train:0.004(f:-0.154+r:0.044+k:0.000+l:0.114) val:-0.393 test:-0.405 [Accr] train:18.956% val:93.924% test:94.264% maxVal:93.989% maxTest:94.028%\n",
      "[09/40] [Loss] train:0.021(f:-0.143+r:0.048+k:0.001+l:0.114) val:-0.324 test:-0.324 [Accr] train:18.598% val:93.034% test:92.845% maxVal:93.034% maxTest:92.845%\n",
      "[11/40] [Loss] train:0.013(f:-0.147+r:0.046+k:0.000+l:0.114) val:-0.352 test:-0.354 [Accr] train:18.690% val:93.663% test:94.007% maxVal:93.663% maxTest:94.007%\n",
      "[13/40] [Loss] train:-0.007(f:-0.161+r:0.040+k:0.000+l:0.114) val:-0.457 test:-0.459 [Accr] train:19.210% val:94.510% test:94.870% maxVal:94.510% maxTest:94.870%\n",
      "[13/40] [Loss] train:0.004(f:-0.153+r:0.043+k:0.000+l:0.114) val:-0.390 test:-0.398 [Accr] train:18.993% val:94.379% test:94.603% maxVal:94.379% maxTest:94.603%\n",
      "[10/40] [Loss] train:0.018(f:-0.144+r:0.047+k:0.001+l:0.114) val:-0.334 test:-0.334 [Accr] train:18.613% val:93.446% test:93.195% maxVal:93.446% maxTest:93.195%\n",
      "[14/40] [Loss] train:-0.010(f:-0.163+r:0.039+k:0.000+l:0.113) val:-0.467 test:-0.467 [Accr] train:19.219% val:94.358% test:94.942% maxVal:94.510% maxTest:94.870%\n",
      "[12/40] [Loss] train:0.011(f:-0.148+r:0.045+k:0.000+l:0.114) val:-0.368 test:-0.366 [Accr] train:18.763% val:94.358% test:94.264% maxVal:94.358% maxTest:94.264%\n",
      "[14/40] [Loss] train:0.001(f:-0.155+r:0.043+k:0.000+l:0.113) val:-0.394 test:-0.403 [Accr] train:19.055% val:94.466% test:94.511% maxVal:94.466% maxTest:94.511%\n",
      "[15/40] [Loss] train:-0.011(f:-0.163+r:0.038+k:0.000+l:0.113) val:-0.459 test:-0.464 [Accr] train:19.332% val:94.510% test:94.953% maxVal:94.510% maxTest:94.870%\n",
      "[11/40] [Loss] train:0.017(f:-0.145+r:0.046+k:0.001+l:0.114) val:-0.332 test:-0.337 [Accr] train:18.650% val:93.359% test:93.287% maxVal:93.446% maxTest:93.195%\n",
      "[13/40] [Loss] train:0.010(f:-0.148+r:0.044+k:0.000+l:0.114) val:-0.355 test:-0.358 [Accr] train:18.810% val:94.444% test:94.737% maxVal:94.444% maxTest:94.737%\n",
      "[15/40] [Loss] train:-0.000(f:-0.155+r:0.042+k:0.000+l:0.113) val:-0.403 test:-0.409 [Accr] train:19.058% val:94.640% test:94.778% maxVal:94.640% maxTest:94.778%\n",
      "[16/40] [Loss] train:-0.016(f:-0.166+r:0.037+k:0.000+l:0.113) val:-0.469 test:-0.474 [Accr] train:19.427% val:94.575% test:94.860% maxVal:94.575% maxTest:94.860%\n",
      "[14/40] [Loss] train:0.008(f:-0.149+r:0.044+k:0.000+l:0.113) val:-0.372 test:-0.369 [Accr] train:18.825% val:94.727% test:94.799% maxVal:94.727% maxTest:94.799%\n",
      "[12/40] [Loss] train:0.014(f:-0.146+r:0.045+k:0.001+l:0.114) val:-0.339 test:-0.343 [Accr] train:18.748% val:93.555% test:93.555% maxVal:93.555% maxTest:93.555%\n",
      "[16/40] [Loss] train:-0.002(f:-0.156+r:0.041+k:0.000+l:0.113) val:-0.403 test:-0.409 [Accr] train:19.163% val:94.531% test:94.644% maxVal:94.640% maxTest:94.778%\n",
      "[17/40] [Loss] train:-0.016(f:-0.166+r:0.037+k:0.000+l:0.113) val:-0.470 test:-0.475 [Accr] train:19.385% val:94.488% test:95.045% maxVal:94.575% maxTest:94.860%\n",
      "[15/40] [Loss] train:0.006(f:-0.151+r:0.043+k:0.000+l:0.113) val:-0.370 test:-0.370 [Accr] train:18.867% val:94.618% test:94.696% maxVal:94.727% maxTest:94.799%\n",
      "[17/40] [Loss] train:-0.004(f:-0.157+r:0.040+k:0.000+l:0.113) val:-0.398 test:-0.409 [Accr] train:19.221% val:94.640% test:94.881% maxVal:94.640% maxTest:94.778%\n",
      "[13/40] [Loss] train:0.013(f:-0.146+r:0.044+k:0.001+l:0.114) val:-0.334 test:-0.339 [Accr] train:18.776% val:94.162% test:94.120% maxVal:94.162% maxTest:94.120%\n",
      "[18/40] [Loss] train:-0.018(f:-0.166+r:0.036+k:0.000+l:0.113) val:-0.470 test:-0.472 [Accr] train:19.544% val:94.640% test:94.788% maxVal:94.640% maxTest:94.788%\n",
      "[16/40] [Loss] train:0.004(f:-0.152+r:0.042+k:0.000+l:0.113) val:-0.384 test:-0.383 [Accr] train:18.940% val:94.748% test:94.850% maxVal:94.748% maxTest:94.850%\n",
      "[18/40] [Loss] train:-0.006(f:-0.159+r:0.040+k:0.000+l:0.113) val:-0.409 test:-0.415 [Accr] train:19.203% val:94.683% test:95.097% maxVal:94.683% maxTest:95.097%\n",
      "[19/40] [Loss] train:-0.018(f:-0.167+r:0.035+k:0.000+l:0.113) val:-0.462 test:-0.469 [Accr] train:19.555% val:94.727% test:95.138% maxVal:94.727% maxTest:95.138%\n",
      "[14/40] [Loss] train:0.011(f:-0.147+r:0.044+k:0.001+l:0.113) val:-0.347 test:-0.348 [Accr] train:18.788% val:94.076% test:94.202% maxVal:94.162% maxTest:94.120%\n",
      "[19/40] [Loss] train:-0.007(f:-0.158+r:0.039+k:0.000+l:0.113) val:-0.406 test:-0.421 [Accr] train:19.310% val:94.640% test:95.066% maxVal:94.683% maxTest:95.097%\n",
      "[17/40] [Loss] train:0.003(f:-0.151+r:0.041+k:0.000+l:0.113) val:-0.378 test:-0.378 [Accr] train:18.982% val:94.792% test:94.850% maxVal:94.792% maxTest:94.850%\n",
      "[20/40] [Loss] train:-0.019(f:-0.167+r:0.035+k:0.000+l:0.113) val:-0.473 test:-0.471 [Accr] train:19.586% val:94.618% test:95.148% maxVal:94.727% maxTest:95.138%\n",
      "[15/40] [Loss] train:0.010(f:-0.147+r:0.042+k:0.001+l:0.113) val:-0.355 test:-0.355 [Accr] train:18.818% val:94.314% test:94.326% maxVal:94.314% maxTest:94.326%\n",
      "[20/40] [Loss] train:-0.008(f:-0.159+r:0.038+k:0.000+l:0.113) val:-0.409 test:-0.416 [Accr] train:19.296% val:94.727% test:95.127% maxVal:94.727% maxTest:95.127%\n",
      "[21/40] [Loss] train:-0.020(f:-0.168+r:0.035+k:0.000+l:0.113) val:-0.469 test:-0.472 [Accr] train:19.621% val:94.640% test:95.076% maxVal:94.727% maxTest:95.138%\n",
      "[18/40] [Loss] train:0.001(f:-0.153+r:0.040+k:0.000+l:0.113) val:-0.384 test:-0.385 [Accr] train:18.982% val:94.878% test:94.881% maxVal:94.878% maxTest:94.881%\n",
      "[16/40] [Loss] train:0.007(f:-0.149+r:0.041+k:0.001+l:0.113) val:-0.361 test:-0.361 [Accr] train:18.902% val:94.510% test:94.603% maxVal:94.510% maxTest:94.603%\n",
      "[21/40] [Loss] train:-0.008(f:-0.160+r:0.038+k:0.000+l:0.113) val:-0.407 test:-0.418 [Accr] train:19.312% val:94.531% test:95.138% maxVal:94.727% maxTest:95.127%\n",
      "[22/40] [Loss] train:-0.020(f:-0.168+r:0.035+k:0.000+l:0.113) val:-0.465 test:-0.470 [Accr] train:19.654% val:94.553% test:95.127% maxVal:94.727% maxTest:95.138%\n",
      "[19/40] [Loss] train:0.001(f:-0.152+r:0.040+k:0.000+l:0.113) val:-0.368 test:-0.370 [Accr] train:19.020% val:94.922% test:95.097% maxVal:94.922% maxTest:95.097%\n",
      "[23/40] [Loss] train:-0.021(f:-0.169+r:0.035+k:0.000+l:0.113) val:-0.468 test:-0.471 [Accr] train:19.666% val:94.640% test:95.169% maxVal:94.727% maxTest:95.138%\n",
      "[22/40] [Loss] train:-0.009(f:-0.160+r:0.038+k:0.000+l:0.113) val:-0.413 test:-0.420 [Accr] train:19.343% val:94.640% test:95.117% maxVal:94.727% maxTest:95.127%\n",
      "[17/40] [Loss] train:0.005(f:-0.149+r:0.040+k:0.001+l:0.113) val:-0.358 test:-0.360 [Accr] train:18.922% val:94.727% test:94.727% maxVal:94.727% maxTest:94.727%\n",
      "[20/40] [Loss] train:-0.000(f:-0.153+r:0.039+k:0.000+l:0.113) val:-0.387 test:-0.383 [Accr] train:19.046% val:95.095% test:95.127% maxVal:95.095% maxTest:95.127%\n",
      "[24/40] [Loss] train:-0.021(f:-0.168+r:0.035+k:0.000+l:0.113) val:-0.465 test:-0.471 [Accr] train:19.655% val:94.553% test:95.158% maxVal:94.727% maxTest:95.138%\n",
      "[23/40] [Loss] train:-0.009(f:-0.160+r:0.038+k:0.000+l:0.113) val:-0.409 test:-0.416 [Accr] train:19.356% val:94.640% test:95.127% maxVal:94.727% maxTest:95.127%\n",
      "[21/40] [Loss] train:-0.002(f:-0.154+r:0.039+k:0.000+l:0.113) val:-0.387 test:-0.387 [Accr] train:19.009% val:94.965% test:95.056% maxVal:95.095% maxTest:95.127%\n",
      "[18/40] [Loss] train:0.003(f:-0.150+r:0.039+k:0.001+l:0.113) val:-0.364 test:-0.369 [Accr] train:18.892% val:94.813% test:94.819% maxVal:94.813% maxTest:94.819%\n",
      "[25/40] [Loss] train:-0.021(f:-0.169+r:0.035+k:0.000+l:0.113) val:-0.464 test:-0.468 [Accr] train:19.721% val:94.640% test:95.086% maxVal:94.727% maxTest:95.138%\n",
      "[24/40] [Loss] train:-0.008(f:-0.160+r:0.038+k:0.000+l:0.113) val:-0.409 test:-0.418 [Accr] train:19.323% val:94.618% test:95.097% maxVal:94.727% maxTest:95.127%\n",
      "[22/40] [Loss] train:-0.002(f:-0.154+r:0.039+k:0.000+l:0.113) val:-0.385 test:-0.385 [Accr] train:19.042% val:94.987% test:95.035% maxVal:95.095% maxTest:95.127%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/40] [Loss] train:0.001(f:-0.150+r:0.038+k:0.001+l:0.113) val:-0.368 test:-0.371 [Accr] train:18.918% val:94.944% test:95.076% maxVal:94.944% maxTest:95.076%\n",
      "[26/40] [Loss] train:-0.022(f:-0.169+r:0.035+k:0.000+l:0.113) val:-0.470 test:-0.472 [Accr] train:19.719% val:94.466% test:95.158% maxVal:94.727% maxTest:95.138%\n",
      "[25/40] [Loss] train:-0.010(f:-0.161+r:0.038+k:0.000+l:0.113) val:-0.411 test:-0.419 [Accr] train:19.369% val:94.640% test:95.117% maxVal:94.727% maxTest:95.127%\n",
      "[23/40] [Loss] train:-0.002(f:-0.154+r:0.039+k:0.000+l:0.113) val:-0.387 test:-0.385 [Accr] train:19.049% val:94.965% test:95.127% maxVal:95.095% maxTest:95.127%\n",
      "[27/40] [Loss] train:-0.022(f:-0.169+r:0.035+k:0.000+l:0.113) val:-0.468 test:-0.472 [Accr] train:19.716% val:94.444% test:95.107% maxVal:94.727% maxTest:95.138%\n",
      "[26/40] [Loss] train:-0.010(f:-0.161+r:0.038+k:0.000+l:0.113) val:-0.410 test:-0.419 [Accr] train:19.394% val:94.618% test:95.076% maxVal:94.727% maxTest:95.127%\n",
      "[20/40] [Loss] train:0.001(f:-0.150+r:0.038+k:0.001+l:0.113) val:-0.367 test:-0.370 [Accr] train:18.940% val:95.009% test:95.076% maxVal:95.009% maxTest:95.076%\n",
      "[28/40] [Loss] train:-0.023(f:-0.170+r:0.035+k:0.000+l:0.113) val:-0.467 test:-0.470 [Accr] train:19.739% val:94.358% test:95.158% maxVal:94.727% maxTest:95.138%\n",
      "[24/40] [Loss] train:-0.001(f:-0.154+r:0.039+k:0.000+l:0.113) val:-0.386 test:-0.386 [Accr] train:19.007% val:94.965% test:95.138% maxVal:95.095% maxTest:95.127%\n",
      "[27/40] [Loss] train:-0.009(f:-0.160+r:0.038+k:0.000+l:0.113) val:-0.409 test:-0.417 [Accr] train:19.371% val:94.727% test:95.107% maxVal:94.727% maxTest:95.127%\n",
      "[21/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.372 [Accr] train:18.923% val:95.009% test:95.045% maxVal:95.009% maxTest:95.076%\n",
      "[29/40] [Loss] train:-0.023(f:-0.170+r:0.034+k:0.000+l:0.113) val:-0.464 test:-0.469 [Accr] train:19.734% val:94.488% test:95.076% maxVal:94.727% maxTest:95.138%\n",
      "[25/40] [Loss] train:-0.002(f:-0.154+r:0.039+k:0.000+l:0.113) val:-0.390 test:-0.383 [Accr] train:19.062% val:94.965% test:95.107% maxVal:95.095% maxTest:95.127%\n",
      "[28/40] [Loss] train:-0.010(f:-0.161+r:0.038+k:0.000+l:0.113) val:-0.405 test:-0.414 [Accr] train:19.400% val:94.640% test:95.086% maxVal:94.727% maxTest:95.127%\n",
      "[30/40] [Loss] train:-0.022(f:-0.169+r:0.034+k:0.000+l:0.113) val:-0.469 test:-0.470 [Accr] train:19.756% val:94.379% test:95.086% maxVal:94.727% maxTest:95.138%\n",
      "[22/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.368 test:-0.370 [Accr] train:18.923% val:95.074% test:95.086% maxVal:95.074% maxTest:95.086%\n",
      "[26/40] [Loss] train:-0.003(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.390 test:-0.389 [Accr] train:19.071% val:94.900% test:95.045% maxVal:95.095% maxTest:95.127%\n",
      "[29/40] [Loss] train:-0.011(f:-0.161+r:0.038+k:0.000+l:0.113) val:-0.405 test:-0.416 [Accr] train:19.398% val:94.705% test:95.138% maxVal:94.727% maxTest:95.127%\n",
      "[31/40] [Loss] train:-0.023(f:-0.170+r:0.035+k:0.000+l:0.113) val:-0.462 test:-0.467 [Accr] train:19.774% val:94.531% test:95.107% maxVal:94.727% maxTest:95.138%\n",
      "[23/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.369 test:-0.370 [Accr] train:18.962% val:95.074% test:95.107% maxVal:95.074% maxTest:95.086%\n",
      "[30/40] [Loss] train:-0.010(f:-0.161+r:0.038+k:0.000+l:0.113) val:-0.411 test:-0.416 [Accr] train:19.400% val:94.748% test:95.127% maxVal:94.748% maxTest:95.127%\n",
      "[27/40] [Loss] train:-0.003(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.384 test:-0.385 [Accr] train:19.060% val:94.944% test:95.097% maxVal:95.095% maxTest:95.127%\n",
      "[32/40] [Loss] train:-0.023(f:-0.170+r:0.034+k:0.000+l:0.113) val:-0.467 test:-0.468 [Accr] train:19.774% val:94.444% test:95.148% maxVal:94.727% maxTest:95.138%\n",
      "[31/40] [Loss] train:-0.011(f:-0.162+r:0.038+k:0.000+l:0.113) val:-0.408 test:-0.415 [Accr] train:19.420% val:94.661% test:95.127% maxVal:94.748% maxTest:95.127%\n",
      "[28/40] [Loss] train:-0.003(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.383 test:-0.385 [Accr] train:19.057% val:94.965% test:95.097% maxVal:95.095% maxTest:95.127%\n",
      "[24/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.373 test:-0.372 [Accr] train:18.940% val:95.182% test:95.148% maxVal:95.182% maxTest:95.148%\n",
      "[33/40] [Loss] train:-0.023(f:-0.170+r:0.034+k:0.000+l:0.113) val:-0.465 test:-0.470 [Accr] train:19.761% val:94.531% test:95.127% maxVal:94.727% maxTest:95.138%\n",
      "[32/40] [Loss] train:-0.010(f:-0.161+r:0.038+k:0.000+l:0.113) val:-0.411 test:-0.417 [Accr] train:19.424% val:94.748% test:95.138% maxVal:94.748% maxTest:95.127%\n",
      "[29/40] [Loss] train:-0.003(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.385 test:-0.386 [Accr] train:19.057% val:95.139% test:95.179% maxVal:95.139% maxTest:95.179%\n",
      "[34/40] [Loss] train:-0.023(f:-0.170+r:0.034+k:0.000+l:0.113) val:-0.464 test:-0.470 [Accr] train:19.785% val:94.553% test:95.086% maxVal:94.727% maxTest:95.138%\n",
      "[25/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.370 test:-0.370 [Accr] train:18.975% val:95.226% test:95.158% maxVal:95.226% maxTest:95.158%\n",
      "[33/40] [Loss] train:-0.010(f:-0.160+r:0.038+k:0.000+l:0.113) val:-0.408 test:-0.418 [Accr] train:19.420% val:94.683% test:95.107% maxVal:94.748% maxTest:95.127%\n",
      "[30/40] [Loss] train:-0.004(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.389 test:-0.386 [Accr] train:19.069% val:94.922% test:95.117% maxVal:95.139% maxTest:95.179%\n",
      "[35/40] [Loss] train:-0.022(f:-0.169+r:0.035+k:0.000+l:0.113) val:-0.463 test:-0.467 [Accr] train:19.758% val:94.466% test:95.117% maxVal:94.727% maxTest:95.138%\n",
      "[26/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.372 [Accr] train:18.978% val:95.139% test:95.097% maxVal:95.226% maxTest:95.158%\n",
      "[34/40] [Loss] train:-0.010(f:-0.161+r:0.038+k:0.000+l:0.113) val:-0.409 test:-0.418 [Accr] train:19.409% val:94.596% test:95.086% maxVal:94.748% maxTest:95.127%\n",
      "[36/40] [Loss] train:-0.023(f:-0.170+r:0.034+k:0.000+l:0.113) val:-0.465 test:-0.464 [Accr] train:19.812% val:94.444% test:95.117% maxVal:94.727% maxTest:95.138%\n",
      "[31/40] [Loss] train:-0.004(f:-0.156+r:0.039+k:0.000+l:0.113) val:-0.387 test:-0.385 [Accr] train:19.097% val:95.009% test:95.076% maxVal:95.139% maxTest:95.179%\n",
      "[35/40] [Loss] train:-0.011(f:-0.161+r:0.038+k:0.000+l:0.113) val:-0.407 test:-0.419 [Accr] train:19.402% val:94.748% test:95.086% maxVal:94.748% maxTest:95.127%\n",
      "[27/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.370 test:-0.377 [Accr] train:18.958% val:95.117% test:95.179% maxVal:95.226% maxTest:95.158%\n",
      "[37/40] [Loss] train:-0.024(f:-0.171+r:0.034+k:0.000+l:0.113) val:-0.467 test:-0.468 [Accr] train:19.809% val:94.596% test:95.097% maxVal:94.727% maxTest:95.138%\n",
      "[32/40] [Loss] train:-0.003(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.389 test:-0.388 [Accr] train:19.093% val:95.030% test:95.148% maxVal:95.139% maxTest:95.179%\n",
      "[36/40] [Loss] train:-0.011(f:-0.162+r:0.038+k:0.000+l:0.113) val:-0.408 test:-0.413 [Accr] train:19.435% val:94.618% test:95.127% maxVal:94.748% maxTest:95.127%\n",
      "[38/40] [Loss] train:-0.023(f:-0.170+r:0.034+k:0.000+l:0.113) val:-0.463 test:-0.468 [Accr] train:19.770% val:94.531% test:95.066% maxVal:94.727% maxTest:95.138%\n",
      "[28/40] [Loss] train:-0.002(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.374 test:-0.374 [Accr] train:18.967% val:95.161% test:95.148% maxVal:95.226% maxTest:95.158%\n",
      "[33/40] [Loss] train:-0.003(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.386 test:-0.387 [Accr] train:19.091% val:95.009% test:95.107% maxVal:95.139% maxTest:95.179%\n",
      "[37/40] [Loss] train:-0.011(f:-0.162+r:0.038+k:0.000+l:0.113) val:-0.412 test:-0.420 [Accr] train:19.405% val:94.748% test:95.158% maxVal:94.748% maxTest:95.127%\n",
      "[39/40] [Loss] train:-0.022(f:-0.169+r:0.035+k:0.000+l:0.113) val:-0.461 test:-0.466 [Accr] train:19.778% val:94.618% test:95.138% maxVal:94.727% maxTest:95.138%\n",
      "[29/40] [Loss] train:-0.002(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.373 [Accr] train:18.962% val:95.269% test:95.241% maxVal:95.269% maxTest:95.241%\n",
      "[34/40] [Loss] train:-0.003(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.387 test:-0.385 [Accr] train:19.095% val:95.030% test:95.076% maxVal:95.139% maxTest:95.179%\n",
      "[38/40] [Loss] train:-0.011(f:-0.161+r:0.038+k:0.000+l:0.113) val:-0.405 test:-0.418 [Accr] train:19.387% val:94.618% test:95.117% maxVal:94.748% maxTest:95.127%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/40] [Loss] train:-0.023(f:-0.170+r:0.034+k:0.000+l:0.113) val:-0.465 test:-0.469 [Accr] train:19.789% val:94.510% test:95.107% maxVal:94.727% maxTest:95.138%\n",
      "Training finished.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "Text name: ../res/res_mnist_rs_err90_basic_kmix15_choiceNet.txt\n",
      "[35/40] [Loss] train:-0.003(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.387 test:-0.386 [Accr] train:19.066% val:95.009% test:95.148% maxVal:95.139% maxTest:95.179%\n",
      "[30/40] [Loss] train:-0.002(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.377 test:-0.378 [Accr] train:18.962% val:95.269% test:95.158% maxVal:95.269% maxTest:95.241%\n",
      "[39/40] [Loss] train:-0.010(f:-0.160+r:0.038+k:0.000+l:0.113) val:-0.407 test:-0.416 [Accr] train:19.413% val:94.770% test:95.169% maxVal:94.770% maxTest:95.169%\n",
      "[36/40] [Loss] train:-0.004(f:-0.156+r:0.039+k:0.000+l:0.113) val:-0.387 test:-0.386 [Accr] train:19.106% val:95.030% test:95.117% maxVal:95.139% maxTest:95.179%\n",
      "[40/40] [Loss] train:-0.011(f:-0.161+r:0.038+k:0.000+l:0.113) val:-0.412 test:-0.416 [Accr] train:19.391% val:94.683% test:95.117% maxVal:94.770% maxTest:95.169%\n",
      "Training finished.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "[00/40] [Loss] train:0.052(f:-0.115+r:0.050+k:0.002+l:0.116) val:-0.063 test:-0.070 [Accr] train:15.667% val:63.824% test:65.759% maxVal:63.824% maxTest:65.759%\n",
      "[31/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.369 test:-0.369 [Accr] train:19.000% val:95.161% test:95.179% maxVal:95.269% maxTest:95.241%\n",
      "Text name: ../res/res_mnist_rs_err90_basic_kmix20_choiceNet.txt\n",
      "[37/40] [Loss] train:-0.004(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.385 test:-0.385 [Accr] train:19.097% val:95.030% test:95.056% maxVal:95.139% maxTest:95.179%\n",
      "[32/40] [Loss] train:-0.001(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.372 test:-0.373 [Accr] train:18.991% val:95.204% test:95.230% maxVal:95.269% maxTest:95.241%\n",
      "[01/40] [Loss] train:0.045(f:-0.124+r:0.051+k:0.002+l:0.115) val:-0.150 test:-0.156 [Accr] train:17.211% val:79.514% test:80.705% maxVal:79.514% maxTest:80.705%\n",
      "[00/40] [Loss] train:0.053(f:-0.116+r:0.050+k:0.003+l:0.116) val:-0.067 test:-0.076 [Accr] train:15.928% val:66.319% test:67.948% maxVal:66.319% maxTest:67.948%\n",
      "[38/40] [Loss] train:-0.003(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.384 test:-0.384 [Accr] train:19.062% val:95.052% test:95.097% maxVal:95.139% maxTest:95.179%\n",
      "[33/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.368 test:-0.371 [Accr] train:19.004% val:95.182% test:95.199% maxVal:95.269% maxTest:95.241%\n",
      "[02/40] [Loss] train:0.040(f:-0.130+r:0.052+k:0.002+l:0.115) val:-0.207 test:-0.214 [Accr] train:17.790% val:84.549% test:85.835% maxVal:84.549% maxTest:85.835%\n",
      "[39/40] [Loss] train:-0.003(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.387 test:-0.389 [Accr] train:19.095% val:95.139% test:95.169% maxVal:95.139% maxTest:95.179%\n",
      "[01/40] [Loss] train:0.045(f:-0.125+r:0.052+k:0.003+l:0.115) val:-0.154 test:-0.161 [Accr] train:17.211% val:79.991% test:80.602% maxVal:79.991% maxTest:80.602%\n",
      "[34/40] [Loss] train:-0.001(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.370 test:-0.370 [Accr] train:18.976% val:95.161% test:95.189% maxVal:95.269% maxTest:95.241%\n",
      "[03/40] [Loss] train:0.036(f:-0.133+r:0.052+k:0.002+l:0.115) val:-0.231 test:-0.241 [Accr] train:18.038% val:87.695% test:88.466% maxVal:87.695% maxTest:88.466%\n",
      "[40/40] [Loss] train:-0.003(f:-0.155+r:0.039+k:0.000+l:0.113) val:-0.391 test:-0.386 [Accr] train:19.071% val:95.030% test:95.097% maxVal:95.139% maxTest:95.179%\n",
      "Training finished.\n",
      "[Worker_02] Done.\n",
      "[35/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.369 test:-0.373 [Accr] train:18.982% val:95.247% test:95.230% maxVal:95.269% maxTest:95.241%\n",
      "[02/40] [Loss] train:0.041(f:-0.130+r:0.053+k:0.003+l:0.115) val:-0.203 test:-0.211 [Accr] train:17.768% val:85.720% test:86.205% maxVal:85.720% maxTest:86.205%\n",
      "[04/40] [Loss] train:0.033(f:-0.136+r:0.052+k:0.002+l:0.115) val:-0.251 test:-0.264 [Accr] train:18.230% val:89.822% test:90.193% maxVal:89.822% maxTest:90.193%\n",
      "[36/40] [Loss] train:-0.002(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.370 test:-0.372 [Accr] train:19.000% val:95.204% test:95.220% maxVal:95.269% maxTest:95.241%\n",
      "[05/40] [Loss] train:0.031(f:-0.137+r:0.052+k:0.002+l:0.115) val:-0.270 test:-0.280 [Accr] train:18.264% val:90.907% test:91.160% maxVal:90.907% maxTest:91.160%\n",
      "[03/40] [Loss] train:0.038(f:-0.133+r:0.053+k:0.003+l:0.115) val:-0.232 test:-0.240 [Accr] train:18.069% val:88.563% test:88.888% maxVal:88.563% maxTest:88.888%\n",
      "[06/40] [Loss] train:0.029(f:-0.139+r:0.051+k:0.002+l:0.115) val:-0.284 test:-0.292 [Accr] train:18.405% val:92.057% test:92.064% maxVal:92.057% maxTest:92.064%\n",
      "[37/40] [Loss] train:-0.002(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.373 test:-0.375 [Accr] train:19.006% val:95.204% test:95.220% maxVal:95.269% maxTest:95.241%\n",
      "[04/40] [Loss] train:0.034(f:-0.136+r:0.052+k:0.003+l:0.115) val:-0.254 test:-0.263 [Accr] train:18.213% val:90.321% test:90.419% maxVal:90.321% maxTest:90.419%\n",
      "[07/40] [Loss] train:0.026(f:-0.141+r:0.051+k:0.002+l:0.115) val:-0.298 test:-0.304 [Accr] train:18.511% val:92.947% test:92.835% maxVal:92.947% maxTest:92.835%\n",
      "[38/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.373 test:-0.374 [Accr] train:18.958% val:95.139% test:95.189% maxVal:95.269% maxTest:95.241%\n",
      "[08/40] [Loss] train:0.024(f:-0.143+r:0.050+k:0.002+l:0.114) val:-0.310 test:-0.318 [Accr] train:18.533% val:93.229% test:93.072% maxVal:93.229% maxTest:93.072%\n",
      "[05/40] [Loss] train:0.032(f:-0.138+r:0.052+k:0.003+l:0.115) val:-0.272 test:-0.278 [Accr] train:18.348% val:91.580% test:91.488% maxVal:91.580% maxTest:91.488%\n",
      "[39/40] [Loss] train:-0.002(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.367 test:-0.370 [Accr] train:18.993% val:95.226% test:95.251% maxVal:95.269% maxTest:95.241%\n",
      "[09/40] [Loss] train:0.023(f:-0.143+r:0.050+k:0.002+l:0.114) val:-0.317 test:-0.322 [Accr] train:18.631% val:93.555% test:93.503% maxVal:93.555% maxTest:93.503%\n",
      "[40/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.374 test:-0.374 [Accr] train:18.973% val:95.161% test:95.210% maxVal:95.269% maxTest:95.241%\n",
      "Training finished.\n",
      "[Worker_03] Done.\n",
      "[06/40] [Loss] train:0.030(f:-0.140+r:0.052+k:0.003+l:0.115) val:-0.288 test:-0.294 [Accr] train:18.443% val:92.448% test:92.393% maxVal:92.448% maxTest:92.393%\n",
      "[10/40] [Loss] train:0.020(f:-0.145+r:0.049+k:0.002+l:0.114) val:-0.327 test:-0.335 [Accr] train:18.699% val:93.924% test:93.894% maxVal:93.924% maxTest:93.894%\n",
      "[07/40] [Loss] train:0.027(f:-0.142+r:0.051+k:0.003+l:0.115) val:-0.301 test:-0.307 [Accr] train:18.575% val:92.839% test:92.907% maxVal:92.839% maxTest:92.907%\n",
      "[11/40] [Loss] train:0.018(f:-0.146+r:0.048+k:0.002+l:0.114) val:-0.331 test:-0.339 [Accr] train:18.717% val:94.119% test:94.387% maxVal:94.119% maxTest:94.387%\n",
      "[08/40] [Loss] train:0.025(f:-0.143+r:0.050+k:0.003+l:0.114) val:-0.309 test:-0.316 [Accr] train:18.589% val:93.316% test:93.390% maxVal:93.316% maxTest:93.390%\n",
      "[12/40] [Loss] train:0.016(f:-0.148+r:0.048+k:0.002+l:0.114) val:-0.341 test:-0.349 [Accr] train:18.810% val:94.271% test:94.254% maxVal:94.271% maxTest:94.254%\n",
      "[13/40] [Loss] train:0.015(f:-0.148+r:0.047+k:0.002+l:0.114) val:-0.343 test:-0.348 [Accr] train:18.876% val:94.661% test:94.624% maxVal:94.661% maxTest:94.624%\n",
      "[09/40] [Loss] train:0.024(f:-0.143+r:0.050+k:0.003+l:0.114) val:-0.317 test:-0.322 [Accr] train:18.679% val:93.837% test:93.606% maxVal:93.837% maxTest:93.606%\n",
      "[14/40] [Loss] train:0.013(f:-0.149+r:0.046+k:0.002+l:0.113) val:-0.350 test:-0.354 [Accr] train:18.909% val:94.857% test:94.809% maxVal:94.857% maxTest:94.809%\n",
      "[10/40] [Loss] train:0.021(f:-0.145+r:0.049+k:0.003+l:0.114) val:-0.325 test:-0.331 [Accr] train:18.703% val:94.184% test:94.182% maxVal:94.184% maxTest:94.182%\n",
      "[15/40] [Loss] train:0.011(f:-0.149+r:0.045+k:0.002+l:0.113) val:-0.355 test:-0.358 [Accr] train:18.900% val:95.009% test:95.045% maxVal:95.009% maxTest:95.045%\n",
      "[11/40] [Loss] train:0.019(f:-0.146+r:0.048+k:0.003+l:0.114) val:-0.333 test:-0.337 [Accr] train:18.739% val:94.466% test:94.336% maxVal:94.466% maxTest:94.336%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/40] [Loss] train:0.009(f:-0.151+r:0.044+k:0.002+l:0.113) val:-0.356 test:-0.362 [Accr] train:18.956% val:95.095% test:95.045% maxVal:95.095% maxTest:95.045%\n",
      "[12/40] [Loss] train:0.016(f:-0.148+r:0.047+k:0.003+l:0.114) val:-0.342 test:-0.348 [Accr] train:18.790% val:94.423% test:94.398% maxVal:94.466% maxTest:94.336%\n",
      "[17/40] [Loss] train:0.007(f:-0.151+r:0.043+k:0.002+l:0.113) val:-0.363 test:-0.365 [Accr] train:18.958% val:94.987% test:95.076% maxVal:95.095% maxTest:95.045%\n",
      "[13/40] [Loss] train:0.015(f:-0.148+r:0.046+k:0.003+l:0.114) val:-0.338 test:-0.344 [Accr] train:18.849% val:94.835% test:94.706% maxVal:94.835% maxTest:94.706%\n",
      "[18/40] [Loss] train:0.005(f:-0.152+r:0.042+k:0.002+l:0.113) val:-0.371 test:-0.375 [Accr] train:18.993% val:95.182% test:95.395% maxVal:95.182% maxTest:95.395%\n",
      "[14/40] [Loss] train:0.012(f:-0.149+r:0.045+k:0.003+l:0.113) val:-0.345 test:-0.350 [Accr] train:18.891% val:94.835% test:94.737% maxVal:94.835% maxTest:94.706%\n",
      "[19/40] [Loss] train:0.004(f:-0.152+r:0.042+k:0.002+l:0.113) val:-0.364 test:-0.368 [Accr] train:19.035% val:95.464% test:95.354% maxVal:95.464% maxTest:95.354%\n",
      "[15/40] [Loss] train:0.011(f:-0.150+r:0.044+k:0.003+l:0.113) val:-0.350 test:-0.354 [Accr] train:18.896% val:94.922% test:94.932% maxVal:94.922% maxTest:94.932%\n",
      "[20/40] [Loss] train:0.003(f:-0.153+r:0.041+k:0.002+l:0.113) val:-0.369 test:-0.372 [Accr] train:19.048% val:95.399% test:95.415% maxVal:95.464% maxTest:95.354%\n",
      "[16/40] [Loss] train:0.008(f:-0.151+r:0.043+k:0.003+l:0.113) val:-0.348 test:-0.353 [Accr] train:18.976% val:94.748% test:94.850% maxVal:94.922% maxTest:94.932%\n",
      "[21/40] [Loss] train:0.002(f:-0.154+r:0.041+k:0.002+l:0.113) val:-0.375 test:-0.377 [Accr] train:19.031% val:95.312% test:95.415% maxVal:95.464% maxTest:95.354%\n",
      "[22/40] [Loss] train:0.002(f:-0.154+r:0.041+k:0.002+l:0.113) val:-0.373 test:-0.376 [Accr] train:19.035% val:95.312% test:95.384% maxVal:95.464% maxTest:95.354%\n",
      "[17/40] [Loss] train:0.007(f:-0.152+r:0.042+k:0.003+l:0.113) val:-0.357 test:-0.358 [Accr] train:19.015% val:95.095% test:95.179% maxVal:95.095% maxTest:95.179%\n",
      "[23/40] [Loss] train:0.002(f:-0.154+r:0.041+k:0.002+l:0.113) val:-0.370 test:-0.374 [Accr] train:19.060% val:95.399% test:95.436% maxVal:95.464% maxTest:95.354%\n",
      "[18/40] [Loss] train:0.005(f:-0.153+r:0.041+k:0.003+l:0.113) val:-0.359 test:-0.362 [Accr] train:19.017% val:95.161% test:95.374% maxVal:95.161% maxTest:95.374%\n",
      "[24/40] [Loss] train:0.002(f:-0.154+r:0.041+k:0.002+l:0.113) val:-0.371 test:-0.375 [Accr] train:19.029% val:95.378% test:95.446% maxVal:95.464% maxTest:95.354%\n",
      "[19/40] [Loss] train:0.002(f:-0.154+r:0.040+k:0.003+l:0.113) val:-0.362 test:-0.370 [Accr] train:19.057% val:95.204% test:95.446% maxVal:95.204% maxTest:95.446%\n",
      "[25/40] [Loss] train:0.002(f:-0.154+r:0.041+k:0.002+l:0.113) val:-0.372 test:-0.374 [Accr] train:19.069% val:95.443% test:95.456% maxVal:95.464% maxTest:95.354%\n",
      "[20/40] [Loss] train:0.002(f:-0.154+r:0.040+k:0.003+l:0.113) val:-0.358 test:-0.363 [Accr] train:19.091% val:95.226% test:95.467% maxVal:95.226% maxTest:95.467%\n",
      "[26/40] [Loss] train:0.001(f:-0.154+r:0.041+k:0.002+l:0.113) val:-0.373 test:-0.376 [Accr] train:19.082% val:95.421% test:95.467% maxVal:95.464% maxTest:95.354%\n",
      "[21/40] [Loss] train:0.001(f:-0.155+r:0.040+k:0.003+l:0.113) val:-0.363 test:-0.368 [Accr] train:19.068% val:95.204% test:95.436% maxVal:95.226% maxTest:95.467%\n",
      "[27/40] [Loss] train:0.001(f:-0.154+r:0.041+k:0.002+l:0.113) val:-0.373 test:-0.377 [Accr] train:19.082% val:95.312% test:95.539% maxVal:95.464% maxTest:95.354%\n",
      "[22/40] [Loss] train:0.001(f:-0.155+r:0.040+k:0.003+l:0.113) val:-0.364 test:-0.368 [Accr] train:19.097% val:95.182% test:95.426% maxVal:95.226% maxTest:95.467%\n",
      "[28/40] [Loss] train:0.001(f:-0.154+r:0.041+k:0.002+l:0.113) val:-0.371 test:-0.376 [Accr] train:19.060% val:95.356% test:95.508% maxVal:95.464% maxTest:95.354%\n",
      "[23/40] [Loss] train:0.001(f:-0.154+r:0.040+k:0.003+l:0.113) val:-0.360 test:-0.364 [Accr] train:19.086% val:95.247% test:95.446% maxVal:95.247% maxTest:95.446%\n",
      "[29/40] [Loss] train:0.001(f:-0.154+r:0.040+k:0.002+l:0.113) val:-0.369 test:-0.372 [Accr] train:19.068% val:95.356% test:95.467% maxVal:95.464% maxTest:95.354%\n",
      "[24/40] [Loss] train:0.001(f:-0.154+r:0.040+k:0.003+l:0.113) val:-0.359 test:-0.365 [Accr] train:19.069% val:95.161% test:95.477% maxVal:95.247% maxTest:95.446%\n",
      "[30/40] [Loss] train:0.000(f:-0.155+r:0.040+k:0.002+l:0.113) val:-0.375 test:-0.375 [Accr] train:19.099% val:95.378% test:95.446% maxVal:95.464% maxTest:95.354%\n",
      "[31/40] [Loss] train:0.000(f:-0.155+r:0.041+k:0.002+l:0.113) val:-0.372 test:-0.376 [Accr] train:19.108% val:95.443% test:95.456% maxVal:95.464% maxTest:95.354%\n",
      "[25/40] [Loss] train:0.001(f:-0.155+r:0.039+k:0.003+l:0.113) val:-0.362 test:-0.364 [Accr] train:19.122% val:95.182% test:95.456% maxVal:95.247% maxTest:95.446%\n",
      "[32/40] [Loss] train:0.001(f:-0.155+r:0.040+k:0.002+l:0.113) val:-0.373 test:-0.376 [Accr] train:19.091% val:95.399% test:95.539% maxVal:95.464% maxTest:95.354%\n",
      "[26/40] [Loss] train:-0.000(f:-0.155+r:0.039+k:0.003+l:0.113) val:-0.364 test:-0.367 [Accr] train:19.137% val:95.182% test:95.405% maxVal:95.247% maxTest:95.446%\n",
      "[33/40] [Loss] train:0.000(f:-0.155+r:0.040+k:0.002+l:0.113) val:-0.373 test:-0.376 [Accr] train:19.093% val:95.378% test:95.518% maxVal:95.464% maxTest:95.354%\n",
      "[34/40] [Loss] train:0.000(f:-0.155+r:0.040+k:0.002+l:0.113) val:-0.371 test:-0.375 [Accr] train:19.104% val:95.378% test:95.467% maxVal:95.464% maxTest:95.354%\n",
      "[28/40] [Loss] train:-0.000(f:-0.155+r:0.039+k:0.003+l:0.113) val:-0.361 test:-0.365 [Accr] train:19.131% val:95.182% test:95.456% maxVal:95.247% maxTest:95.446%\n",
      "[35/40] [Loss] train:0.001(f:-0.155+r:0.041+k:0.002+l:0.113) val:-0.368 test:-0.374 [Accr] train:19.082% val:95.399% test:95.518% maxVal:95.464% maxTest:95.354%\n",
      "[29/40] [Loss] train:-0.000(f:-0.155+r:0.039+k:0.003+l:0.113) val:-0.357 test:-0.361 [Accr] train:19.141% val:95.312% test:95.487% maxVal:95.312% maxTest:95.487%\n",
      "[36/40] [Loss] train:0.000(f:-0.155+r:0.040+k:0.002+l:0.113) val:-0.371 test:-0.374 [Accr] train:19.108% val:95.356% test:95.456% maxVal:95.464% maxTest:95.354%\n",
      "[30/40] [Loss] train:-0.000(f:-0.155+r:0.039+k:0.003+l:0.113) val:-0.359 test:-0.362 [Accr] train:19.153% val:95.226% test:95.498% maxVal:95.312% maxTest:95.487%\n",
      "[37/40] [Loss] train:0.001(f:-0.155+r:0.041+k:0.002+l:0.113) val:-0.372 test:-0.377 [Accr] train:19.117% val:95.443% test:95.549% maxVal:95.464% maxTest:95.354%\n",
      "[31/40] [Loss] train:-0.001(f:-0.156+r:0.039+k:0.003+l:0.113) val:-0.362 test:-0.366 [Accr] train:19.159% val:95.139% test:95.477% maxVal:95.312% maxTest:95.487%\n",
      "[38/40] [Loss] train:0.001(f:-0.154+r:0.040+k:0.002+l:0.113) val:-0.370 test:-0.372 [Accr] train:19.080% val:95.464% test:95.415% maxVal:95.464% maxTest:95.354%\n",
      "[32/40] [Loss] train:-0.001(f:-0.156+r:0.039+k:0.003+l:0.113) val:-0.360 test:-0.364 [Accr] train:19.144% val:95.247% test:95.456% maxVal:95.312% maxTest:95.487%\n",
      "[39/40] [Loss] train:0.000(f:-0.155+r:0.041+k:0.002+l:0.113) val:-0.369 test:-0.374 [Accr] train:19.084% val:95.443% test:95.487% maxVal:95.464% maxTest:95.354%\n",
      "[33/40] [Loss] train:-0.000(f:-0.155+r:0.039+k:0.003+l:0.113) val:-0.361 test:-0.364 [Accr] train:19.155% val:95.269% test:95.436% maxVal:95.312% maxTest:95.487%\n",
      "[40/40] [Loss] train:0.000(f:-0.155+r:0.040+k:0.002+l:0.113) val:-0.374 test:-0.375 [Accr] train:19.082% val:95.443% test:95.539% maxVal:95.464% maxTest:95.354%\n",
      "Training finished.\n",
      "[Worker_00] Done.\n",
      "[34/40] [Loss] train:-0.001(f:-0.155+r:0.039+k:0.003+l:0.113) val:-0.360 test:-0.365 [Accr] train:19.155% val:95.226% test:95.487% maxVal:95.312% maxTest:95.487%\n",
      "[35/40] [Loss] train:-0.000(f:-0.155+r:0.039+k:0.003+l:0.113) val:-0.361 test:-0.365 [Accr] train:19.144% val:95.226% test:95.498% maxVal:95.312% maxTest:95.487%\n",
      "[36/40] [Loss] train:-0.001(f:-0.155+r:0.039+k:0.003+l:0.113) val:-0.360 test:-0.363 [Accr] train:19.163% val:95.291% test:95.498% maxVal:95.312% maxTest:95.487%\n",
      "[37/40] [Loss] train:-0.000(f:-0.155+r:0.039+k:0.003+l:0.113) val:-0.360 test:-0.363 [Accr] train:19.170% val:95.247% test:95.498% maxVal:95.312% maxTest:95.487%\n",
      "[38/40] [Loss] train:-0.000(f:-0.155+r:0.039+k:0.003+l:0.113) val:-0.357 test:-0.361 [Accr] train:19.137% val:95.226% test:95.467% maxVal:95.312% maxTest:95.487%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39/40] [Loss] train:-0.001(f:-0.156+r:0.039+k:0.003+l:0.113) val:-0.358 test:-0.365 [Accr] train:19.146% val:95.204% test:95.487% maxVal:95.312% maxTest:95.487%\n",
      "[40/40] [Loss] train:-0.001(f:-0.156+r:0.039+k:0.003+l:0.113) val:-0.364 test:-0.366 [Accr] train:19.137% val:95.182% test:95.456% maxVal:95.312% maxTest:95.487%\n",
      "Training finished.\n",
      "[Worker_01] Done.\n"
     ]
    }
   ],
   "source": [
    "for i in range(nWorker):\n",
    "    WORKERS[i].start(); # Start process\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
