{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Classification (MNIST) Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded.\n"
     ]
    }
   ],
   "source": [
    "import nbloader,time\n",
    "from nips_cls_config_1 import worker_class\n",
    "if __name__ == \"__main__\": \n",
    "    print (\"Packages loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Worker_00] Instantiated.\n",
      "[Worker_01] Instantiated.\n",
      "[Worker_02] Instantiated.\n",
      "[Worker_03] Instantiated.\n"
     ]
    }
   ],
   "source": [
    "nWorker = 4\n",
    "maxGPU  = 2\n",
    "WORKERS = ['']*nWorker\n",
    "for i in range(nWorker):\n",
    "    WORKERS[i] = worker_class(_idx=i,_maxProcessID=nWorker,_maxGPU=maxGPU,_name='Worker_%02d'%(i)\n",
    "                              ,_period=1.0,_maxTick=10,_VERBOSE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting [Worker_00]\n",
      "processID:[0/4] GPU_ID:[0] #Config:[3]\n",
      "WARNING:tensorflow:From <string>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Starting [Worker_01]\n",
      "processID:[1/4] GPU_ID:[1] #Config:[3]\n",
      "WARNING:tensorflow:From <string>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Starting [Worker_02]\n",
      "processID:[2/4] GPU_ID:[0] #Config:[2]\n",
      "WARNING:tensorflow:From <string>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From <string>:224: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Starting [Worker_03]\n",
      "processID:[3/4] GPU_ID:[1] #Config:[2]\n",
      "WARNING:tensorflow:From <string>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <string>:224: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <string>:224: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From <string>:224: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "Text name: ../res/res_mnist_rs_err90_basic_tau_inv1_choiceNet.txt\n",
      "Text name: ../res/res_mnist_rs_err90_basic_tau_inv10_choiceNet.txt\n",
      "Text name: ../res/res_mnist_rs_err90_basic_tau_inv100_choiceNet.txt\n",
      "Text name: ../res/res_mnist_rs_err90_basic_tau_inv1000_choiceNet.txt\n",
      "[00/40] [Loss] train:0.079(f:-0.106+r:0.068+k:0.001+l:0.116) val:0.037 test:0.031 [Accr] train:14.086% val:48.806% test:50.740% maxVal:48.806% maxTest:50.740%\n",
      "[00/40] [Loss] train:0.055(f:-0.113+r:0.051+k:0.001+l:0.116) val:-0.039 test:-0.050 [Accr] train:15.163% val:57.574% test:59.981% maxVal:57.574% maxTest:59.981%\n",
      "[00/40] [Loss] train:0.504(f:-0.099+r:0.487+k:0.001+l:0.116) val:0.505 test:0.501 [Accr] train:10.832% val:16.102% test:17.712% maxVal:16.102% maxTest:17.712%\n",
      "[00/40] [Loss] train:0.176(f:-0.100+r:0.160+k:0.001+l:0.116) val:0.170 test:0.167 [Accr] train:12.372% val:28.277% test:31.538% maxVal:28.277% maxTest:31.538%\n",
      "[01/40] [Loss] train:0.074(f:-0.111+r:0.068+k:0.001+l:0.115) val:-0.024 test:-0.027 [Accr] train:16.160% val:69.379% test:70.857% maxVal:69.379% maxTest:70.857%\n",
      "[01/40] [Loss] train:0.048(f:-0.121+r:0.052+k:0.001+l:0.115) val:-0.130 test:-0.135 [Accr] train:16.833% val:75.543% test:77.292% maxVal:75.543% maxTest:77.292%\n",
      "[01/40] [Loss] train:0.502(f:-0.100+r:0.486+k:0.001+l:0.115) val:0.498 test:0.501 [Accr] train:11.907% val:25.825% test:28.331% maxVal:25.825% maxTest:28.331%\n",
      "[01/40] [Loss] train:0.174(f:-0.101+r:0.158+k:0.001+l:0.115) val:0.157 test:0.158 [Accr] train:14.329% val:51.128% test:53.546% maxVal:51.128% maxTest:53.546%\n",
      "[02/40] [Loss] train:0.069(f:-0.116+r:0.069+k:0.001+l:0.115) val:-0.061 test:-0.068 [Accr] train:16.990% val:76.953% test:78.187% maxVal:76.953% maxTest:78.187%\n",
      "[02/40] [Loss] train:0.042(f:-0.128+r:0.053+k:0.001+l:0.115) val:-0.187 test:-0.195 [Accr] train:17.410% val:82.205% test:82.987% maxVal:82.205% maxTest:82.987%\n",
      "[02/40] [Loss] train:0.171(f:-0.103+r:0.157+k:0.001+l:0.115) val:0.151 test:0.146 [Accr] train:15.260% val:61.914% test:64.042% maxVal:61.914% maxTest:64.042%\n",
      "[02/40] [Loss] train:0.500(f:-0.100+r:0.484+k:0.001+l:0.115) val:0.500 test:0.495 [Accr] train:13.278% val:40.864% test:43.185% maxVal:40.864% maxTest:43.185%\n",
      "[03/40] [Loss] train:0.066(f:-0.120+r:0.069+k:0.001+l:0.115) val:-0.089 test:-0.097 [Accr] train:17.299% val:80.990% test:82.319% maxVal:80.990% maxTest:82.319%\n",
      "[03/40] [Loss] train:0.039(f:-0.131+r:0.054+k:0.001+l:0.115) val:-0.217 test:-0.227 [Accr] train:17.704% val:85.221% test:85.650% maxVal:85.221% maxTest:85.650%\n",
      "[03/40] [Loss] train:0.169(f:-0.104+r:0.156+k:0.001+l:0.115) val:0.140 test:0.139 [Accr] train:15.963% val:68.012% test:70.611% maxVal:68.012% maxTest:70.611%\n",
      "[03/40] [Loss] train:0.498(f:-0.101+r:0.482+k:0.001+l:0.115) val:0.496 test:0.494 [Accr] train:13.969% val:48.741% test:51.007% maxVal:48.741% maxTest:51.007%\n",
      "[04/40] [Loss] train:0.063(f:-0.122+r:0.069+k:0.001+l:0.115) val:-0.113 test:-0.122 [Accr] train:17.574% val:83.659% test:84.344% maxVal:83.659% maxTest:84.344%\n",
      "[04/40] [Loss] train:0.036(f:-0.134+r:0.053+k:0.001+l:0.115) val:-0.235 test:-0.247 [Accr] train:17.976% val:87.522% test:87.685% maxVal:87.522% maxTest:87.685%\n",
      "[04/40] [Loss] train:0.168(f:-0.104+r:0.155+k:0.001+l:0.115) val:0.133 test:0.129 [Accr] train:16.406% val:71.398% test:73.941% maxVal:71.398% maxTest:73.941%\n",
      "[04/40] [Loss] train:0.497(f:-0.100+r:0.481+k:0.001+l:0.115) val:0.494 test:0.490 [Accr] train:14.491% val:53.255% test:55.140% maxVal:53.255% maxTest:55.140%\n",
      "[05/40] [Loss] train:0.061(f:-0.124+r:0.069+k:0.001+l:0.115) val:-0.134 test:-0.142 [Accr] train:17.660% val:85.200% test:85.999% maxVal:85.200% maxTest:85.999%\n",
      "[05/40] [Loss] train:0.034(f:-0.136+r:0.053+k:0.001+l:0.115) val:-0.261 test:-0.267 [Accr] train:18.106% val:89.084% test:89.052% maxVal:89.084% maxTest:89.052%\n",
      "[05/40] [Loss] train:0.165(f:-0.105+r:0.154+k:0.001+l:0.115) val:0.126 test:0.122 [Accr] train:16.487% val:73.329% test:75.699% maxVal:73.329% maxTest:75.699%\n",
      "[05/40] [Loss] train:0.494(f:-0.101+r:0.479+k:0.001+l:0.115) val:0.491 test:0.488 [Accr] train:14.908% val:59.722% test:59.920% maxVal:59.722% maxTest:59.920%\n",
      "[06/40] [Loss] train:0.059(f:-0.125+r:0.068+k:0.001+l:0.115) val:-0.146 test:-0.154 [Accr] train:17.883% val:87.131% test:87.747% maxVal:87.131% maxTest:87.747%\n",
      "[06/40] [Loss] train:0.031(f:-0.137+r:0.053+k:0.001+l:0.115) val:-0.270 test:-0.278 [Accr] train:18.246% val:90.668% test:90.687% maxVal:90.668% maxTest:90.687%\n",
      "[06/40] [Loss] train:0.163(f:-0.105+r:0.153+k:0.001+l:0.115) val:0.120 test:0.117 [Accr] train:16.753% val:76.194% test:78.320% maxVal:76.194% maxTest:78.320%\n",
      "[06/40] [Loss] train:0.492(f:-0.101+r:0.477+k:0.001+l:0.115) val:0.487 test:0.485 [Accr] train:15.229% val:62.131% test:62.911% maxVal:62.131% maxTest:62.911%\n",
      "[07/40] [Loss] train:0.056(f:-0.128+r:0.068+k:0.001+l:0.115) val:-0.162 test:-0.174 [Accr] train:17.965% val:88.346% test:88.775% maxVal:88.346% maxTest:88.775%\n",
      "[07/40] [Loss] train:0.029(f:-0.139+r:0.052+k:0.001+l:0.115) val:-0.284 test:-0.292 [Accr] train:18.352% val:91.471% test:91.447% maxVal:91.471% maxTest:91.447%\n",
      "[07/40] [Loss] train:0.161(f:-0.106+r:0.151+k:0.001+l:0.115) val:0.114 test:0.111 [Accr] train:16.910% val:78.646% test:79.862% maxVal:78.646% maxTest:79.862%\n",
      "[07/40] [Loss] train:0.489(f:-0.101+r:0.475+k:0.001+l:0.115) val:0.484 test:0.483 [Accr] train:15.351% val:64.280% test:64.751% maxVal:64.280% maxTest:64.751%\n",
      "[08/40] [Loss] train:0.055(f:-0.128+r:0.068+k:0.001+l:0.114) val:-0.178 test:-0.187 [Accr] train:18.058% val:89.714% test:89.505% maxVal:89.714% maxTest:89.505%\n",
      "[08/40] [Loss] train:0.027(f:-0.140+r:0.051+k:0.001+l:0.114) val:-0.302 test:-0.307 [Accr] train:18.378% val:92.079% test:91.807% maxVal:92.079% maxTest:91.807%\n",
      "[08/40] [Loss] train:0.160(f:-0.105+r:0.150+k:0.001+l:0.114) val:0.109 test:0.108 [Accr] train:17.093% val:80.425% test:81.682% maxVal:80.425% maxTest:81.682%\n",
      "[08/40] [Loss] train:0.489(f:-0.100+r:0.473+k:0.001+l:0.114) val:0.480 test:0.483 [Accr] train:15.578% val:65.755% test:67.280% maxVal:65.755% maxTest:67.280%\n",
      "[09/40] [Loss] train:0.052(f:-0.130+r:0.067+k:0.001+l:0.114) val:-0.192 test:-0.198 [Accr] train:18.058% val:89.887% test:90.121% maxVal:89.887% maxTest:90.121%\n",
      "[09/40] [Loss] train:0.025(f:-0.142+r:0.051+k:0.001+l:0.114) val:-0.310 test:-0.310 [Accr] train:18.473% val:92.556% test:92.434% maxVal:92.556% maxTest:92.434%\n",
      "[09/40] [Loss] train:0.157(f:-0.107+r:0.149+k:0.001+l:0.114) val:0.106 test:0.102 [Accr] train:17.182% val:81.402% test:82.175% maxVal:81.402% maxTest:82.175%\n",
      "[09/40] [Loss] train:0.485(f:-0.101+r:0.471+k:0.001+l:0.114) val:0.479 test:0.477 [Accr] train:15.747% val:67.752% test:69.356% maxVal:67.752% maxTest:69.356%\n",
      "[10/40] [Loss] train:0.051(f:-0.131+r:0.067+k:0.001+l:0.114) val:-0.202 test:-0.209 [Accr] train:18.153% val:90.799% test:90.718% maxVal:90.799% maxTest:90.718%\n",
      "[10/40] [Loss] train:0.023(f:-0.143+r:0.050+k:0.001+l:0.114) val:-0.317 test:-0.321 [Accr] train:18.549% val:92.882% test:92.784% maxVal:92.882% maxTest:92.784%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/40] [Loss] train:0.156(f:-0.107+r:0.148+k:0.001+l:0.114) val:0.101 test:0.100 [Accr] train:17.239% val:81.901% test:82.607% maxVal:81.901% maxTest:82.607%\n",
      "[10/40] [Loss] train:0.484(f:-0.101+r:0.470+k:0.001+l:0.114) val:0.478 test:0.477 [Accr] train:15.860% val:69.206% test:70.744% maxVal:69.206% maxTest:70.744%\n",
      "[11/40] [Loss] train:0.049(f:-0.132+r:0.066+k:0.001+l:0.114) val:-0.198 test:-0.213 [Accr] train:18.206% val:91.037% test:91.139% maxVal:91.037% maxTest:91.139%\n",
      "[11/40] [Loss] train:0.155(f:-0.106+r:0.146+k:0.001+l:0.114) val:0.103 test:0.095 [Accr] train:17.377% val:83.225% test:83.676% maxVal:83.225% maxTest:83.676%\n",
      "[11/40] [Loss] train:0.021(f:-0.143+r:0.049+k:0.001+l:0.114) val:-0.316 test:-0.324 [Accr] train:18.586% val:93.012% test:93.020% maxVal:93.012% maxTest:93.020%\n",
      "[11/40] [Loss] train:0.484(f:-0.100+r:0.469+k:0.001+l:0.114) val:0.479 test:0.473 [Accr] train:15.937% val:70.378% test:71.094% maxVal:70.378% maxTest:71.094%\n",
      "[12/40] [Loss] train:0.047(f:-0.134+r:0.065+k:0.001+l:0.114) val:-0.215 test:-0.228 [Accr] train:18.279% val:91.623% test:91.221% maxVal:91.623% maxTest:91.221%\n",
      "[12/40] [Loss] train:0.152(f:-0.108+r:0.144+k:0.001+l:0.114) val:0.097 test:0.092 [Accr] train:17.551% val:83.789% test:84.601% maxVal:83.789% maxTest:84.601%\n",
      "[12/40] [Loss] train:0.018(f:-0.145+r:0.049+k:0.001+l:0.114) val:-0.329 test:-0.336 [Accr] train:18.661% val:93.555% test:93.226% maxVal:93.555% maxTest:93.226%\n",
      "[12/40] [Loss] train:0.479(f:-0.101+r:0.466+k:0.001+l:0.114) val:0.475 test:0.471 [Accr] train:16.092% val:71.723% test:72.738% maxVal:71.723% maxTest:72.738%\n",
      "[13/40] [Loss] train:0.046(f:-0.134+r:0.065+k:0.001+l:0.114) val:-0.218 test:-0.228 [Accr] train:18.385% val:92.057% test:91.910% maxVal:92.057% maxTest:91.910%\n",
      "[13/40] [Loss] train:0.151(f:-0.107+r:0.143+k:0.001+l:0.114) val:0.096 test:0.090 [Accr] train:17.637% val:84.918% test:85.629% maxVal:84.918% maxTest:85.629%\n",
      "[13/40] [Loss] train:0.018(f:-0.145+r:0.047+k:0.001+l:0.114) val:-0.322 test:-0.329 [Accr] train:18.704% val:93.837% test:93.935% maxVal:93.837% maxTest:93.935%\n",
      "[13/40] [Loss] train:0.478(f:-0.101+r:0.464+k:0.001+l:0.114) val:0.472 test:0.471 [Accr] train:15.997% val:71.181% test:71.947% maxVal:71.723% maxTest:72.738%\n",
      "[14/40] [Loss] train:0.044(f:-0.135+r:0.064+k:0.001+l:0.113) val:-0.233 test:-0.237 [Accr] train:18.361% val:92.209% test:92.116% maxVal:92.209% maxTest:92.116%\n",
      "[14/40] [Loss] train:0.149(f:-0.107+r:0.142+k:0.001+l:0.113) val:0.090 test:0.089 [Accr] train:17.697% val:85.395% test:85.547% maxVal:85.395% maxTest:85.547%\n",
      "[14/40] [Loss] train:0.016(f:-0.146+r:0.047+k:0.001+l:0.113) val:-0.337 test:-0.339 [Accr] train:18.690% val:93.924% test:94.007% maxVal:93.924% maxTest:94.007%\n",
      "[14/40] [Loss] train:0.476(f:-0.101+r:0.463+k:0.001+l:0.113) val:0.467 test:0.467 [Accr] train:16.176% val:73.394% test:73.962% maxVal:73.394% maxTest:73.962%\n",
      "[15/40] [Loss] train:0.043(f:-0.134+r:0.063+k:0.001+l:0.113) val:-0.233 test:-0.240 [Accr] train:18.414% val:92.274% test:92.157% maxVal:92.274% maxTest:92.157%\n",
      "[15/40] [Loss] train:0.147(f:-0.107+r:0.140+k:0.001+l:0.113) val:0.087 test:0.084 [Accr] train:17.724% val:85.590% test:86.174% maxVal:85.590% maxTest:86.174%\n",
      "[15/40] [Loss] train:0.014(f:-0.146+r:0.046+k:0.001+l:0.113) val:-0.343 test:-0.344 [Accr] train:18.730% val:94.097% test:94.274% maxVal:94.097% maxTest:94.274%\n",
      "[15/40] [Loss] train:0.473(f:-0.101+r:0.460+k:0.001+l:0.113) val:0.465 test:0.465 [Accr] train:16.466% val:75.326% test:75.668% maxVal:75.326% maxTest:75.668%\n",
      "[16/40] [Loss] train:0.041(f:-0.136+r:0.063+k:0.001+l:0.113) val:-0.244 test:-0.252 [Accr] train:18.493% val:92.556% test:92.403% maxVal:92.556% maxTest:92.403%\n",
      "[16/40] [Loss] train:0.146(f:-0.107+r:0.139+k:0.001+l:0.113) val:0.086 test:0.079 [Accr] train:17.753% val:86.046% test:86.287% maxVal:86.046% maxTest:86.287%\n",
      "[16/40] [Loss] train:0.011(f:-0.148+r:0.045+k:0.001+l:0.113) val:-0.350 test:-0.352 [Accr] train:18.799% val:94.293% test:94.161% maxVal:94.293% maxTest:94.161%\n",
      "[16/40] [Loss] train:0.472(f:-0.101+r:0.459+k:0.001+l:0.113) val:0.465 test:0.461 [Accr] train:16.496% val:75.043% test:75.051% maxVal:75.326% maxTest:75.668%\n",
      "[17/40] [Loss] train:0.039(f:-0.137+r:0.061+k:0.001+l:0.113) val:-0.241 test:-0.252 [Accr] train:18.507% val:92.817% test:92.722% maxVal:92.817% maxTest:92.722%\n",
      "[17/40] [Loss] train:0.144(f:-0.108+r:0.137+k:0.001+l:0.113) val:0.084 test:0.079 [Accr] train:17.826% val:86.654% test:86.801% maxVal:86.654% maxTest:86.801%\n",
      "[17/40] [Loss] train:0.010(f:-0.148+r:0.044+k:0.001+l:0.113) val:-0.347 test:-0.351 [Accr] train:18.796% val:94.358% test:94.490% maxVal:94.358% maxTest:94.490%\n",
      "[17/40] [Loss] train:0.469(f:-0.101+r:0.456+k:0.001+l:0.113) val:0.463 test:0.460 [Accr] train:16.556% val:76.042% test:76.203% maxVal:76.042% maxTest:76.203%\n",
      "[18/40] [Loss] train:0.037(f:-0.138+r:0.061+k:0.001+l:0.113) val:-0.250 test:-0.260 [Accr] train:18.491% val:93.186% test:92.989% maxVal:93.186% maxTest:92.989%\n",
      "[18/40] [Loss] train:0.142(f:-0.108+r:0.136+k:0.001+l:0.113) val:0.082 test:0.075 [Accr] train:17.777% val:86.784% test:86.791% maxVal:86.784% maxTest:86.791%\n",
      "[18/40] [Loss] train:0.008(f:-0.149+r:0.043+k:0.001+l:0.113) val:-0.354 test:-0.359 [Accr] train:18.807% val:94.575% test:94.593% maxVal:94.575% maxTest:94.593%\n",
      "[18/40] [Loss] train:0.466(f:-0.102+r:0.454+k:0.001+l:0.113) val:0.462 test:0.459 [Accr] train:16.463% val:75.282% test:76.347% maxVal:76.042% maxTest:76.203%\n",
      "[19/40] [Loss] train:0.036(f:-0.137+r:0.060+k:0.001+l:0.113) val:-0.251 test:-0.261 [Accr] train:18.527% val:93.034% test:93.082% maxVal:93.186% maxTest:92.989%\n",
      "[19/40] [Loss] train:0.141(f:-0.108+r:0.135+k:0.001+l:0.113) val:0.083 test:0.076 [Accr] train:17.885% val:87.261% test:86.863% maxVal:87.261% maxTest:86.863%\n",
      "[19/40] [Loss] train:0.466(f:-0.101+r:0.454+k:0.001+l:0.113) val:0.463 test:0.456 [Accr] train:16.514% val:75.955% test:76.090% maxVal:76.042% maxTest:76.203%\n",
      "[19/40] [Loss] train:0.007(f:-0.149+r:0.041+k:0.001+l:0.113) val:-0.353 test:-0.355 [Accr] train:18.889% val:94.618% test:94.942% maxVal:94.618% maxTest:94.942%\n",
      "[20/40] [Loss] train:0.036(f:-0.138+r:0.060+k:0.001+l:0.113) val:-0.253 test:-0.261 [Accr] train:18.535% val:93.229% test:93.185% maxVal:93.229% maxTest:93.185%\n",
      "[20/40] [Loss] train:0.140(f:-0.108+r:0.134+k:0.001+l:0.113) val:0.078 test:0.073 [Accr] train:17.874% val:87.326% test:87.027% maxVal:87.326% maxTest:87.027%\n",
      "[20/40] [Loss] train:0.465(f:-0.101+r:0.452+k:0.001+l:0.113) val:0.457 test:0.455 [Accr] train:16.594% val:76.194% test:76.388% maxVal:76.194% maxTest:76.388%\n",
      "[20/40] [Loss] train:0.006(f:-0.149+r:0.041+k:0.001+l:0.113) val:-0.355 test:-0.358 [Accr] train:18.892% val:94.683% test:94.963% maxVal:94.683% maxTest:94.963%\n",
      "[21/40] [Loss] train:0.035(f:-0.138+r:0.060+k:0.001+l:0.113) val:-0.256 test:-0.263 [Accr] train:18.538% val:92.990% test:93.123% maxVal:93.229% maxTest:93.185%\n",
      "[21/40] [Loss] train:0.141(f:-0.107+r:0.134+k:0.001+l:0.113) val:0.079 test:0.075 [Accr] train:17.885% val:87.370% test:87.058% maxVal:87.370% maxTest:87.058%\n",
      "[21/40] [Loss] train:0.466(f:-0.101+r:0.452+k:0.001+l:0.113) val:0.458 test:0.455 [Accr] train:16.578% val:76.476% test:76.593% maxVal:76.476% maxTest:76.593%\n",
      "[21/40] [Loss] train:0.005(f:-0.150+r:0.041+k:0.001+l:0.113) val:-0.358 test:-0.360 [Accr] train:18.870% val:94.618% test:94.963% maxVal:94.683% maxTest:94.963%\n",
      "[22/40] [Loss] train:0.036(f:-0.138+r:0.060+k:0.001+l:0.113) val:-0.255 test:-0.263 [Accr] train:18.557% val:93.229% test:93.215% maxVal:93.229% maxTest:93.185%\n",
      "[22/40] [Loss] train:0.141(f:-0.107+r:0.134+k:0.001+l:0.113) val:0.080 test:0.074 [Accr] train:17.936% val:87.565% test:87.305% maxVal:87.565% maxTest:87.305%\n",
      "[22/40] [Loss] train:0.465(f:-0.100+r:0.452+k:0.001+l:0.113) val:0.459 test:0.455 [Accr] train:16.614% val:76.324% test:76.778% maxVal:76.476% maxTest:76.593%\n",
      "[22/40] [Loss] train:0.005(f:-0.150+r:0.041+k:0.001+l:0.113) val:-0.355 test:-0.359 [Accr] train:18.880% val:94.661% test:94.953% maxVal:94.683% maxTest:94.963%\n",
      "[23/40] [Loss] train:0.035(f:-0.138+r:0.060+k:0.001+l:0.113) val:-0.257 test:-0.265 [Accr] train:18.555% val:93.186% test:93.298% maxVal:93.229% maxTest:93.185%\n",
      "[23/40] [Loss] train:0.141(f:-0.107+r:0.134+k:0.001+l:0.113) val:0.077 test:0.074 [Accr] train:17.887% val:87.587% test:87.294% maxVal:87.587% maxTest:87.294%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/40] [Loss] train:0.465(f:-0.101+r:0.452+k:0.001+l:0.113) val:0.457 test:0.455 [Accr] train:16.673% val:76.541% test:76.963% maxVal:76.541% maxTest:76.963%\n",
      "[23/40] [Loss] train:0.005(f:-0.150+r:0.041+k:0.001+l:0.113) val:-0.357 test:-0.360 [Accr] train:18.876% val:94.683% test:94.963% maxVal:94.683% maxTest:94.963%\n",
      "[24/40] [Loss] train:0.035(f:-0.138+r:0.060+k:0.001+l:0.113) val:-0.260 test:-0.264 [Accr] train:18.524% val:93.316% test:93.339% maxVal:93.316% maxTest:93.339%\n",
      "[24/40] [Loss] train:0.140(f:-0.108+r:0.134+k:0.001+l:0.113) val:0.078 test:0.073 [Accr] train:17.870% val:87.522% test:87.192% maxVal:87.587% maxTest:87.294%\n",
      "[24/40] [Loss] train:0.464(f:-0.101+r:0.451+k:0.001+l:0.113) val:0.459 test:0.456 [Accr] train:16.642% val:76.432% test:77.005% maxVal:76.541% maxTest:76.963%\n",
      "[24/40] [Loss] train:0.005(f:-0.150+r:0.041+k:0.001+l:0.113) val:-0.360 test:-0.360 [Accr] train:18.858% val:94.661% test:95.014% maxVal:94.683% maxTest:94.963%\n",
      "[25/40] [Loss] train:0.035(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.256 test:-0.265 [Accr] train:18.558% val:93.359% test:93.359% maxVal:93.359% maxTest:93.359%\n",
      "[25/40] [Loss] train:0.140(f:-0.107+r:0.134+k:0.001+l:0.113) val:0.078 test:0.073 [Accr] train:17.914% val:87.457% test:87.325% maxVal:87.587% maxTest:87.294%\n",
      "[25/40] [Loss] train:0.465(f:-0.101+r:0.452+k:0.001+l:0.113) val:0.459 test:0.455 [Accr] train:16.662% val:76.693% test:77.107% maxVal:76.693% maxTest:77.107%\n",
      "[25/40] [Loss] train:0.004(f:-0.150+r:0.040+k:0.001+l:0.113) val:-0.357 test:-0.360 [Accr] train:18.885% val:94.683% test:95.014% maxVal:94.683% maxTest:94.963%\n",
      "[26/40] [Loss] train:0.035(f:-0.138+r:0.059+k:0.001+l:0.113) val:-0.257 test:-0.266 [Accr] train:18.578% val:93.338% test:93.318% maxVal:93.359% maxTest:93.359%\n",
      "[26/40] [Loss] train:0.141(f:-0.107+r:0.134+k:0.001+l:0.113) val:0.077 test:0.072 [Accr] train:17.960% val:87.695% test:87.325% maxVal:87.695% maxTest:87.325%\n",
      "[26/40] [Loss] train:0.465(f:-0.100+r:0.452+k:0.001+l:0.113) val:0.459 test:0.453 [Accr] train:16.609% val:76.649% test:77.015% maxVal:76.693% maxTest:77.107%\n",
      "[26/40] [Loss] train:0.004(f:-0.150+r:0.040+k:0.001+l:0.113) val:-0.359 test:-0.360 [Accr] train:18.905% val:94.705% test:94.994% maxVal:94.705% maxTest:94.994%\n",
      "[27/40] [Loss] train:0.034(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.260 test:-0.271 [Accr] train:18.555% val:93.338% test:93.400% maxVal:93.359% maxTest:93.359%\n",
      "[27/40] [Loss] train:0.139(f:-0.108+r:0.133+k:0.001+l:0.113) val:0.075 test:0.070 [Accr] train:17.960% val:87.674% test:87.356% maxVal:87.695% maxTest:87.325%\n",
      "[27/40] [Loss] train:0.464(f:-0.100+r:0.451+k:0.001+l:0.113) val:0.455 test:0.454 [Accr] train:16.611% val:76.736% test:77.118% maxVal:76.736% maxTest:77.118%\n",
      "[27/40] [Loss] train:0.004(f:-0.151+r:0.040+k:0.001+l:0.113) val:-0.359 test:-0.366 [Accr] train:18.894% val:94.661% test:95.035% maxVal:94.705% maxTest:94.994%\n",
      "[28/40] [Loss] train:0.034(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.261 test:-0.265 [Accr] train:18.553% val:93.468% test:93.472% maxVal:93.468% maxTest:93.472%\n",
      "[28/40] [Loss] train:0.139(f:-0.108+r:0.133+k:0.001+l:0.113) val:0.077 test:0.072 [Accr] train:17.952% val:87.912% test:87.562% maxVal:87.912% maxTest:87.562%\n",
      "[28/40] [Loss] train:0.463(f:-0.101+r:0.450+k:0.001+l:0.113) val:0.456 test:0.454 [Accr] train:16.629% val:76.931% test:77.272% maxVal:76.931% maxTest:77.272%\n",
      "[28/40] [Loss] train:0.003(f:-0.151+r:0.040+k:0.001+l:0.113) val:-0.361 test:-0.363 [Accr] train:18.900% val:94.770% test:95.097% maxVal:94.770% maxTest:95.097%\n",
      "[29/40] [Loss] train:0.034(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.259 test:-0.266 [Accr] train:18.564% val:93.641% test:93.555% maxVal:93.641% maxTest:93.555%\n",
      "[29/40] [Loss] train:0.139(f:-0.108+r:0.133+k:0.001+l:0.113) val:0.076 test:0.071 [Accr] train:17.927% val:87.717% test:87.582% maxVal:87.912% maxTest:87.562%\n",
      "[29/40] [Loss] train:0.463(f:-0.101+r:0.450+k:0.001+l:0.113) val:0.454 test:0.455 [Accr] train:16.671% val:76.953% test:77.416% maxVal:76.953% maxTest:77.416%\n",
      "[29/40] [Loss] train:0.003(f:-0.151+r:0.040+k:0.001+l:0.113) val:-0.359 test:-0.362 [Accr] train:18.911% val:94.813% test:95.158% maxVal:94.813% maxTest:95.158%\n",
      "[30/40] [Loss] train:0.033(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.264 test:-0.273 [Accr] train:18.536% val:93.381% test:93.359% maxVal:93.641% maxTest:93.555%\n",
      "[30/40] [Loss] train:0.138(f:-0.108+r:0.132+k:0.001+l:0.113) val:0.072 test:0.068 [Accr] train:17.914% val:87.478% test:87.438% maxVal:87.912% maxTest:87.562%\n",
      "[30/40] [Loss] train:0.462(f:-0.101+r:0.450+k:0.001+l:0.113) val:0.456 test:0.454 [Accr] train:16.609% val:76.780% test:77.179% maxVal:76.953% maxTest:77.416%\n",
      "[30/40] [Loss] train:0.003(f:-0.151+r:0.040+k:0.001+l:0.113) val:-0.364 test:-0.366 [Accr] train:18.887% val:94.683% test:95.035% maxVal:94.813% maxTest:95.158%\n",
      "[31/40] [Loss] train:0.034(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.261 test:-0.268 [Accr] train:18.582% val:93.424% test:93.421% maxVal:93.641% maxTest:93.555%\n",
      "[31/40] [Loss] train:0.138(f:-0.108+r:0.132+k:0.001+l:0.113) val:0.074 test:0.071 [Accr] train:17.949% val:87.674% test:87.582% maxVal:87.912% maxTest:87.562%\n",
      "[31/40] [Loss] train:0.462(f:-0.101+r:0.449+k:0.001+l:0.113) val:0.456 test:0.454 [Accr] train:16.633% val:76.519% test:77.066% maxVal:76.953% maxTest:77.416%\n",
      "[31/40] [Loss] train:0.004(f:-0.150+r:0.040+k:0.001+l:0.113) val:-0.358 test:-0.360 [Accr] train:18.938% val:94.705% test:95.086% maxVal:94.813% maxTest:95.158%\n",
      "[32/40] [Loss] train:0.034(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.263 test:-0.269 [Accr] train:18.578% val:93.576% test:93.472% maxVal:93.641% maxTest:93.555%\n",
      "[32/40] [Loss] train:0.138(f:-0.108+r:0.132+k:0.001+l:0.113) val:0.075 test:0.071 [Accr] train:17.976% val:87.977% test:87.541% maxVal:87.977% maxTest:87.541%\n",
      "[32/40] [Loss] train:0.462(f:-0.101+r:0.450+k:0.001+l:0.113) val:0.455 test:0.453 [Accr] train:16.627% val:76.801% test:77.159% maxVal:76.953% maxTest:77.416%\n",
      "[32/40] [Loss] train:0.003(f:-0.150+r:0.040+k:0.001+l:0.113) val:-0.360 test:-0.362 [Accr] train:18.911% val:94.813% test:95.189% maxVal:94.813% maxTest:95.158%\n",
      "[33/40] [Loss] train:0.034(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.256 test:-0.268 [Accr] train:18.584% val:93.511% test:93.524% maxVal:93.641% maxTest:93.555%\n",
      "[33/40] [Loss] train:0.139(f:-0.108+r:0.133+k:0.001+l:0.113) val:0.079 test:0.071 [Accr] train:17.965% val:87.847% test:87.582% maxVal:87.977% maxTest:87.541%\n",
      "[33/40] [Loss] train:0.463(f:-0.101+r:0.450+k:0.001+l:0.113) val:0.458 test:0.453 [Accr] train:16.620% val:76.866% test:77.148% maxVal:76.953% maxTest:77.416%\n",
      "[33/40] [Loss] train:0.003(f:-0.151+r:0.040+k:0.001+l:0.113) val:-0.356 test:-0.361 [Accr] train:18.929% val:94.748% test:95.199% maxVal:94.813% maxTest:95.158%\n",
      "[34/40] [Loss] train:0.034(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.258 test:-0.267 [Accr] train:18.584% val:93.468% test:93.442% maxVal:93.641% maxTest:93.555%\n",
      "[34/40] [Loss] train:0.139(f:-0.107+r:0.133+k:0.001+l:0.113) val:0.078 test:0.071 [Accr] train:17.969% val:87.587% test:87.510% maxVal:87.977% maxTest:87.541%\n",
      "[34/40] [Loss] train:0.463(f:-0.101+r:0.450+k:0.001+l:0.113) val:0.458 test:0.452 [Accr] train:16.686% val:76.866% test:77.447% maxVal:76.953% maxTest:77.416%\n",
      "[34/40] [Loss] train:0.004(f:-0.150+r:0.040+k:0.001+l:0.113) val:-0.357 test:-0.360 [Accr] train:18.923% val:94.683% test:95.066% maxVal:94.813% maxTest:95.158%\n",
      "[35/40] [Loss] train:0.034(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.259 test:-0.267 [Accr] train:18.573% val:93.555% test:93.472% maxVal:93.641% maxTest:93.555%\n",
      "[35/40] [Loss] train:0.139(f:-0.108+r:0.133+k:0.001+l:0.113) val:0.078 test:0.071 [Accr] train:17.949% val:87.804% test:87.675% maxVal:87.977% maxTest:87.541%\n",
      "[35/40] [Loss] train:0.004(f:-0.150+r:0.040+k:0.001+l:0.113) val:-0.356 test:-0.361 [Accr] train:18.900% val:94.770% test:95.179% maxVal:94.813% maxTest:95.158%\n",
      "[35/40] [Loss] train:0.464(f:-0.101+r:0.451+k:0.001+l:0.113) val:0.458 test:0.453 [Accr] train:16.649% val:76.801% test:77.179% maxVal:76.953% maxTest:77.416%\n",
      "[36/40] [Loss] train:0.139(f:-0.108+r:0.133+k:0.001+l:0.113) val:0.077 test:0.071 [Accr] train:17.972% val:87.609% test:87.500% maxVal:87.977% maxTest:87.541%\n",
      "[36/40] [Loss] train:0.034(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.259 test:-0.268 [Accr] train:18.593% val:93.424% test:93.411% maxVal:93.641% maxTest:93.555%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/40] [Loss] train:0.003(f:-0.150+r:0.040+k:0.001+l:0.113) val:-0.359 test:-0.362 [Accr] train:18.927% val:94.640% test:95.107% maxVal:94.813% maxTest:95.158%\n",
      "[36/40] [Loss] train:0.464(f:-0.101+r:0.451+k:0.001+l:0.113) val:0.457 test:0.453 [Accr] train:16.676% val:76.606% test:77.169% maxVal:76.953% maxTest:77.416%\n",
      "[37/40] [Loss] train:0.137(f:-0.108+r:0.132+k:0.001+l:0.113) val:0.071 test:0.067 [Accr] train:17.971% val:87.717% test:87.551% maxVal:87.977% maxTest:87.541%\n",
      "[37/40] [Loss] train:0.033(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.264 test:-0.271 [Accr] train:18.575% val:93.424% test:93.503% maxVal:93.641% maxTest:93.555%\n",
      "[37/40] [Loss] train:0.003(f:-0.151+r:0.040+k:0.001+l:0.113) val:-0.362 test:-0.365 [Accr] train:18.931% val:94.748% test:95.117% maxVal:94.813% maxTest:95.158%\n",
      "[38/40] [Loss] train:0.138(f:-0.108+r:0.132+k:0.001+l:0.113) val:0.073 test:0.069 [Accr] train:17.940% val:87.609% test:87.603% maxVal:87.977% maxTest:87.541%\n",
      "[37/40] [Loss] train:0.462(f:-0.101+r:0.449+k:0.001+l:0.113) val:0.454 test:0.452 [Accr] train:16.660% val:76.671% test:77.169% maxVal:76.953% maxTest:77.416%\n",
      "[38/40] [Loss] train:0.003(f:-0.150+r:0.040+k:0.001+l:0.113) val:-0.361 test:-0.363 [Accr] train:18.892% val:94.683% test:95.066% maxVal:94.813% maxTest:95.158%\n",
      "[38/40] [Loss] train:0.034(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.263 test:-0.269 [Accr] train:18.555% val:93.424% test:93.493% maxVal:93.641% maxTest:93.555%\n",
      "[39/40] [Loss] train:0.139(f:-0.108+r:0.133+k:0.001+l:0.113) val:0.076 test:0.072 [Accr] train:17.960% val:87.760% test:87.613% maxVal:87.977% maxTest:87.541%\n",
      "[39/40] [Loss] train:0.003(f:-0.151+r:0.040+k:0.001+l:0.113) val:-0.356 test:-0.359 [Accr] train:18.905% val:94.813% test:95.117% maxVal:94.813% maxTest:95.158%\n",
      "[38/40] [Loss] train:0.463(f:-0.101+r:0.450+k:0.001+l:0.113) val:0.456 test:0.453 [Accr] train:16.678% val:76.866% test:77.426% maxVal:76.953% maxTest:77.416%\n",
      "[39/40] [Loss] train:0.033(f:-0.139+r:0.059+k:0.001+l:0.113) val:-0.261 test:-0.266 [Accr] train:18.571% val:93.576% test:93.503% maxVal:93.641% maxTest:93.555%\n",
      "[40/40] [Loss] train:0.138(f:-0.108+r:0.133+k:0.001+l:0.113) val:0.074 test:0.070 [Accr] train:17.954% val:87.674% test:87.541% maxVal:87.977% maxTest:87.541%\n",
      "Training finished.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "Text name: ../res/res_mnist_rs_err90_basic_tau_inv1e-06_choiceNet.txt\n",
      "[40/40] [Loss] train:0.003(f:-0.151+r:0.040+k:0.001+l:0.113) val:-0.362 test:-0.363 [Accr] train:18.907% val:94.683% test:95.169% maxVal:94.813% maxTest:95.158%\n",
      "Training finished.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "Text name: ../res/res_mnist_rs_err90_basic_tau_inv1e-10_choiceNet.txt\n",
      "[39/40] [Loss] train:0.463(f:-0.101+r:0.450+k:0.001+l:0.113) val:0.455 test:0.455 [Accr] train:16.673% val:76.823% test:77.200% maxVal:76.953% maxTest:77.416%\n",
      "[00/40] [Loss] train:0.051(f:-0.114+r:0.049+k:0.001+l:0.116) val:-0.059 test:-0.070 [Accr] train:15.311% val:58.919% test:61.493% maxVal:58.919% maxTest:61.493%\n",
      "[40/40] [Loss] train:0.033(f:-0.140+r:0.059+k:0.001+l:0.113) val:-0.263 test:-0.269 [Accr] train:18.562% val:93.490% test:93.472% maxVal:93.641% maxTest:93.555%\n",
      "Training finished.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "Text name: ../res/res_mnist_rs_err90_basic_tau_inv1e-08_choiceNet.txt\n",
      "[00/40] [Loss] train:0.051(f:-0.115+r:0.049+k:0.001+l:0.116) val:-0.059 test:-0.070 [Accr] train:15.269% val:59.375% test:61.256% maxVal:59.375% maxTest:61.256%\n",
      "[40/40] [Loss] train:0.464(f:-0.101+r:0.451+k:0.001+l:0.113) val:0.454 test:0.453 [Accr] train:16.613% val:76.432% test:76.943% maxVal:76.953% maxTest:77.416%\n",
      "Training finished.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "Text name: ../res/res_mnist_rs_err90_basic_tau_inv1e-04_choiceNet.txt\n",
      "[01/40] [Loss] train:0.043(f:-0.123+r:0.050+k:0.001+l:0.115) val:-0.152 test:-0.156 [Accr] train:16.987% val:77.127% test:78.598% maxVal:77.127% maxTest:78.598%\n",
      "[00/40] [Loss] train:0.051(f:-0.114+r:0.049+k:0.001+l:0.116) val:-0.059 test:-0.070 [Accr] train:15.298% val:59.288% test:61.451% maxVal:59.288% maxTest:61.451%\n",
      "[01/40] [Loss] train:0.043(f:-0.123+r:0.050+k:0.001+l:0.115) val:-0.151 test:-0.155 [Accr] train:16.939% val:77.279% test:78.403% maxVal:77.279% maxTest:78.403%\n",
      "[00/40] [Loss] train:0.051(f:-0.114+r:0.049+k:0.001+l:0.116) val:-0.059 test:-0.070 [Accr] train:15.249% val:58.854% test:61.092% maxVal:58.854% maxTest:61.092%\n",
      "[02/40] [Loss] train:0.037(f:-0.130+r:0.051+k:0.001+l:0.115) val:-0.210 test:-0.216 [Accr] train:17.534% val:83.290% test:84.272% maxVal:83.290% maxTest:84.272%\n",
      "[01/40] [Loss] train:0.043(f:-0.123+r:0.050+k:0.001+l:0.115) val:-0.151 test:-0.155 [Accr] train:16.979% val:77.300% test:78.238% maxVal:77.300% maxTest:78.238%\n",
      "[02/40] [Loss] train:0.038(f:-0.130+r:0.051+k:0.001+l:0.115) val:-0.208 test:-0.215 [Accr] train:17.571% val:83.073% test:84.426% maxVal:83.073% maxTest:84.426%\n",
      "[01/40] [Loss] train:0.043(f:-0.123+r:0.050+k:0.001+l:0.115) val:-0.151 test:-0.155 [Accr] train:16.965% val:77.170% test:78.289% maxVal:77.170% maxTest:78.289%\n",
      "[03/40] [Loss] train:0.034(f:-0.133+r:0.051+k:0.001+l:0.115) val:-0.237 test:-0.246 [Accr] train:17.788% val:86.003% test:86.904% maxVal:86.003% maxTest:86.904%\n",
      "[02/40] [Loss] train:0.037(f:-0.130+r:0.051+k:0.001+l:0.115) val:-0.208 test:-0.215 [Accr] train:17.571% val:83.312% test:84.067% maxVal:83.312% maxTest:84.067%\n",
      "[03/40] [Loss] train:0.034(f:-0.133+r:0.051+k:0.001+l:0.115) val:-0.237 test:-0.246 [Accr] train:17.821% val:85.959% test:86.688% maxVal:85.959% maxTest:86.688%\n",
      "[02/40] [Loss] train:0.037(f:-0.130+r:0.051+k:0.001+l:0.115) val:-0.209 test:-0.216 [Accr] train:17.522% val:83.225% test:83.995% maxVal:83.225% maxTest:83.995%\n",
      "[04/40] [Loss] train:0.032(f:-0.135+r:0.051+k:0.001+l:0.115) val:-0.255 test:-0.265 [Accr] train:18.097% val:88.346% test:88.528% maxVal:88.346% maxTest:88.528%\n",
      "[03/40] [Loss] train:0.034(f:-0.133+r:0.051+k:0.001+l:0.115) val:-0.236 test:-0.245 [Accr] train:17.863% val:85.959% test:86.688% maxVal:85.959% maxTest:86.688%\n",
      "[04/40] [Loss] train:0.032(f:-0.135+r:0.051+k:0.001+l:0.115) val:-0.255 test:-0.266 [Accr] train:18.067% val:88.194% test:88.435% maxVal:88.194% maxTest:88.435%\n",
      "[03/40] [Loss] train:0.034(f:-0.133+r:0.051+k:0.001+l:0.115) val:-0.236 test:-0.246 [Accr] train:17.768% val:85.786% test:86.513% maxVal:85.786% maxTest:86.513%\n",
      "[05/40] [Loss] train:0.029(f:-0.137+r:0.051+k:0.001+l:0.115) val:-0.281 test:-0.284 [Accr] train:18.148% val:89.670% test:89.648% maxVal:89.670% maxTest:89.648%\n",
      "[04/40] [Loss] train:0.032(f:-0.135+r:0.051+k:0.001+l:0.115) val:-0.255 test:-0.265 [Accr] train:18.071% val:87.999% test:88.466% maxVal:87.999% maxTest:88.466%\n",
      "[05/40] [Loss] train:0.030(f:-0.137+r:0.051+k:0.001+l:0.115) val:-0.279 test:-0.283 [Accr] train:18.177% val:89.692% test:89.844% maxVal:89.692% maxTest:89.844%\n",
      "[04/40] [Loss] train:0.032(f:-0.135+r:0.051+k:0.001+l:0.115) val:-0.254 test:-0.264 [Accr] train:18.071% val:88.043% test:88.528% maxVal:88.043% maxTest:88.528%\n",
      "[06/40] [Loss] train:0.027(f:-0.139+r:0.050+k:0.001+l:0.115) val:-0.289 test:-0.296 [Accr] train:18.328% val:91.406% test:91.283% maxVal:91.406% maxTest:91.283%\n",
      "[05/40] [Loss] train:0.030(f:-0.137+r:0.051+k:0.001+l:0.115) val:-0.277 test:-0.282 [Accr] train:18.190% val:89.735% test:89.556% maxVal:89.735% maxTest:89.556%\n",
      "[06/40] [Loss] train:0.027(f:-0.139+r:0.050+k:0.001+l:0.115) val:-0.289 test:-0.297 [Accr] train:18.308% val:91.254% test:91.190% maxVal:91.254% maxTest:91.190%\n",
      "[05/40] [Loss] train:0.030(f:-0.137+r:0.051+k:0.001+l:0.115) val:-0.278 test:-0.283 [Accr] train:18.193% val:89.779% test:89.731% maxVal:89.779% maxTest:89.731%\n",
      "[07/40] [Loss] train:0.025(f:-0.141+r:0.049+k:0.001+l:0.115) val:-0.301 test:-0.308 [Accr] train:18.445% val:92.101% test:91.920% maxVal:92.101% maxTest:91.920%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/40] [Loss] train:0.027(f:-0.139+r:0.050+k:0.001+l:0.115) val:-0.287 test:-0.295 [Accr] train:18.348% val:91.189% test:91.108% maxVal:91.189% maxTest:91.108%\n",
      "[07/40] [Loss] train:0.025(f:-0.140+r:0.049+k:0.001+l:0.115) val:-0.299 test:-0.307 [Accr] train:18.473% val:91.905% test:91.920% maxVal:91.905% maxTest:91.920%\n",
      "[06/40] [Loss] train:0.027(f:-0.139+r:0.050+k:0.001+l:0.115) val:-0.289 test:-0.296 [Accr] train:18.348% val:91.341% test:91.221% maxVal:91.341% maxTest:91.221%\n",
      "[08/40] [Loss] train:0.023(f:-0.142+r:0.049+k:0.001+l:0.114) val:-0.319 test:-0.324 [Accr] train:18.504% val:92.535% test:92.290% maxVal:92.535% maxTest:92.290%\n",
      "[07/40] [Loss] train:0.025(f:-0.140+r:0.049+k:0.001+l:0.115) val:-0.299 test:-0.307 [Accr] train:18.449% val:92.188% test:91.910% maxVal:92.188% maxTest:91.910%\n",
      "[08/40] [Loss] train:0.023(f:-0.142+r:0.049+k:0.001+l:0.114) val:-0.319 test:-0.323 [Accr] train:18.474% val:92.513% test:92.444% maxVal:92.513% maxTest:92.444%\n",
      "[07/40] [Loss] train:0.025(f:-0.141+r:0.050+k:0.001+l:0.115) val:-0.302 test:-0.309 [Accr] train:18.460% val:92.144% test:91.838% maxVal:92.144% maxTest:91.838%\n",
      "[09/40] [Loss] train:0.020(f:-0.143+r:0.048+k:0.001+l:0.114) val:-0.326 test:-0.327 [Accr] train:18.578% val:93.034% test:92.743% maxVal:93.034% maxTest:92.743%\n",
      "[08/40] [Loss] train:0.023(f:-0.142+r:0.049+k:0.001+l:0.114) val:-0.318 test:-0.323 [Accr] train:18.520% val:92.904% test:92.321% maxVal:92.904% maxTest:92.321%\n",
      "[09/40] [Loss] train:0.021(f:-0.143+r:0.048+k:0.001+l:0.114) val:-0.325 test:-0.325 [Accr] train:18.544% val:92.795% test:92.876% maxVal:92.795% maxTest:92.876%\n",
      "[08/40] [Loss] train:0.023(f:-0.142+r:0.049+k:0.001+l:0.114) val:-0.317 test:-0.322 [Accr] train:18.505% val:92.817% test:92.444% maxVal:92.817% maxTest:92.444%\n",
      "[10/40] [Loss] train:0.018(f:-0.144+r:0.047+k:0.001+l:0.114) val:-0.336 test:-0.337 [Accr] train:18.617% val:93.490% test:93.267% maxVal:93.490% maxTest:93.267%\n",
      "[09/40] [Loss] train:0.021(f:-0.143+r:0.048+k:0.001+l:0.114) val:-0.323 test:-0.324 [Accr] train:18.591% val:93.164% test:92.743% maxVal:93.164% maxTest:92.743%\n",
      "[10/40] [Loss] train:0.018(f:-0.144+r:0.047+k:0.001+l:0.114) val:-0.336 test:-0.338 [Accr] train:18.624% val:93.338% test:93.318% maxVal:93.338% maxTest:93.318%\n",
      "[09/40] [Loss] train:0.021(f:-0.143+r:0.048+k:0.001+l:0.114) val:-0.324 test:-0.324 [Accr] train:18.598% val:93.012% test:92.979% maxVal:93.012% maxTest:92.979%\n",
      "[11/40] [Loss] train:0.016(f:-0.145+r:0.046+k:0.001+l:0.114) val:-0.336 test:-0.341 [Accr] train:18.657% val:93.598% test:93.390% maxVal:93.598% maxTest:93.390%\n",
      "[10/40] [Loss] train:0.018(f:-0.144+r:0.047+k:0.001+l:0.114) val:-0.334 test:-0.336 [Accr] train:18.628% val:93.641% test:93.215% maxVal:93.641% maxTest:93.215%\n",
      "[11/40] [Loss] train:0.017(f:-0.145+r:0.046+k:0.001+l:0.114) val:-0.334 test:-0.339 [Accr] train:18.670% val:93.273% test:93.524% maxVal:93.338% maxTest:93.318%\n",
      "[10/40] [Loss] train:0.018(f:-0.144+r:0.047+k:0.001+l:0.114) val:-0.334 test:-0.335 [Accr] train:18.655% val:93.490% test:93.329% maxVal:93.490% maxTest:93.329%\n",
      "[12/40] [Loss] train:0.014(f:-0.146+r:0.046+k:0.001+l:0.114) val:-0.343 test:-0.347 [Accr] train:18.750% val:93.750% test:93.565% maxVal:93.750% maxTest:93.565%\n",
      "[11/40] [Loss] train:0.017(f:-0.145+r:0.046+k:0.001+l:0.114) val:-0.332 test:-0.338 [Accr] train:18.615% val:93.641% test:93.400% maxVal:93.641% maxTest:93.215%\n",
      "[12/40] [Loss] train:0.014(f:-0.146+r:0.045+k:0.001+l:0.114) val:-0.342 test:-0.347 [Accr] train:18.743% val:93.641% test:93.709% maxVal:93.641% maxTest:93.709%\n",
      "[11/40] [Loss] train:0.017(f:-0.145+r:0.046+k:0.001+l:0.114) val:-0.332 test:-0.337 [Accr] train:18.672% val:93.576% test:93.483% maxVal:93.576% maxTest:93.483%\n",
      "[13/40] [Loss] train:0.013(f:-0.146+r:0.044+k:0.001+l:0.114) val:-0.336 test:-0.341 [Accr] train:18.794% val:94.076% test:94.058% maxVal:94.076% maxTest:94.058%\n",
      "[12/40] [Loss] train:0.014(f:-0.146+r:0.046+k:0.001+l:0.114) val:-0.342 test:-0.347 [Accr] train:18.741% val:93.880% test:93.524% maxVal:93.880% maxTest:93.524%\n",
      "[13/40] [Loss] train:0.013(f:-0.146+r:0.044+k:0.001+l:0.114) val:-0.334 test:-0.340 [Accr] train:18.776% val:94.076% test:94.038% maxVal:94.076% maxTest:94.038%\n",
      "[12/40] [Loss] train:0.014(f:-0.146+r:0.045+k:0.001+l:0.114) val:-0.341 test:-0.346 [Accr] train:18.759% val:93.924% test:93.709% maxVal:93.924% maxTest:93.709%\n",
      "[14/40] [Loss] train:0.011(f:-0.147+r:0.043+k:0.001+l:0.113) val:-0.349 test:-0.350 [Accr] train:18.781% val:94.271% test:94.202% maxVal:94.271% maxTest:94.202%\n",
      "[13/40] [Loss] train:0.013(f:-0.146+r:0.044+k:0.001+l:0.114) val:-0.335 test:-0.342 [Accr] train:18.812% val:94.314% test:94.161% maxVal:94.314% maxTest:94.161%\n",
      "[14/40] [Loss] train:0.012(f:-0.147+r:0.044+k:0.001+l:0.113) val:-0.347 test:-0.349 [Accr] train:18.783% val:94.206% test:94.202% maxVal:94.206% maxTest:94.202%\n",
      "[13/40] [Loss] train:0.013(f:-0.146+r:0.044+k:0.001+l:0.114) val:-0.337 test:-0.344 [Accr] train:18.765% val:94.184% test:94.233% maxVal:94.184% maxTest:94.233%\n",
      "[15/40] [Loss] train:0.009(f:-0.147+r:0.042+k:0.001+l:0.113) val:-0.357 test:-0.357 [Accr] train:18.810% val:94.444% test:94.243% maxVal:94.444% maxTest:94.243%\n",
      "[14/40] [Loss] train:0.011(f:-0.147+r:0.044+k:0.001+l:0.113) val:-0.349 test:-0.351 [Accr] train:18.810% val:94.227% test:94.223% maxVal:94.314% maxTest:94.161%\n",
      "[15/40] [Loss] train:0.010(f:-0.147+r:0.042+k:0.001+l:0.113) val:-0.356 test:-0.356 [Accr] train:18.797% val:94.206% test:94.387% maxVal:94.206% maxTest:94.202%\n",
      "[14/40] [Loss] train:0.011(f:-0.147+r:0.044+k:0.001+l:0.113) val:-0.347 test:-0.348 [Accr] train:18.807% val:94.271% test:94.161% maxVal:94.271% maxTest:94.161%\n",
      "[16/40] [Loss] train:0.007(f:-0.149+r:0.041+k:0.001+l:0.113) val:-0.362 test:-0.361 [Accr] train:18.887% val:94.596% test:94.572% maxVal:94.596% maxTest:94.572%\n",
      "[15/40] [Loss] train:0.009(f:-0.148+r:0.042+k:0.001+l:0.113) val:-0.357 test:-0.358 [Accr] train:18.814% val:94.553% test:94.213% maxVal:94.553% maxTest:94.213%\n",
      "[16/40] [Loss] train:0.006(f:-0.149+r:0.041+k:0.001+l:0.113) val:-0.365 test:-0.365 [Accr] train:18.887% val:94.444% test:94.511% maxVal:94.444% maxTest:94.511%\n",
      "[15/40] [Loss] train:0.010(f:-0.147+r:0.042+k:0.001+l:0.113) val:-0.354 test:-0.355 [Accr] train:18.814% val:94.444% test:94.531% maxVal:94.444% maxTest:94.531%\n",
      "[17/40] [Loss] train:0.005(f:-0.149+r:0.040+k:0.001+l:0.113) val:-0.365 test:-0.366 [Accr] train:18.885% val:94.531% test:94.593% maxVal:94.596% maxTest:94.572%\n",
      "[16/40] [Loss] train:0.007(f:-0.149+r:0.041+k:0.001+l:0.113) val:-0.362 test:-0.362 [Accr] train:18.902% val:94.575% test:94.583% maxVal:94.575% maxTest:94.583%\n",
      "[17/40] [Loss] train:0.005(f:-0.149+r:0.040+k:0.001+l:0.113) val:-0.360 test:-0.362 [Accr] train:18.902% val:94.683% test:94.449% maxVal:94.683% maxTest:94.449%\n",
      "[16/40] [Loss] train:0.006(f:-0.149+r:0.041+k:0.001+l:0.113) val:-0.364 test:-0.365 [Accr] train:18.891% val:94.618% test:94.747% maxVal:94.618% maxTest:94.747%\n",
      "[18/40] [Loss] train:0.003(f:-0.150+r:0.039+k:0.001+l:0.113) val:-0.367 test:-0.370 [Accr] train:18.863% val:94.640% test:94.737% maxVal:94.640% maxTest:94.737%\n",
      "[17/40] [Loss] train:0.005(f:-0.149+r:0.040+k:0.001+l:0.113) val:-0.361 test:-0.364 [Accr] train:18.887% val:94.661% test:94.644% maxVal:94.661% maxTest:94.644%\n",
      "[18/40] [Loss] train:0.003(f:-0.150+r:0.039+k:0.001+l:0.113) val:-0.365 test:-0.370 [Accr] train:18.892% val:94.878% test:94.829% maxVal:94.878% maxTest:94.829%\n",
      "[17/40] [Loss] train:0.005(f:-0.149+r:0.040+k:0.001+l:0.113) val:-0.361 test:-0.363 [Accr] train:18.878% val:94.661% test:94.665% maxVal:94.661% maxTest:94.665%\n",
      "[19/40] [Loss] train:0.001(f:-0.150+r:0.038+k:0.001+l:0.113) val:-0.368 test:-0.370 [Accr] train:18.912% val:94.987% test:95.035% maxVal:94.987% maxTest:95.035%\n",
      "[18/40] [Loss] train:0.003(f:-0.150+r:0.039+k:0.001+l:0.113) val:-0.368 test:-0.374 [Accr] train:18.880% val:94.900% test:94.829% maxVal:94.900% maxTest:94.829%\n",
      "[19/40] [Loss] train:0.001(f:-0.150+r:0.037+k:0.001+l:0.113) val:-0.367 test:-0.370 [Accr] train:18.918% val:95.009% test:95.066% maxVal:95.009% maxTest:95.066%\n",
      "[18/40] [Loss] train:0.003(f:-0.150+r:0.039+k:0.001+l:0.113) val:-0.370 test:-0.375 [Accr] train:18.885% val:95.009% test:94.984% maxVal:95.009% maxTest:94.984%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/40] [Loss] train:0.001(f:-0.150+r:0.038+k:0.001+l:0.113) val:-0.369 test:-0.370 [Accr] train:18.953% val:95.030% test:95.086% maxVal:95.030% maxTest:95.086%\n",
      "[19/40] [Loss] train:0.001(f:-0.150+r:0.038+k:0.001+l:0.113) val:-0.367 test:-0.370 [Accr] train:18.956% val:95.052% test:95.127% maxVal:95.052% maxTest:95.127%\n",
      "[20/40] [Loss] train:0.001(f:-0.150+r:0.037+k:0.001+l:0.113) val:-0.367 test:-0.370 [Accr] train:18.940% val:95.009% test:95.086% maxVal:95.009% maxTest:95.066%\n",
      "[19/40] [Loss] train:0.001(f:-0.150+r:0.037+k:0.001+l:0.113) val:-0.369 test:-0.372 [Accr] train:18.896% val:95.030% test:95.271% maxVal:95.030% maxTest:95.271%\n",
      "[21/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.373 test:-0.372 [Accr] train:18.943% val:94.965% test:94.932% maxVal:95.030% maxTest:95.086%\n",
      "[20/40] [Loss] train:0.001(f:-0.150+r:0.038+k:0.001+l:0.113) val:-0.366 test:-0.370 [Accr] train:18.993% val:95.182% test:95.138% maxVal:95.182% maxTest:95.138%\n",
      "[21/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.372 [Accr] train:18.920% val:94.900% test:95.004% maxVal:95.009% maxTest:95.066%\n",
      "[20/40] [Loss] train:0.001(f:-0.150+r:0.037+k:0.001+l:0.113) val:-0.369 test:-0.370 [Accr] train:18.914% val:95.161% test:95.230% maxVal:95.161% maxTest:95.230%\n",
      "[22/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.369 test:-0.370 [Accr] train:18.942% val:94.987% test:94.912% maxVal:95.030% maxTest:95.086%\n",
      "[21/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.372 [Accr] train:18.969% val:95.161% test:95.035% maxVal:95.182% maxTest:95.138%\n",
      "[22/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.369 test:-0.371 [Accr] train:18.938% val:94.900% test:95.025% maxVal:95.009% maxTest:95.066%\n",
      "[21/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.372 [Accr] train:18.911% val:95.139% test:95.117% maxVal:95.161% maxTest:95.230%\n",
      "[23/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.371 [Accr] train:18.956% val:95.009% test:95.025% maxVal:95.030% maxTest:95.086%\n",
      "[22/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.367 test:-0.370 [Accr] train:18.975% val:95.204% test:95.056% maxVal:95.204% maxTest:95.056%\n",
      "[23/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.369 test:-0.371 [Accr] train:18.949% val:94.944% test:95.138% maxVal:95.009% maxTest:95.066%\n",
      "[22/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.369 test:-0.371 [Accr] train:18.931% val:95.139% test:95.127% maxVal:95.161% maxTest:95.230%\n",
      "[24/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.375 test:-0.373 [Accr] train:18.925% val:94.922% test:95.004% maxVal:95.030% maxTest:95.086%\n",
      "[23/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.368 test:-0.371 [Accr] train:18.991% val:95.247% test:95.086% maxVal:95.247% maxTest:95.086%\n",
      "[24/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.374 test:-0.373 [Accr] train:18.927% val:94.878% test:95.107% maxVal:95.009% maxTest:95.066%\n",
      "[23/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.370 test:-0.371 [Accr] train:18.934% val:95.030% test:95.158% maxVal:95.161% maxTest:95.230%\n",
      "[25/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.371 [Accr] train:18.980% val:95.009% test:95.014% maxVal:95.030% maxTest:95.086%\n",
      "[24/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.373 test:-0.373 [Accr] train:18.960% val:95.334% test:95.066% maxVal:95.334% maxTest:95.066%\n",
      "[25/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.371 [Accr] train:18.985% val:94.944% test:95.045% maxVal:95.009% maxTest:95.066%\n",
      "[26/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.373 test:-0.373 [Accr] train:18.989% val:94.922% test:95.127% maxVal:95.009% maxTest:95.066%\n",
      "[25/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.371 [Accr] train:18.960% val:95.052% test:95.179% maxVal:95.161% maxTest:95.230%\n",
      "[27/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.377 [Accr] train:18.962% val:95.009% test:95.107% maxVal:95.030% maxTest:95.086%\n",
      "[26/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.373 [Accr] train:19.009% val:95.182% test:95.086% maxVal:95.334% maxTest:95.066%\n",
      "[27/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.378 [Accr] train:18.980% val:94.878% test:95.097% maxVal:95.009% maxTest:95.066%\n",
      "[26/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.373 [Accr] train:18.973% val:95.030% test:95.076% maxVal:95.161% maxTest:95.230%\n",
      "[28/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.375 test:-0.375 [Accr] train:18.962% val:94.944% test:95.025% maxVal:95.030% maxTest:95.086%\n",
      "[27/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.378 [Accr] train:19.017% val:95.247% test:95.169% maxVal:95.334% maxTest:95.066%\n",
      "[28/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.376 test:-0.376 [Accr] train:18.975% val:94.922% test:95.127% maxVal:95.009% maxTest:95.066%\n",
      "[27/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.378 [Accr] train:18.967% val:95.117% test:95.210% maxVal:95.161% maxTest:95.230%\n",
      "[29/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.374 test:-0.374 [Accr] train:18.993% val:94.987% test:95.035% maxVal:95.030% maxTest:95.086%\n",
      "[28/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.374 test:-0.375 [Accr] train:19.002% val:95.356% test:95.117% maxVal:95.356% maxTest:95.117%\n",
      "[29/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.373 test:-0.375 [Accr] train:18.975% val:95.074% test:95.323% maxVal:95.074% maxTest:95.323%\n",
      "[28/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.375 test:-0.375 [Accr] train:18.954% val:95.074% test:95.189% maxVal:95.161% maxTest:95.230%\n",
      "[30/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.379 test:-0.378 [Accr] train:18.984% val:94.944% test:94.994% maxVal:95.030% maxTest:95.086%\n",
      "[29/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.371 test:-0.374 [Accr] train:18.998% val:95.464% test:95.179% maxVal:95.464% maxTest:95.179%\n",
      "[30/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.379 test:-0.380 [Accr] train:18.984% val:94.922% test:95.097% maxVal:95.074% maxTest:95.323%\n",
      "[29/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.373 test:-0.374 [Accr] train:18.943% val:95.052% test:95.292% maxVal:95.161% maxTest:95.230%\n",
      "[31/40] [Loss] train:-0.001(f:-0.151+r:0.036+k:0.001+l:0.113) val:-0.371 test:-0.370 [Accr] train:19.006% val:95.074% test:95.066% maxVal:95.074% maxTest:95.066%\n",
      "[30/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.377 test:-0.379 [Accr] train:18.996% val:95.312% test:95.169% maxVal:95.464% maxTest:95.179%\n",
      "[31/40] [Loss] train:-0.001(f:-0.151+r:0.036+k:0.001+l:0.113) val:-0.371 test:-0.372 [Accr] train:19.013% val:94.965% test:95.189% maxVal:95.074% maxTest:95.323%\n",
      "[30/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.378 test:-0.378 [Accr] train:18.969% val:95.052% test:95.210% maxVal:95.161% maxTest:95.230%\n",
      "[32/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.373 test:-0.374 [Accr] train:18.996% val:94.987% test:95.076% maxVal:95.074% maxTest:95.066%\n",
      "[31/40] [Loss] train:-0.001(f:-0.151+r:0.036+k:0.001+l:0.113) val:-0.370 test:-0.371 [Accr] train:19.027% val:95.312% test:95.117% maxVal:95.464% maxTest:95.179%\n",
      "[32/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.374 test:-0.375 [Accr] train:19.011% val:94.965% test:95.210% maxVal:95.074% maxTest:95.323%\n",
      "[31/40] [Loss] train:-0.001(f:-0.151+r:0.036+k:0.001+l:0.113) val:-0.371 test:-0.370 [Accr] train:18.989% val:94.965% test:95.210% maxVal:95.161% maxTest:95.230%\n",
      "[33/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.369 test:-0.371 [Accr] train:18.996% val:95.009% test:95.066% maxVal:95.074% maxTest:95.066%\n",
      "[32/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.372 test:-0.374 [Accr] train:19.033% val:95.399% test:95.138% maxVal:95.464% maxTest:95.179%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.369 test:-0.372 [Accr] train:19.024% val:94.965% test:95.189% maxVal:95.074% maxTest:95.323%\n",
      "[34/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.371 test:-0.371 [Accr] train:19.009% val:94.922% test:95.014% maxVal:95.074% maxTest:95.066%\n",
      "[32/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.373 test:-0.374 [Accr] train:18.973% val:95.074% test:95.251% maxVal:95.161% maxTest:95.230%\n",
      "[33/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.367 test:-0.371 [Accr] train:19.037% val:95.334% test:95.169% maxVal:95.464% maxTest:95.179%\n",
      "[34/40] [Loss] train:-0.001(f:-0.151+r:0.036+k:0.001+l:0.113) val:-0.370 test:-0.373 [Accr] train:19.006% val:94.965% test:95.199% maxVal:95.074% maxTest:95.323%\n",
      "[35/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.370 test:-0.373 [Accr] train:18.987% val:94.944% test:95.004% maxVal:95.074% maxTest:95.066%\n",
      "[35/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.370 test:-0.374 [Accr] train:18.984% val:95.009% test:95.169% maxVal:95.074% maxTest:95.323%\n",
      "[33/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.368 test:-0.371 [Accr] train:18.985% val:95.052% test:95.251% maxVal:95.161% maxTest:95.230%\n",
      "[34/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.369 test:-0.372 [Accr] train:19.011% val:95.291% test:95.138% maxVal:95.464% maxTest:95.179%\n",
      "[36/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.372 test:-0.373 [Accr] train:19.009% val:94.878% test:95.035% maxVal:95.074% maxTest:95.066%\n",
      "[36/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.372 test:-0.375 [Accr] train:19.009% val:94.965% test:95.148% maxVal:95.074% maxTest:95.323%\n",
      "[37/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.374 test:-0.375 [Accr] train:19.013% val:94.965% test:95.086% maxVal:95.074% maxTest:95.066%\n",
      "[34/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.370 test:-0.372 [Accr] train:18.984% val:94.922% test:95.220% maxVal:95.161% maxTest:95.230%\n",
      "[35/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.369 test:-0.374 [Accr] train:19.004% val:95.378% test:95.148% maxVal:95.464% maxTest:95.179%\n",
      "[37/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.374 test:-0.377 [Accr] train:19.024% val:95.030% test:95.179% maxVal:95.074% maxTest:95.323%\n",
      "[38/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.373 test:-0.374 [Accr] train:18.993% val:94.857% test:95.086% maxVal:95.074% maxTest:95.066%\n",
      "[38/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.374 test:-0.376 [Accr] train:18.985% val:95.095% test:95.148% maxVal:95.095% maxTest:95.148%\n",
      "[36/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.370 test:-0.374 [Accr] train:19.035% val:95.312% test:95.117% maxVal:95.464% maxTest:95.179%\n",
      "[35/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.369 test:-0.373 [Accr] train:18.954% val:95.117% test:95.271% maxVal:95.161% maxTest:95.230%\n",
      "[39/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.368 test:-0.369 [Accr] train:18.995% val:95.052% test:95.056% maxVal:95.074% maxTest:95.066%\n",
      "[39/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.368 test:-0.371 [Accr] train:18.976% val:95.117% test:95.220% maxVal:95.117% maxTest:95.220%\n",
      "[40/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.375 test:-0.374 [Accr] train:18.987% val:94.922% test:95.014% maxVal:95.074% maxTest:95.066%\n",
      "Training finished.\n",
      "[Worker_02] Done.\n",
      "[40/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.375 test:-0.375 [Accr] train:18.995% val:94.965% test:95.127% maxVal:95.117% maxTest:95.220%\n",
      "Training finished.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "[37/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.373 test:-0.377 [Accr] train:19.038% val:95.378% test:95.210% maxVal:95.464% maxTest:95.179%\n",
      "[36/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.371 test:-0.373 [Accr] train:18.991% val:94.987% test:95.199% maxVal:95.161% maxTest:95.230%\n",
      "Text name: ../res/res_mnist_rs_err90_basic_tau_inv1e-02_choiceNet.txt\n",
      "[00/40] [Loss] train:0.051(f:-0.115+r:0.049+k:0.001+l:0.116) val:-0.059 test:-0.070 [Accr] train:15.342% val:59.266% test:61.585% maxVal:59.266% maxTest:61.585%\n",
      "[01/40] [Loss] train:0.043(f:-0.123+r:0.050+k:0.001+l:0.115) val:-0.152 test:-0.156 [Accr] train:17.010% val:77.517% test:78.433% maxVal:77.517% maxTest:78.433%\n",
      "[38/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.372 test:-0.374 [Accr] train:19.002% val:95.334% test:95.169% maxVal:95.464% maxTest:95.179%\n",
      "[37/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.374 test:-0.376 [Accr] train:18.995% val:95.009% test:95.261% maxVal:95.161% maxTest:95.230%\n",
      "[02/40] [Loss] train:0.037(f:-0.130+r:0.051+k:0.001+l:0.115) val:-0.209 test:-0.216 [Accr] train:17.582% val:83.312% test:84.015% maxVal:83.312% maxTest:84.015%\n",
      "[03/40] [Loss] train:0.034(f:-0.133+r:0.051+k:0.001+l:0.115) val:-0.237 test:-0.246 [Accr] train:17.819% val:86.480% test:86.832% maxVal:86.480% maxTest:86.832%\n",
      "[04/40] [Loss] train:0.032(f:-0.135+r:0.051+k:0.001+l:0.115) val:-0.255 test:-0.265 [Accr] train:18.062% val:88.281% test:88.528% maxVal:88.281% maxTest:88.528%\n",
      "[39/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.367 test:-0.371 [Accr] train:19.013% val:95.356% test:95.210% maxVal:95.464% maxTest:95.179%\n",
      "[38/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.373 test:-0.374 [Accr] train:18.969% val:95.052% test:95.230% maxVal:95.161% maxTest:95.230%\n",
      "[05/40] [Loss] train:0.030(f:-0.137+r:0.051+k:0.001+l:0.115) val:-0.278 test:-0.281 [Accr] train:18.182% val:89.844% test:89.926% maxVal:89.844% maxTest:89.926%\n",
      "[06/40] [Loss] train:0.027(f:-0.139+r:0.050+k:0.001+l:0.115) val:-0.289 test:-0.296 [Accr] train:18.367% val:91.037% test:91.201% maxVal:91.037% maxTest:91.201%\n",
      "[07/40] [Loss] train:0.025(f:-0.140+r:0.049+k:0.001+l:0.115) val:-0.299 test:-0.307 [Accr] train:18.460% val:92.079% test:91.817% maxVal:92.079% maxTest:91.817%\n",
      "[40/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.374 test:-0.375 [Accr] train:19.018% val:95.356% test:95.169% maxVal:95.464% maxTest:95.179%\n",
      "Training finished.\n",
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "[39/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.368 test:-0.370 [Accr] train:18.976% val:94.944% test:95.220% maxVal:95.161% maxTest:95.230%\n",
      "[08/40] [Loss] train:0.023(f:-0.142+r:0.049+k:0.001+l:0.114) val:-0.319 test:-0.324 [Accr] train:18.509% val:92.578% test:92.486% maxVal:92.578% maxTest:92.486%\n",
      "Text name: ../res/res_mnist_rs_err90_basic_tau_inv1e-01_choiceNet.txt\n",
      "[09/40] [Loss] train:0.021(f:-0.143+r:0.048+k:0.001+l:0.114) val:-0.324 test:-0.325 [Accr] train:18.571% val:92.990% test:92.815% maxVal:92.990% maxTest:92.815%\n",
      "[10/40] [Loss] train:0.019(f:-0.144+r:0.047+k:0.001+l:0.114) val:-0.334 test:-0.335 [Accr] train:18.613% val:93.490% test:93.267% maxVal:93.490% maxTest:93.267%\n",
      "[40/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.374 test:-0.374 [Accr] train:18.971% val:95.009% test:95.230% maxVal:95.161% maxTest:95.230%\n",
      "Training finished.\n",
      "[Worker_03] Done.\n",
      "[11/40] [Loss] train:0.017(f:-0.145+r:0.046+k:0.001+l:0.114) val:-0.334 test:-0.339 [Accr] train:18.646% val:93.576% test:93.400% maxVal:93.576% maxTest:93.400%\n",
      "[00/40] [Loss] train:0.052(f:-0.114+r:0.049+k:0.001+l:0.116) val:-0.056 test:-0.067 [Accr] train:15.285% val:58.941% test:61.123% maxVal:58.941% maxTest:61.123%\n",
      "[12/40] [Loss] train:0.014(f:-0.146+r:0.045+k:0.001+l:0.114) val:-0.342 test:-0.345 [Accr] train:18.757% val:93.902% test:93.503% maxVal:93.902% maxTest:93.503%\n",
      "[01/40] [Loss] train:0.044(f:-0.123+r:0.050+k:0.001+l:0.115) val:-0.149 test:-0.153 [Accr] train:16.992% val:76.823% test:78.557% maxVal:76.823% maxTest:78.557%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/40] [Loss] train:0.013(f:-0.146+r:0.044+k:0.001+l:0.114) val:-0.337 test:-0.341 [Accr] train:18.785% val:94.314% test:94.069% maxVal:94.314% maxTest:94.069%\n",
      "[02/40] [Loss] train:0.038(f:-0.130+r:0.051+k:0.001+l:0.115) val:-0.206 test:-0.213 [Accr] train:17.565% val:83.225% test:84.056% maxVal:83.225% maxTest:84.056%\n",
      "[14/40] [Loss] train:0.012(f:-0.147+r:0.044+k:0.001+l:0.113) val:-0.347 test:-0.348 [Accr] train:18.801% val:94.314% test:94.069% maxVal:94.314% maxTest:94.069%\n",
      "[03/40] [Loss] train:0.035(f:-0.133+r:0.051+k:0.001+l:0.115) val:-0.236 test:-0.244 [Accr] train:17.781% val:85.807% test:86.451% maxVal:85.807% maxTest:86.451%\n",
      "[15/40] [Loss] train:0.010(f:-0.147+r:0.042+k:0.001+l:0.113) val:-0.355 test:-0.353 [Accr] train:18.808% val:94.293% test:94.171% maxVal:94.314% maxTest:94.069%\n",
      "[04/40] [Loss] train:0.032(f:-0.135+r:0.051+k:0.001+l:0.115) val:-0.253 test:-0.263 [Accr] train:18.058% val:87.934% test:88.261% maxVal:87.934% maxTest:88.261%\n",
      "[16/40] [Loss] train:0.007(f:-0.149+r:0.041+k:0.001+l:0.113) val:-0.364 test:-0.364 [Accr] train:18.887% val:94.531% test:94.367% maxVal:94.531% maxTest:94.367%\n",
      "[05/40] [Loss] train:0.030(f:-0.137+r:0.051+k:0.001+l:0.115) val:-0.278 test:-0.282 [Accr] train:18.107% val:89.757% test:89.844% maxVal:89.757% maxTest:89.844%\n",
      "[06/40] [Loss] train:0.028(f:-0.139+r:0.050+k:0.001+l:0.115) val:-0.288 test:-0.294 [Accr] train:18.330% val:91.168% test:91.345% maxVal:91.168% maxTest:91.345%\n",
      "[17/40] [Loss] train:0.005(f:-0.149+r:0.040+k:0.001+l:0.113) val:-0.362 test:-0.363 [Accr] train:18.925% val:94.575% test:94.511% maxVal:94.575% maxTest:94.511%\n",
      "[07/40] [Loss] train:0.025(f:-0.140+r:0.050+k:0.001+l:0.115) val:-0.299 test:-0.306 [Accr] train:18.456% val:91.927% test:91.879% maxVal:91.927% maxTest:91.879%\n",
      "[18/40] [Loss] train:0.003(f:-0.150+r:0.039+k:0.001+l:0.113) val:-0.366 test:-0.371 [Accr] train:18.889% val:94.792% test:94.562% maxVal:94.792% maxTest:94.562%\n",
      "[08/40] [Loss] train:0.023(f:-0.142+r:0.049+k:0.001+l:0.114) val:-0.317 test:-0.322 [Accr] train:18.520% val:92.535% test:92.331% maxVal:92.535% maxTest:92.331%\n",
      "[19/40] [Loss] train:0.002(f:-0.150+r:0.038+k:0.001+l:0.113) val:-0.367 test:-0.369 [Accr] train:18.960% val:95.030% test:94.973% maxVal:95.030% maxTest:94.973%\n",
      "[09/40] [Loss] train:0.021(f:-0.143+r:0.048+k:0.001+l:0.114) val:-0.323 test:-0.323 [Accr] train:18.562% val:92.882% test:92.784% maxVal:92.882% maxTest:92.784%\n",
      "[20/40] [Loss] train:0.001(f:-0.150+r:0.038+k:0.001+l:0.113) val:-0.367 test:-0.369 [Accr] train:18.975% val:95.182% test:94.953% maxVal:95.182% maxTest:94.953%\n",
      "[10/40] [Loss] train:0.019(f:-0.144+r:0.048+k:0.001+l:0.114) val:-0.333 test:-0.333 [Accr] train:18.630% val:93.490% test:93.226% maxVal:93.490% maxTest:93.226%\n",
      "[21/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.370 [Accr] train:18.976% val:95.095% test:94.881% maxVal:95.182% maxTest:94.953%\n",
      "[11/40] [Loss] train:0.017(f:-0.144+r:0.046+k:0.001+l:0.114) val:-0.332 test:-0.337 [Accr] train:18.657% val:93.511% test:93.452% maxVal:93.511% maxTest:93.452%\n",
      "[22/40] [Loss] train:0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.368 test:-0.369 [Accr] train:18.975% val:95.095% test:94.901% maxVal:95.182% maxTest:94.953%\n",
      "[12/40] [Loss] train:0.015(f:-0.146+r:0.046+k:0.001+l:0.114) val:-0.343 test:-0.346 [Accr] train:18.723% val:93.750% test:93.606% maxVal:93.750% maxTest:93.606%\n",
      "[23/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.369 test:-0.370 [Accr] train:18.989% val:95.182% test:94.942% maxVal:95.182% maxTest:94.953%\n",
      "[13/40] [Loss] train:0.013(f:-0.146+r:0.044+k:0.001+l:0.114) val:-0.334 test:-0.339 [Accr] train:18.757% val:93.989% test:94.048% maxVal:93.989% maxTest:94.048%\n",
      "[24/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.374 test:-0.371 [Accr] train:18.969% val:95.204% test:94.901% maxVal:95.204% maxTest:94.901%\n",
      "[14/40] [Loss] train:0.012(f:-0.147+r:0.044+k:0.001+l:0.113) val:-0.348 test:-0.348 [Accr] train:18.825% val:94.314% test:94.213% maxVal:94.314% maxTest:94.213%\n",
      "[25/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.370 test:-0.370 [Accr] train:19.006% val:95.161% test:94.932% maxVal:95.204% maxTest:94.901%\n",
      "[15/40] [Loss] train:0.010(f:-0.147+r:0.043+k:0.001+l:0.113) val:-0.355 test:-0.355 [Accr] train:18.814% val:94.336% test:94.326% maxVal:94.336% maxTest:94.326%\n",
      "[26/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.371 [Accr] train:19.006% val:95.095% test:94.912% maxVal:95.204% maxTest:94.901%\n",
      "[16/40] [Loss] train:0.007(f:-0.149+r:0.042+k:0.001+l:0.113) val:-0.364 test:-0.363 [Accr] train:18.883% val:94.683% test:94.603% maxVal:94.683% maxTest:94.603%\n",
      "[27/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.376 [Accr] train:18.987% val:95.161% test:94.963% maxVal:95.204% maxTest:94.901%\n",
      "[17/40] [Loss] train:0.006(f:-0.149+r:0.041+k:0.001+l:0.113) val:-0.363 test:-0.363 [Accr] train:18.911% val:94.857% test:94.552% maxVal:94.857% maxTest:94.552%\n",
      "[28/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.374 test:-0.374 [Accr] train:18.991% val:95.052% test:94.973% maxVal:95.204% maxTest:94.901%\n",
      "[18/40] [Loss] train:0.004(f:-0.150+r:0.040+k:0.001+l:0.113) val:-0.367 test:-0.371 [Accr] train:18.883% val:94.792% test:94.727% maxVal:94.857% maxTest:94.552%\n",
      "[29/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.373 test:-0.373 [Accr] train:19.009% val:95.139% test:94.994% maxVal:95.204% maxTest:94.901%\n",
      "[19/40] [Loss] train:0.002(f:-0.150+r:0.038+k:0.001+l:0.113) val:-0.364 test:-0.367 [Accr] train:18.940% val:95.095% test:95.189% maxVal:95.095% maxTest:95.189%\n",
      "[30/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.378 test:-0.378 [Accr] train:19.026% val:95.030% test:94.963% maxVal:95.204% maxTest:94.901%\n",
      "[20/40] [Loss] train:0.002(f:-0.150+r:0.038+k:0.001+l:0.113) val:-0.366 test:-0.369 [Accr] train:18.971% val:95.226% test:95.210% maxVal:95.226% maxTest:95.210%\n",
      "[31/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.370 test:-0.369 [Accr] train:19.018% val:95.074% test:94.922% maxVal:95.204% maxTest:94.901%\n",
      "[21/40] [Loss] train:0.001(f:-0.151+r:0.038+k:0.001+l:0.113) val:-0.371 test:-0.372 [Accr] train:18.967% val:95.117% test:95.086% maxVal:95.226% maxTest:95.210%\n",
      "[32/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.373 test:-0.373 [Accr] train:19.013% val:95.074% test:95.035% maxVal:95.204% maxTest:94.901%\n",
      "[22/40] [Loss] train:0.001(f:-0.151+r:0.038+k:0.001+l:0.113) val:-0.369 test:-0.369 [Accr] train:18.969% val:95.009% test:95.086% maxVal:95.226% maxTest:95.210%\n",
      "[33/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.368 test:-0.370 [Accr] train:19.011% val:95.074% test:95.014% maxVal:95.204% maxTest:94.901%\n",
      "[23/40] [Loss] train:0.001(f:-0.151+r:0.038+k:0.001+l:0.113) val:-0.370 test:-0.371 [Accr] train:18.967% val:95.182% test:95.158% maxVal:95.226% maxTest:95.210%\n",
      "[34/40] [Loss] train:-0.001(f:-0.151+r:0.036+k:0.001+l:0.113) val:-0.370 test:-0.371 [Accr] train:19.035% val:95.052% test:94.932% maxVal:95.204% maxTest:94.901%\n",
      "[24/40] [Loss] train:0.000(f:-0.151+r:0.038+k:0.001+l:0.113) val:-0.374 test:-0.372 [Accr] train:18.929% val:95.161% test:95.127% maxVal:95.226% maxTest:95.210%\n",
      "[35/40] [Loss] train:-0.001(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.370 test:-0.373 [Accr] train:19.002% val:95.117% test:95.035% maxVal:95.204% maxTest:94.901%\n",
      "[25/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.370 test:-0.371 [Accr] train:18.991% val:95.139% test:95.158% maxVal:95.226% maxTest:95.210%\n",
      "[36/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.371 test:-0.372 [Accr] train:19.055% val:95.074% test:94.963% maxVal:95.204% maxTest:94.901%\n",
      "[26/40] [Loss] train:-0.000(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.372 [Accr] train:18.987% val:95.009% test:95.066% maxVal:95.226% maxTest:95.210%\n",
      "[37/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.373 test:-0.375 [Accr] train:19.033% val:95.095% test:94.994% maxVal:95.204% maxTest:94.901%\n",
      "[27/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.376 [Accr] train:18.985% val:95.030% test:95.230% maxVal:95.226% maxTest:95.210%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.372 test:-0.373 [Accr] train:19.013% val:95.095% test:94.984% maxVal:95.204% maxTest:94.901%\n",
      "[28/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.374 test:-0.374 [Accr] train:18.980% val:95.052% test:95.189% maxVal:95.226% maxTest:95.210%\n",
      "[39/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.368 test:-0.369 [Accr] train:19.020% val:95.139% test:95.056% maxVal:95.204% maxTest:94.901%\n",
      "[29/40] [Loss] train:-0.002(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.373 [Accr] train:18.998% val:95.182% test:95.241% maxVal:95.226% maxTest:95.210%\n",
      "[40/40] [Loss] train:-0.002(f:-0.152+r:0.036+k:0.001+l:0.113) val:-0.375 test:-0.374 [Accr] train:19.009% val:95.095% test:95.035% maxVal:95.204% maxTest:94.901%\n",
      "Training finished.\n",
      "[Worker_00] Done.\n",
      "[30/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.377 test:-0.378 [Accr] train:19.007% val:95.030% test:95.220% maxVal:95.226% maxTest:95.210%\n",
      "[31/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.370 [Accr] train:19.035% val:95.030% test:95.199% maxVal:95.226% maxTest:95.210%\n",
      "[32/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.373 test:-0.373 [Accr] train:18.991% val:95.095% test:95.210% maxVal:95.226% maxTest:95.210%\n",
      "[33/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.368 test:-0.371 [Accr] train:19.013% val:95.074% test:95.199% maxVal:95.226% maxTest:95.210%\n",
      "[34/40] [Loss] train:-0.001(f:-0.151+r:0.037+k:0.001+l:0.113) val:-0.370 test:-0.371 [Accr] train:19.011% val:95.009% test:95.230% maxVal:95.226% maxTest:95.210%\n",
      "[35/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.370 test:-0.373 [Accr] train:18.993% val:95.074% test:95.189% maxVal:95.226% maxTest:95.210%\n",
      "[36/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.371 test:-0.373 [Accr] train:19.027% val:95.030% test:95.127% maxVal:95.226% maxTest:95.210%\n",
      "[37/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.373 test:-0.375 [Accr] train:19.033% val:95.074% test:95.292% maxVal:95.226% maxTest:95.210%\n",
      "[38/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.372 test:-0.373 [Accr] train:18.987% val:95.074% test:95.230% maxVal:95.226% maxTest:95.210%\n",
      "[39/40] [Loss] train:-0.001(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.368 test:-0.370 [Accr] train:19.011% val:95.269% test:95.261% maxVal:95.269% maxTest:95.261%\n",
      "[40/40] [Loss] train:-0.002(f:-0.152+r:0.037+k:0.001+l:0.113) val:-0.374 test:-0.374 [Accr] train:19.002% val:95.052% test:95.251% maxVal:95.269% maxTest:95.261%\n",
      "Training finished.\n",
      "[Worker_01] Done.\n"
     ]
    }
   ],
   "source": [
    "for i in range(nWorker):\n",
    "    WORKERS[i].start(); # Start process\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
