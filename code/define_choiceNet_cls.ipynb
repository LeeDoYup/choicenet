{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChoiceNet for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version is [1.4.1].\n"
     ]
    }
   ],
   "source": [
    "import nbloader,os,warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from sklearn.utils import shuffle\n",
    "from util import gpusession,create_gradient_clipping,load_mnist_with_noise,print_n_txt,mixup\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "if __name__ == \"__main__\":\n",
    "    print (\"TensorFlow version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ChoiceNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choiceNet_cls_class defined.\n"
     ]
    }
   ],
   "source": [
    "class choiceNet_cls_class(object):\n",
    "    def __init__(self,_name='',_xdim=[28,28,1],_ydim=10,_hdims=[64,64],_filterSizes=[3,3],_max_pools=[2,2],_feat_dim=128\n",
    "                 ,_kmix=5,_actv=tf.nn.relu,_bn=slim.batch_norm\n",
    "                 ,_rho_ref_train=0.95,_tau_inv=1e-4,_pi1_bias=0.0,_logSigmaZval=-2\n",
    "                 ,_logsumexp_coef=0.1,_kl_reg_coef=0.1,_l2_reg_coef=1e-5\n",
    "                 ,_momentum = 0.5\n",
    "                 ,_USE_INPUT_BN=False,_USE_RESNET=False,_USE_GAP=False,_USE_KENDALL_LOSS=False,_USE_SGD=False\n",
    "                 ,_USE_MIXUP=False\n",
    "                 ,_GPU_ID=0,_VERBOSE=True):\n",
    "        self.name = _name\n",
    "        self.xdim = _xdim\n",
    "        self.ydim = _ydim\n",
    "        self.hdims = _hdims\n",
    "        self.filterSizes = _filterSizes\n",
    "        self.max_pools = _max_pools\n",
    "        self.feat_dim = _feat_dim\n",
    "        self.kmix = _kmix\n",
    "        self.actv = _actv \n",
    "        self.bn   = _bn # slim.batch_norm / None\n",
    "        self.rho_ref_train = _rho_ref_train\n",
    "        self.tau_inv = _tau_inv\n",
    "        self.pi1_bias = _pi1_bias\n",
    "        self.logSigmaZval = _logSigmaZval\n",
    "        self.logsumexp_coef = _logsumexp_coef\n",
    "        self.kl_reg_coef = _kl_reg_coef\n",
    "        self.l2_reg_coef = _l2_reg_coef\n",
    "        self.momentum = _momentum\n",
    "        self.USE_INPUT_BN = _USE_INPUT_BN\n",
    "        self.USE_RESNET = _USE_RESNET\n",
    "        self.USE_GAP = _USE_GAP\n",
    "        self.USE_KENDALL_LOSS = _USE_KENDALL_LOSS\n",
    "        self.USE_SGD = _USE_SGD\n",
    "        self.USE_MIXUP = _USE_MIXUP\n",
    "        self.GPU_ID = (int)(_GPU_ID)\n",
    "        self.VERBOSE = _VERBOSE\n",
    "        with tf.device('/device:GPU:%d'%(self.GPU_ID)):\n",
    "            # Build model\n",
    "            self.build_model()\n",
    "            # Build graph\n",
    "            self.build_graph()\n",
    "            # Check parameters\n",
    "            self.check_params()\n",
    "        \n",
    "    def build_model(self):\n",
    "        _xdim = self.xdim[0]*self.xdim[1]*self.xdim[2] # Total dimension\n",
    "        self.x = tf.placeholder(dtype=tf.float32,shape=[None,_xdim],name='x') # Input [None x xdim]\n",
    "        self.t = tf.placeholder(dtype=tf.float32,shape=[None,self.ydim],name='t') # Output [None x ydim]\n",
    "        self.kp = tf.placeholder(dtype=tf.float32,shape=[],name='kp') # Keep probability \n",
    "        self.lr = tf.placeholder(dtype=tf.float32,shape=[],name='lr') # Learning rate\n",
    "        self.is_training = tf.placeholder(dtype=tf.bool,shape=[]) # Training flag\n",
    "        self.rho_ref = tf.placeholder(dtype=tf.float32,shape=[],name='rho_ref') \n",
    "        # Initailizers\n",
    "        self.fully_init  = tf.random_normal_initializer(stddev=0.01)\n",
    "        self.bias_init   = tf.constant_initializer(0.)\n",
    "        self.bn_init     = {'beta': tf.constant_initializer(0.),\n",
    "                           'gamma': tf.random_normal_initializer(1., 0.01)}\n",
    "        self.bn_params   = {'is_training':self.is_training,'decay':0.9,'epsilon':1e-5,\n",
    "                           'param_initializers':self.bn_init,'updates_collections':None}\n",
    "\n",
    "        # Build graph\n",
    "        with tf.variable_scope(self.name,reuse=False) as scope:\n",
    "            with slim.arg_scope([slim.fully_connected],activation_fn=self.actv,\n",
    "                                weights_initializer=self.fully_init,biases_initializer=self.bias_init,\n",
    "                                normalizer_fn=self.bn,normalizer_params=self.bn_params,\n",
    "                                weights_regularizer=None):            \n",
    "                \n",
    "                # List of features\n",
    "                self.layers = []\n",
    "                self.layers.append(self.x)\n",
    "\n",
    "                # Reshape input \n",
    "                _net = tf.reshape(self.x,[-1]+self.xdim) \n",
    "                self.layers.append(_net)\n",
    "\n",
    "                # Input normalization \n",
    "                if self.USE_INPUT_BN:\n",
    "                    _net = slim.batch_norm(_net,param_initializers=self.bn_init,is_training=self.is_training,updates_collections=None)\n",
    "                \n",
    "                for hidx,hdim in enumerate(self.hdims): # For all layers\n",
    "                    fs = self.filterSizes[hidx]\n",
    "                    if self.USE_RESNET: # Use residual connection \n",
    "                        cChannelSize = _net.get_shape()[3] # Current channel size\n",
    "                        if cChannelSize == hdim:\n",
    "                            _identity = _net\n",
    "                        else: # Expand dimension if required \n",
    "                            _identity = slim.conv2d(_net,hdim,[fs,fs],padding='SAME',activation_fn=None \n",
    "                                                  , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                                  , normalizer_fn       = self.bn\n",
    "                                                  , normalizer_params   = self.bn_params\n",
    "                                                  , scope='identity_%d'%(hidx))\n",
    "                        # First conv \n",
    "                        _net = slim.conv2d(_net,hdim,[fs,fs],padding='SAME'\n",
    "                                         , activation_fn       = None \n",
    "                                         , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                         , normalizer_fn       = self.bn\n",
    "                                         , normalizer_params   = self.bn_params\n",
    "                                         , scope='res_a_%d'%(hidx))\n",
    "                        # Relu\n",
    "                        _net = self.actv(_net)\n",
    "                        self.layers.append(_net) # Append to layers\n",
    "                        # Second conv\n",
    "                        _net = slim.conv2d(_net,hdim,[fs,fs],padding='SAME'\n",
    "                                         , activation_fn       = None\n",
    "                                         , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                         , normalizer_fn       = self.bn\n",
    "                                         , normalizer_params   = self.bn_params\n",
    "                                         , scope='res_b_%d'%(hidx))\n",
    "                        # Skip connection\n",
    "                        _net = _net + _identity\n",
    "                        # Relu\n",
    "                        _net = self.actv(_net)\n",
    "                        self.layers.append(_net) # Append to layers\n",
    "                    else: # Without residual connection\n",
    "                        _net = slim.conv2d(_net,hdim,[fs,fs],padding='SAME'\n",
    "                                         , activation_fn       = self.actv\n",
    "                                         , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                         , normalizer_fn       = self.bn\n",
    "                                         , normalizer_params   = self.bn_params\n",
    "                                         , scope='conv_%d'%(hidx))\n",
    "                    # Max pooling (if required)\n",
    "                    max_pool = self.max_pools[hidx]\n",
    "                    if max_pool > 1:\n",
    "                        _net = slim.max_pool2d(_net,[max_pool,max_pool],scope='pool_%d'%(hidx))\n",
    "                        self.layers.append(_net) # Append to layers\n",
    "                        \n",
    "                if self.USE_GAP: # Global average pooling \n",
    "                    _net = tf.reduce_mean(_net,[1,2]) # [N x R]\n",
    "                    self.layers.append(_net) # Append to layers\n",
    "                    # Optional dense layer after GAP (this increases performance)\n",
    "                    _net = slim.fully_connected(_net,self.feat_dim,scope='gap_fc') # [N x Q]\n",
    "                    # Feature\n",
    "                    self.feat = _net # [N x Q]\n",
    "                else:\n",
    "                    # Flatten \n",
    "                    _net = slim.flatten(_net, scope='flatten')\n",
    "                    self.layers.append(_net) # Append to layers\n",
    "                    # Dense\n",
    "                    _net = slim.fully_connected(_net,self.feat_dim,scope='fc')\n",
    "                    self.layers.append(_net) # Append to layers\n",
    "                    # Feature\n",
    "                    self.feat = _net # [N x Q]\n",
    "                \n",
    "                # Feature to K rhos\n",
    "                _rho_raw = slim.fully_connected(self.feat,self.kmix,scope='rho_raw')\n",
    "                # self.rho_temp = tf.nn.tanh(_rho_raw) # [N x K] # Regression\n",
    "                self.rho_temp = tf.nn.sigmoid(_rho_raw) # [N x K] # Classification\n",
    "                self.rho = tf.concat([self.rho_temp[:,0:1]*0.0+self.rho_ref,self.rho_temp[:,1:]]\n",
    "                                     ,axis=1) # [N x K]\n",
    "                \n",
    "                # Sampler variables\n",
    "                _Q = self.feat.get_shape().as_list()[1] # Feature dimension\n",
    "                self.Q = _Q\n",
    "                self.muW = tf.get_variable(name='muW',shape=[_Q,self.ydim],\n",
    "                                          initializer=tf.random_normal_initializer(stddev=0.1)\n",
    "                                           ,dtype=tf.float32) # [Q x D]\n",
    "                self.logSigmaW = tf.get_variable(name='logSigmaW'\n",
    "                                        ,shape=[_Q,self.ydim]\n",
    "                                        ,initializer=tf.constant_initializer(-3.0)\n",
    "                                        ,dtype=tf.float32) # [Q x D]\n",
    "                self.muZ = tf.constant(np.zeros((_Q,self.ydim))\n",
    "                                        ,name='muZ',dtype=tf.float32) # [Q x D]\n",
    "                self.logSigmaZ = tf.constant(self.logSigmaZval*np.ones((_Q,self.ydim)) # -2.0 <== Important Heuristics\n",
    "                                        ,name='logSigmaZ',dtype=tf.float32) # [Q x D]\n",
    "                \n",
    "                # Make sampler\n",
    "                _N = tf.shape(self.x)[0]\n",
    "                _muW_tile = tf.tile(self.muW[tf.newaxis,:,:]\n",
    "                                    ,multiples=[_N,1,1]) # [N x Q x D]\n",
    "                _sigmaW_tile = tf.exp(tf.tile(self.logSigmaW[tf.newaxis,:,:]\n",
    "                                              ,multiples=[_N,1,1])) # [N x Q x D]\n",
    "                _muZ_tile = tf.tile(self.muZ[tf.newaxis,:,:]\n",
    "                                    ,multiples=[_N,1,1]) # [N x Q x D]\n",
    "                _sigmaZ_tile = tf.exp(tf.tile(self.logSigmaZ[tf.newaxis,:,:]\n",
    "                                              ,multiples=[_N,1,1])) # [N x Q x D]\n",
    "                samplerList = []\n",
    "                for jIdx in range(self.kmix): # For all K mixtures\n",
    "                    _rho_j = self.rho[:,jIdx:jIdx+1] # [N x 1] \n",
    "                    _rho_tile = tf.tile(_rho_j[:,:,tf.newaxis]\n",
    "                                        ,multiples=[1,_Q,self.ydim]) # [N x Q x D]\n",
    "                    _epsW = tf.random_normal(shape=[_N,_Q,self.ydim],mean=0,stddev=1\n",
    "                                             ,dtype=tf.float32) # [N x Q x D]\n",
    "                    _W = _muW_tile + tf.sqrt(_sigmaW_tile)*_epsW # [N x Q x D]\n",
    "                    _epsZ = tf.random_normal(shape=[_N,_Q,self.ydim]\n",
    "                                             ,mean=0,stddev=1,dtype=tf.float32) # [N x Q x D]\n",
    "                    _Z = _muZ_tile + tf.sqrt(_sigmaZ_tile)*_epsZ # [N x Q x D]\n",
    "                    # Append to list\n",
    "                    _Y = _rho_tile*_muW_tile + (1.0-_rho_tile**2) \\\n",
    "                        *(_rho_tile*tf.sqrt(_sigmaZ_tile)/tf.sqrt(_sigmaW_tile) \\\n",
    "                              *(_W-_muW_tile)+tf.sqrt(1-_rho_tile**2)*_Z)\n",
    "                    samplerList.append(_Y) # Append \n",
    "                # Make list to tensor\n",
    "                WlistConcat = tf.convert_to_tensor(samplerList) # K*[N x Q x D] => [K x N x Q x D]\n",
    "                self.wSample = tf.transpose(WlistConcat,perm=[1,3,0,2]) # [N x D x K x Q]\n",
    "\n",
    "                # K mean mixtures [N x D x K]\n",
    "                _wTemp = tf.reshape(self.wSample\n",
    "                                ,shape=[_N,self.kmix*self.ydim,_Q]) # [N x KD x Q]\n",
    "                _featRsh = tf.reshape(self.feat,shape=[_N,_Q,1]) # [N x Q x 1]\n",
    "                _mu = tf.matmul(_wTemp,_featRsh) # [N x KD x Q] x [N x Q x 1] => [N x KD x 1]\n",
    "                self.mu = tf.reshape(_mu,shape=[_N,self.ydim,self.kmix]) # [N x D x K]\n",
    "                \n",
    "                # (optional) Add bias to mu\n",
    "                USE_BIAS = False\n",
    "                if USE_BIAS:\n",
    "                    self.muBias = tf.get_variable(name='muBias'\n",
    "                                            ,shape=[self.ydim]\n",
    "                                            ,initializer=tf.constant_initializer(0.0)\n",
    "                                            ,dtype=tf.float32) # [D]\n",
    "                    muBias_tile = tf.tile(self.muBias[tf.newaxis,:,tf.newaxis]\n",
    "                                        ,multiples=[_N,1,self.kmix]) # [N x D x K]\n",
    "                    self.mu += muBias_tile\n",
    "\n",
    "                # K var mixtures [N x D x K]\n",
    "                _logvar_raw = slim.fully_connected(self.feat,self.ydim,scope='var_raw') # [N x D]\n",
    "                _var_raw = tf.exp(_logvar_raw) # [N x D]\n",
    "                _var_tile = tf.tile(_var_raw[:,:,tf.newaxis]\n",
    "                                    ,multiples=[1,1,self.kmix]) # [N x D x K]\n",
    "                _rho_tile = tf.tile(self.rho[:,tf.newaxis,:]\n",
    "                                    ,multiples=[1,self.ydim,1]) # [N x D x K]\n",
    "                _tau_inv = self.tau_inv\n",
    "                self.var = (1.0-_rho_tile**2)*_var_tile + _tau_inv # [N x D x K]\n",
    "                \n",
    "                # Weight allocation probability pi [N x K]\n",
    "                _pi_logits = slim.fully_connected(self.feat,self.kmix\n",
    "                                                  ,scope='pi_logits') # [N x K]\n",
    "                self.pi_temp = tf.nn.softmax(_pi_logits,dim=1) # [N x K]\n",
    "                # Some heuristics to ensure that pi_1(x) is high enough\n",
    "                self.pi_temp = tf.concat([self.pi_temp[:,0:1]+self.pi1_bias\n",
    "                                          ,self.pi_temp[:,1:]],axis=1) # [N x K]\n",
    "                self.pi = tf.nn.softmax(self.pi_temp,dim=1) # [N x K]\n",
    "                \n",
    "                # Intermediate tensors\n",
    "                self.tensors = [self.x,self.feat,self.rho,self.mu,self.var,self.pi] \n",
    "    \n",
    "    # Build graph\n",
    "    def build_graph(self):\n",
    "        # MDN loss\n",
    "        _N = tf.shape(self.x)[0]\n",
    "        t,mu,var = self.t,self.mu,self.var\n",
    "        pi = self.pi # [N x K]\n",
    "        yhat = mu + tf.sqrt(var)*tf.random_normal(shape=[_N,self.ydim,self.kmix]) # Sampled y [N x D x K]\n",
    "        tTile = tf.tile(t[:,:,tf.newaxis],[1,1,self.kmix]) # Target [N x D x K]\n",
    "        piTile = tf.tile(pi[:,tf.newaxis,:],[1,self.ydim,1]) # piTile: [N x D x K]\n",
    "        \n",
    "        if self.USE_KENDALL_LOSS: # Alex Kendal's loss extended to a mixture model\n",
    "            self._loss_fit = tf.reduce_sum(-piTile*yhat*tTile,axis=[1,2]) # [N]\n",
    "            self.loss_fit = tf.reduce_mean(self._loss_fit) # [1]\n",
    "            \n",
    "            self._loss_reg = pi*tf.reduce_logsumexp(yhat,axis=[1]) # [N x K]\n",
    "            self.__loss_reg = tf.reduce_sum(self._loss_reg,axis=[1]) # [N]\n",
    "            self.loss_reg = tf.reduce_mean(self.__loss_reg) # [1] \n",
    "            \n",
    "            # self._loss_reg = tf.reduce_logsumexp(piTile*yhat,axis=[1,2]) # [N]\n",
    "            # self.loss_reg = tf.reduce_mean(self._loss_reg) # [1]\n",
    "        else: # Mine (normalized x)\n",
    "            self.yhat_normalized = tf.nn.softmax(yhat,dim=1) # [N x D x K]\n",
    "            self._loss_fit = tf.reduce_sum(-piTile*self.yhat_normalized*tTile,axis=[1,2]) # [N]\n",
    "            self.loss_fit = tf.reduce_mean(self._loss_fit) # [1]\n",
    "            \n",
    "            self._loss_reg = pi*tf.reduce_logsumexp(yhat,axis=[1]) # [N x K]\n",
    "            self.__loss_reg = tf.reduce_sum(self._loss_reg,axis=[1]) # [N]\n",
    "            self.loss_reg = self.logsumexp_coef*tf.reduce_mean(self.__loss_reg) # [1] \n",
    "            \n",
    "            # self._loss_reg = self.logsumexp_coef*tf.reduce_logsumexp(piTile*yhat,axis=[1,2]) # [N]\n",
    "            # self.loss_reg = tf.reduce_mean(self._loss_reg) # [1]\n",
    "        \n",
    "        # KL-divergence regularizer \n",
    "        _eps = 1e-8\n",
    "        self._kl_reg = self.kl_reg_coef*tf.reduce_sum(-self.rho\n",
    "                        *(tf.log(self.pi+_eps)-tf.log(self.rho+_eps)),axis=1) # (N)\n",
    "        self.kl_reg = tf.reduce_mean(self._kl_reg) # (1)\n",
    "        \n",
    "        # Weight decay \n",
    "        # _g_vars = tf.global_variables()\n",
    "        _g_vars = tf.trainable_variables()\n",
    "        self.c_vars = [var for var in _g_vars if '%s/'%(self.name) in var.name]\n",
    "        self.l2_reg = self.l2_reg_coef*tf.reduce_sum(tf.stack([tf.nn.l2_loss(v) for v in self.c_vars])) # [1]\n",
    "\n",
    "        # Total loss\n",
    "        self.loss_total = tf.reduce_mean(self.loss_fit+self.loss_reg+self.kl_reg+self.l2_reg) # [1]\n",
    "        # Optimizer\n",
    "        GRAD_CLIP = True\n",
    "        if GRAD_CLIP: # Gradient clipping\n",
    "            if self.USE_SGD:\n",
    "                # _optm = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
    "                _optm = tf.train.MomentumOptimizer(learning_rate=self.lr,momentum=self.momentum)\n",
    "            else:\n",
    "                _optm = tf.train.AdamOptimizer(learning_rate=self.lr\n",
    "                                               ,beta1=0.9,beta2=0.999,epsilon=1e-6)\n",
    "            self.optm = create_gradient_clipping(self.loss_total\n",
    "                                            ,_optm,tf.trainable_variables(),clipVal=1.0)\n",
    "        else:\n",
    "            if self.USE_SGD:\n",
    "                self.optm = tf.train.GradientDescentOptimizer(learning_rate=self.lr).minimize(self.loss_total) \n",
    "            else:\n",
    "                self.optm = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss_total) \n",
    "        \n",
    "        # Compute accuray \n",
    "        maxIdx = tf.argmax(input=pi,axis=1, output_type=tf.int32) # Argmax Index [N]\n",
    "        maxIdx = 0*tf.ones_like(maxIdx)\n",
    "        coords = tf.stack([tf.transpose(gv) for gv in tf.meshgrid(tf.range(_N),tf.range(self.ydim))] + \n",
    "                          [tf.reshape(tf.tile(maxIdx[:,tf.newaxis],[1,self.ydim]),shape=(_N,self.ydim))]\n",
    "                          ,axis=2) # [N x D x 3]\n",
    "        mu_bar = tf.gather_nd(mu,coords) # [N x D]\n",
    "        _corr = tf.equal(tf.argmax(mu_bar, 1), tf.argmax(self.t, 1))    \n",
    "        self.accr = tf.reduce_mean(tf.cast(_corr,tf.float32)) # Accuracy\n",
    "        \n",
    "    # Check parameters\n",
    "    def check_params(self):\n",
    "        _g_vars = tf.global_variables()\n",
    "        self.g_vars = [var for var in _g_vars if '%s/'%(self.name) in var.name]\n",
    "        if self.VERBOSE:\n",
    "            print (\"==== Global Variables ====\")\n",
    "        for i in range(len(self.g_vars)):\n",
    "            w_name  = self.g_vars[i].name\n",
    "            w_shape = self.g_vars[i].get_shape().as_list()\n",
    "            if self.VERBOSE:\n",
    "                print (\" [%02d] Name:[%s] Shape:[%s]\" % (i,w_name,w_shape))\n",
    "        # Print layers\n",
    "        if self.VERBOSE:\n",
    "            print (\"Layers:\")\n",
    "            nLayers = len(self.layers)\n",
    "            for i in range(nLayers):\n",
    "                print (\"[%02d/%d] %s %s\"%(i,nLayers,self.layers[i].name,self.layers[i].shape))\n",
    "    \n",
    "    # Sampler\n",
    "    def sampler(self,_sess,_x,n_samples=10):\n",
    "        pi, mu, var = _sess.run([self.pi, self.mu, self.var],\n",
    "                                feed_dict={self.x:_x,self.kp:1.0,self.is_training:False\n",
    "                                          ,self.rho_ref:1.0}) #\n",
    "        n_points = _x.shape[0]\n",
    "        _y_sampled = np.zeros([n_points,self.ydim,n_samples])\n",
    "        for i in range(n_points):\n",
    "            for j in range(n_samples):\n",
    "                k = np.random.choice(self.kmix,p=pi[i,:])\n",
    "                k = 0\n",
    "                _y_sampled[i,:,j] = mu[i,:,k] # + np.random.randn(1,self.ydim)*np.sqrt(var[i,:,k])\n",
    "        return _y_sampled\n",
    "    \n",
    "    # Save \n",
    "    def save(self,_sess,_savename=None):\n",
    "        \"\"\" Save name \"\"\"\n",
    "        if _savename==None:\n",
    "            _savename='../net/net_%s.npz'%(self.name)\n",
    "        \"\"\" Get global variables \"\"\"\n",
    "        self.g_wnames,self.g_wvals,self.g_wshapes = [],[],[]\n",
    "        for i in range(len(self.g_vars)):\n",
    "            curr_wname = self.g_vars[i].name\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==curr_wname][0]\n",
    "            curr_wval  = _sess.run(curr_wvar)\n",
    "            \n",
    "            curr_wval_sqz = curr_wval\n",
    "            # curr_wval_sqz  = curr_wval.squeeze() # ???\n",
    "            curr_wval_sqz = np.asanyarray(curr_wval_sqz,order=(1,-1))\n",
    "            \n",
    "            self.g_wnames.append(curr_wname)\n",
    "            self.g_wvals.append(curr_wval_sqz)\n",
    "            self.g_wshapes.append(curr_wval.shape)\n",
    "        \"\"\" Save \"\"\"\n",
    "        np.savez(_savename,g_wnames=self.g_wnames,g_wvals=self.g_wvals,g_wshapes=self.g_wshapes)\n",
    "        if self.VERBOSE:\n",
    "            print (\"[%s] Saved. Size is [%.4f]MB\" % \n",
    "                   (_savename,os.path.getsize(_savename)/1000./1000.))\n",
    "        \n",
    "    # Restore\n",
    "    def restore(self,_sess,_loadname=None):\n",
    "        if _loadname==None:\n",
    "            _loadname='../net/net_%s.npz'%(self.name)\n",
    "        l = np.load(_loadname)\n",
    "        g_wnames = l['g_wnames']\n",
    "        g_wvals  = l['g_wvals']\n",
    "        g_wshapes = l['g_wshapes']\n",
    "        for widx,wname in enumerate(g_wnames):\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==wname][0]\n",
    "            _sess.run(tf.assign(curr_wvar,g_wvals[widx].reshape(g_wshapes[widx])))\n",
    "        if self.VERBOSE:\n",
    "            print (\"Weight restored from [%s] Size is [%.4f]MB\" % \n",
    "                   (_loadname,os.path.getsize(_loadname)/1000./1000.))\n",
    "    \n",
    "    # Train \n",
    "    def train(self,_sess,_trainimg,_trainlabel,_testimg,_testlabel,_valimg,_vallabel\n",
    "              ,_maxEpoch=10,_batchSize=256,_lr=1e-3,_kp=0.8\n",
    "              ,_LR_SCHEDULE=False,_PRINT_EVERY=10,_SAVE_BEST=True,_DO_AUGMENTATION=False,_VERBOSE_TRAIN=True):\n",
    "        tf.set_random_seed(0)\n",
    "        nTrain,nVal,nTest = _trainimg.shape[0],_valimg.shape[0],_testimg.shape[0]\n",
    "        txtName = ('../res/res_%s.txt'%(self.name))\n",
    "        f = open(txtName,'w') # Open txt file\n",
    "        print_n_txt(_f=f,_chars='Text name: '+txtName)\n",
    "        print_period=max(1,_maxEpoch//_PRINT_EVERY)\n",
    "        maxIter,maxValAccr,maxTestAccr = max(nTrain//_batchSize,1),0.0,0.0\n",
    "        for epoch in range(_maxEpoch+1): # For every epoch \n",
    "            _trainimg,_trainlabel = shuffle(_trainimg,_trainlabel) \n",
    "            for iter in range(maxIter): # For every iteration in one epoch\n",
    "                start,end = iter*_batchSize,(iter+1)*_batchSize\n",
    "                # Learning rate scheduling\n",
    "                if _LR_SCHEDULE:\n",
    "                    if epoch < 0.5*_maxEpoch:\n",
    "                        _lr_use = _lr\n",
    "                    elif epoch < 0.75*_maxEpoch:\n",
    "                        _lr_use = _lr/10.0\n",
    "                    else:\n",
    "                        _lr_use = _lr/100.0\n",
    "                else:\n",
    "                    _lr_use = _lr\n",
    "                if _DO_AUGMENTATION:\n",
    "                    trainImgBatch = augment_img(_trainimg[start:end,:],self.xdim) \n",
    "                else:\n",
    "                    trainImgBatch = _trainimg[start:end,:]\n",
    "                if self.USE_MIXUP:\n",
    "                    xBatch = trainImgBatch\n",
    "                    tBatch = _trainlabel[start:end,:]\n",
    "                    xBatch,tBatch = mixup(xBatch,tBatch,1/2)\n",
    "                else:\n",
    "                    xBatch = trainImgBatch\n",
    "                    tBatch = _trainlabel[start:end,:]\n",
    "                feeds = {self.x:xBatch,self.t:tBatch\n",
    "                         ,self.rho_ref:self.rho_ref_train,self.kp:_kp,self.lr:_lr_use,self.is_training:True}\n",
    "                _sess.run(self.optm,feed_dict=feeds)\n",
    "            # Print training losses, training accuracy, validation accuracy, and test accuracy\n",
    "            if (epoch%print_period)==0 or (epoch==(_maxEpoch)):\n",
    "                batchSize4print = 512 \n",
    "                # Compute train loss and accuracy\n",
    "                maxIter4print = max(nTrain//batchSize4print,1)\n",
    "                trainLoss,trainAccr,fit,reg,kl,l2,nTemp = 0,0,0,0,0,0,0\n",
    "                for iter in range(maxIter4print):\n",
    "                    start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "                    feeds_train = {self.x:_trainimg[start:end,:],self.t:_trainlabel[start:end,:]\n",
    "                                   ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "                    opers_train = [self.loss_total,self.accr,self.loss_fit,self.loss_reg,self.kl_reg,self.l2_reg]\n",
    "                    _trainLoss,_trainAccr,_fit,_reg,_kl,_l2 = _sess.run(opers_train,feed_dict=feeds_train) \n",
    "                    _nTemp = end-start; nTemp+=_nTemp\n",
    "                    trainLoss+=(_nTemp*_trainLoss);trainAccr+=(_nTemp*_trainAccr)\n",
    "                    fit+=(_nTemp*_fit);reg+=(_nTemp*_reg);kl+=(_nTemp*_kl);l2+=(_nTemp*_l2)\n",
    "                trainLoss/=nTemp;trainAccr/=nTemp\n",
    "                fit/=nTemp;reg/=nTemp;kl/=nTemp;l2/=nTemp;\n",
    "                # Compute validation loss and accuracy\n",
    "                maxIter4print = max(nVal//batchSize4print,1)\n",
    "                valLoss,valAccr,nTemp = 0,0,0\n",
    "                for iter in range(maxIter4print):\n",
    "                    start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "                    feeds_val = {self.x:_valimg[start:end,:],self.t:_vallabel[start:end,:]\n",
    "                                 ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "                    _valLoss,_valAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_val) \n",
    "                    _nTemp = end-start; nTemp+=_nTemp\n",
    "                    valLoss+=(_nTemp*_valLoss); valAccr+=(_nTemp*_valAccr)\n",
    "                valLoss/=nTemp;valAccr/=nTemp \n",
    "                # Compute test loss and accuracy\n",
    "                maxIter4print = max(nTest//batchSize4print,1)\n",
    "                testLoss,testAccr,nTemp = 0,0,0\n",
    "                for iter in range(maxIter4print):\n",
    "                    start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "                    feeds_test = {self.x:_testimg[start:end,:],self.t:_testlabel[start:end,:]\n",
    "                                  ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "                    _testLoss,_testAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_test) \n",
    "                    _nTemp = end-start; nTemp+=_nTemp\n",
    "                    testLoss+=(_nTemp*_testLoss); testAccr+=(_nTemp*_testAccr)\n",
    "                testLoss/=nTemp;testAccr/=nTemp\n",
    "                # Compute max val accr \n",
    "                if valAccr > maxValAccr:\n",
    "                    maxValAccr = valAccr\n",
    "                    maxTestAccr = testAccr\n",
    "                    if _SAVE_BEST: self.save(_sess) \n",
    "                strTemp = ((\"[%02d/%d] [Loss] train:%.3f(f:%.3f+r:%.3f+k:%.3f+l:%.3f) val:%.3f test:%.3f\"\n",
    "                            +\" [Accr] train:%.1f%% val:%.1f%% test:%.1f%% maxVal:%.1f%% maxTest:%.1f%%\")\n",
    "                       %(epoch,_maxEpoch,trainLoss,fit,reg,kl,l2,valLoss,testLoss\n",
    "                         ,trainAccr*100,valAccr*100,testAccr*100,maxValAccr*100,maxTestAccr*100))\n",
    "                print_n_txt(_f=f,_chars=strTemp,_DO_PRINT=_VERBOSE_TRAIN)\n",
    "        # Done \n",
    "        print (\"Training finished.\")\n",
    "    \n",
    "    # Test\n",
    "    def test(self,_sess,_trainimg,_trainlabel,_testimg,_testlabel,_valimg,_vallabel):\n",
    "        nTrain,nVal,nTest = _trainimg.shape[0],_valimg.shape[0],_testimg.shape[0]\n",
    "        # Check accuracies (train, val, and test)\n",
    "        batchSize4print = 512 \n",
    "        # Compute train loss and accuracy\n",
    "        maxIter4print = max(nTrain//batchSize4print,1)\n",
    "        trainLoss,trainAccr,nTemp = 0,0,0\n",
    "        for iter in range(maxIter4print):\n",
    "            start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "            feeds_train = {self.x:_trainimg[start:end,:],self.t:_trainlabel[start:end,:]\n",
    "                           ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "            _trainLoss,_trainAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_train) \n",
    "            _nTemp = end-start; nTemp+=_nTemp\n",
    "            trainLoss+=(_nTemp*_trainLoss); trainAccr+=(_nTemp*_trainAccr)\n",
    "        trainLoss/=nTemp;trainAccr/=nTemp\n",
    "        # Compute validation loss and accuracy\n",
    "        maxIter4print = max(nVal//batchSize4print,1)\n",
    "        valLoss,valAccr,nTemp = 0,0,0\n",
    "        for iter in range(maxIter4print):\n",
    "            start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "            feeds_val = {self.x:_valimg[start:end,:],self.t:_vallabel[start:end,:]\n",
    "                         ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "            _valLoss,_valAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_val) \n",
    "            _nTemp = end-start; nTemp+=_nTemp\n",
    "            valLoss+=(_nTemp*_valLoss); valAccr+=(_nTemp*_valAccr)\n",
    "        valLoss/=nTemp;valAccr/=nTemp\n",
    "        # Compute test loss and accuracy\n",
    "        maxIter4print = max(nTest//batchSize4print,1)\n",
    "        testLoss,testAccr,nTemp = 0,0,0\n",
    "        for iter in range(maxIter4print):\n",
    "            start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "            feeds_test = {self.x:_testimg[start:end,:],self.t:_testlabel[start:end,:]\n",
    "                          ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "            _testLoss,_testAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_test) \n",
    "            _nTemp = end-start; nTemp+=_nTemp\n",
    "            testLoss+=(_nTemp*_testLoss); testAccr+=(_nTemp*_testAccr)\n",
    "        testLoss/=nTemp;testAccr/=nTemp\n",
    "        strTemp = ((\"[%s] [Loss] train:%.3f val:%.3f test:%.3f\"\n",
    "                    +\" [Accr] train:%.3f%% val:%.3f%% test:%.3f%%\")\n",
    "               %(self.name,trainLoss,valLoss,testLoss,trainAccr*100,valAccr*100,testAccr*100))\n",
    "        print(strTemp)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    print (\"choiceNet_cls_class defined.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ChoiceNet on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting\n",
    "def get_mnist_config():\n",
    "    trainimg,trainlabel,testimg,testlabel,valimg,vallabel \\\n",
    "        = load_mnist_with_noise(_errType='rp',_outlierRatio=0.9,_seed=0)\n",
    "    xdim,ydim,hdims,filterSizes,max_pools,feat_dim = [28,28,1],10,[64,64],[3,3],[2,2],256\n",
    "    kmix,actv,bn,VERBOSE = 10,tf.nn.relu,slim.batch_norm,True\n",
    "    rho_ref_train,tau_inv,pi1_bias,logSigmaZval = 0.95,1e-4,0.0,-2\n",
    "    logsumexp_coef,kl_reg_coef,l2_reg_coef = 1e-2,1e-4,1e-5\n",
    "    USE_INPUT_BN,USE_RESNET,USE_GAP,USE_KENDALL_LOSS = False,True,False,False\n",
    "    USE_MIXUP = False\n",
    "    return trainimg,trainlabel,testimg,testlabel,valimg,vallabel, \\\n",
    "        xdim,ydim,hdims,filterSizes,max_pools,feat_dim, \\\n",
    "        kmix,actv,bn,VERBOSE, \\\n",
    "        rho_ref_train,tau_inv,pi1_bias,logSigmaZval,logsumexp_coef,kl_reg_coef,l2_reg_coef, \\\n",
    "        USE_INPUT_BN,USE_RESNET,USE_GAP,USE_KENDALL_LOSS,USE_MIXUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "==== Global Variables ====\n",
      " [00] Name:[basic_choicenet_mnist/identity_0/weights:0] Shape:[[3, 3, 1, 64]]\n",
      " [01] Name:[basic_choicenet_mnist/identity_0/BatchNorm/beta:0] Shape:[[64]]\n",
      " [02] Name:[basic_choicenet_mnist/identity_0/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [03] Name:[basic_choicenet_mnist/identity_0/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [04] Name:[basic_choicenet_mnist/res_a_0/weights:0] Shape:[[3, 3, 1, 64]]\n",
      " [05] Name:[basic_choicenet_mnist/res_a_0/BatchNorm/beta:0] Shape:[[64]]\n",
      " [06] Name:[basic_choicenet_mnist/res_a_0/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [07] Name:[basic_choicenet_mnist/res_a_0/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [08] Name:[basic_choicenet_mnist/res_b_0/weights:0] Shape:[[3, 3, 64, 64]]\n",
      " [09] Name:[basic_choicenet_mnist/res_b_0/BatchNorm/beta:0] Shape:[[64]]\n",
      " [10] Name:[basic_choicenet_mnist/res_b_0/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [11] Name:[basic_choicenet_mnist/res_b_0/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [12] Name:[basic_choicenet_mnist/res_a_1/weights:0] Shape:[[3, 3, 64, 64]]\n",
      " [13] Name:[basic_choicenet_mnist/res_a_1/BatchNorm/beta:0] Shape:[[64]]\n",
      " [14] Name:[basic_choicenet_mnist/res_a_1/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [15] Name:[basic_choicenet_mnist/res_a_1/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [16] Name:[basic_choicenet_mnist/res_b_1/weights:0] Shape:[[3, 3, 64, 64]]\n",
      " [17] Name:[basic_choicenet_mnist/res_b_1/BatchNorm/beta:0] Shape:[[64]]\n",
      " [18] Name:[basic_choicenet_mnist/res_b_1/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [19] Name:[basic_choicenet_mnist/res_b_1/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [20] Name:[basic_choicenet_mnist/fc/weights:0] Shape:[[3136, 256]]\n",
      " [21] Name:[basic_choicenet_mnist/fc/BatchNorm/beta:0] Shape:[[256]]\n",
      " [22] Name:[basic_choicenet_mnist/fc/BatchNorm/moving_mean:0] Shape:[[256]]\n",
      " [23] Name:[basic_choicenet_mnist/fc/BatchNorm/moving_variance:0] Shape:[[256]]\n",
      " [24] Name:[basic_choicenet_mnist/rho_raw/weights:0] Shape:[[256, 10]]\n",
      " [25] Name:[basic_choicenet_mnist/rho_raw/BatchNorm/beta:0] Shape:[[10]]\n",
      " [26] Name:[basic_choicenet_mnist/rho_raw/BatchNorm/moving_mean:0] Shape:[[10]]\n",
      " [27] Name:[basic_choicenet_mnist/rho_raw/BatchNorm/moving_variance:0] Shape:[[10]]\n",
      " [28] Name:[basic_choicenet_mnist/muW:0] Shape:[[256, 10]]\n",
      " [29] Name:[basic_choicenet_mnist/logSigmaW:0] Shape:[[256, 10]]\n",
      " [30] Name:[basic_choicenet_mnist/var_raw/weights:0] Shape:[[256, 10]]\n",
      " [31] Name:[basic_choicenet_mnist/var_raw/BatchNorm/beta:0] Shape:[[10]]\n",
      " [32] Name:[basic_choicenet_mnist/var_raw/BatchNorm/moving_mean:0] Shape:[[10]]\n",
      " [33] Name:[basic_choicenet_mnist/var_raw/BatchNorm/moving_variance:0] Shape:[[10]]\n",
      " [34] Name:[basic_choicenet_mnist/pi_logits/weights:0] Shape:[[256, 10]]\n",
      " [35] Name:[basic_choicenet_mnist/pi_logits/BatchNorm/beta:0] Shape:[[10]]\n",
      " [36] Name:[basic_choicenet_mnist/pi_logits/BatchNorm/moving_mean:0] Shape:[[10]]\n",
      " [37] Name:[basic_choicenet_mnist/pi_logits/BatchNorm/moving_variance:0] Shape:[[10]]\n",
      " [38] Name:[basic_choicenet_mnist/identity_0/weights/Adam:0] Shape:[[3, 3, 1, 64]]\n",
      " [39] Name:[basic_choicenet_mnist/identity_0/weights/Adam_1:0] Shape:[[3, 3, 1, 64]]\n",
      " [40] Name:[basic_choicenet_mnist/identity_0/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [41] Name:[basic_choicenet_mnist/identity_0/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [42] Name:[basic_choicenet_mnist/res_a_0/weights/Adam:0] Shape:[[3, 3, 1, 64]]\n",
      " [43] Name:[basic_choicenet_mnist/res_a_0/weights/Adam_1:0] Shape:[[3, 3, 1, 64]]\n",
      " [44] Name:[basic_choicenet_mnist/res_a_0/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [45] Name:[basic_choicenet_mnist/res_a_0/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [46] Name:[basic_choicenet_mnist/res_b_0/weights/Adam:0] Shape:[[3, 3, 64, 64]]\n",
      " [47] Name:[basic_choicenet_mnist/res_b_0/weights/Adam_1:0] Shape:[[3, 3, 64, 64]]\n",
      " [48] Name:[basic_choicenet_mnist/res_b_0/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [49] Name:[basic_choicenet_mnist/res_b_0/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [50] Name:[basic_choicenet_mnist/res_a_1/weights/Adam:0] Shape:[[3, 3, 64, 64]]\n",
      " [51] Name:[basic_choicenet_mnist/res_a_1/weights/Adam_1:0] Shape:[[3, 3, 64, 64]]\n",
      " [52] Name:[basic_choicenet_mnist/res_a_1/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [53] Name:[basic_choicenet_mnist/res_a_1/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [54] Name:[basic_choicenet_mnist/res_b_1/weights/Adam:0] Shape:[[3, 3, 64, 64]]\n",
      " [55] Name:[basic_choicenet_mnist/res_b_1/weights/Adam_1:0] Shape:[[3, 3, 64, 64]]\n",
      " [56] Name:[basic_choicenet_mnist/res_b_1/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [57] Name:[basic_choicenet_mnist/res_b_1/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [58] Name:[basic_choicenet_mnist/fc/weights/Adam:0] Shape:[[3136, 256]]\n",
      " [59] Name:[basic_choicenet_mnist/fc/weights/Adam_1:0] Shape:[[3136, 256]]\n",
      " [60] Name:[basic_choicenet_mnist/fc/BatchNorm/beta/Adam:0] Shape:[[256]]\n",
      " [61] Name:[basic_choicenet_mnist/fc/BatchNorm/beta/Adam_1:0] Shape:[[256]]\n",
      " [62] Name:[basic_choicenet_mnist/rho_raw/weights/Adam:0] Shape:[[256, 10]]\n",
      " [63] Name:[basic_choicenet_mnist/rho_raw/weights/Adam_1:0] Shape:[[256, 10]]\n",
      " [64] Name:[basic_choicenet_mnist/rho_raw/BatchNorm/beta/Adam:0] Shape:[[10]]\n",
      " [65] Name:[basic_choicenet_mnist/rho_raw/BatchNorm/beta/Adam_1:0] Shape:[[10]]\n",
      " [66] Name:[basic_choicenet_mnist/muW/Adam:0] Shape:[[256, 10]]\n",
      " [67] Name:[basic_choicenet_mnist/muW/Adam_1:0] Shape:[[256, 10]]\n",
      " [68] Name:[basic_choicenet_mnist/logSigmaW/Adam:0] Shape:[[256, 10]]\n",
      " [69] Name:[basic_choicenet_mnist/logSigmaW/Adam_1:0] Shape:[[256, 10]]\n",
      " [70] Name:[basic_choicenet_mnist/var_raw/weights/Adam:0] Shape:[[256, 10]]\n",
      " [71] Name:[basic_choicenet_mnist/var_raw/weights/Adam_1:0] Shape:[[256, 10]]\n",
      " [72] Name:[basic_choicenet_mnist/var_raw/BatchNorm/beta/Adam:0] Shape:[[10]]\n",
      " [73] Name:[basic_choicenet_mnist/var_raw/BatchNorm/beta/Adam_1:0] Shape:[[10]]\n",
      " [74] Name:[basic_choicenet_mnist/pi_logits/weights/Adam:0] Shape:[[256, 10]]\n",
      " [75] Name:[basic_choicenet_mnist/pi_logits/weights/Adam_1:0] Shape:[[256, 10]]\n",
      " [76] Name:[basic_choicenet_mnist/pi_logits/BatchNorm/beta/Adam:0] Shape:[[10]]\n",
      " [77] Name:[basic_choicenet_mnist/pi_logits/BatchNorm/beta/Adam_1:0] Shape:[[10]]\n",
      "Layers:\n",
      "[00/10] x:0 (?, 784)\n",
      "[01/10] basic_choicenet_mnist/Reshape:0 (?, 28, 28, 1)\n",
      "[02/10] basic_choicenet_mnist/Relu:0 (?, 28, 28, 64)\n",
      "[03/10] basic_choicenet_mnist/Relu_1:0 (?, 28, 28, 64)\n",
      "[04/10] basic_choicenet_mnist/pool_0/MaxPool:0 (?, 14, 14, 64)\n",
      "[05/10] basic_choicenet_mnist/Relu_2:0 (?, 14, 14, 64)\n",
      "[06/10] basic_choicenet_mnist/Relu_3:0 (?, 14, 14, 64)\n",
      "[07/10] basic_choicenet_mnist/pool_1/MaxPool:0 (?, 7, 7, 64)\n",
      "[08/10] basic_choicenet_mnist/flatten/flatten/Reshape:0 (?, 3136)\n",
      "[09/10] basic_choicenet_mnist/fc/Relu:0 (?, 256)\n",
      "Text name: ../res/res_basic_choicenet_mnist.txt\n",
      "[../net/net_basic_choicenet_mnist.npz] Saved. Size is [16.2472]MB\n",
      "[00/50] [Loss] train:-0.110(f:-0.286+r:0.059+k:0.001+l:0.116) val:-0.240 test:-0.249 [Accr] train:45.8% val:75.4% test:76.4% maxVal:75.4% maxTest:76.4%\n",
      "[../net/net_basic_choicenet_mnist.npz] Saved. Size is [16.2326]MB\n",
      "[05/50] [Loss] train:-0.224(f:-0.413+r:0.073+k:0.001+l:0.115) val:-0.473 test:-0.476 [Accr] train:52.9% val:94.7% test:94.8% maxVal:94.7% maxTest:94.8%\n",
      "[../net/net_basic_choicenet_mnist.npz] Saved. Size is [16.2451]MB\n",
      "[10/50] [Loss] train:-0.256(f:-0.447+r:0.076+k:0.001+l:0.114) val:-0.538 test:-0.542 [Accr] train:53.9% val:97.2% test:97.0% maxVal:97.2% maxTest:97.0%\n",
      "[../net/net_basic_choicenet_mnist.npz] Saved. Size is [16.2482]MB\n",
      "[15/50] [Loss] train:-0.277(f:-0.467+r:0.076+k:0.001+l:0.113) val:-0.576 test:-0.576 [Accr] train:54.3% val:97.7% test:97.6% maxVal:97.7% maxTest:97.6%\n",
      "[../net/net_basic_choicenet_mnist.npz] Saved. Size is [16.2392]MB\n",
      "[20/50] [Loss] train:-0.292(f:-0.481+r:0.075+k:0.001+l:0.113) val:-0.599 test:-0.602 [Accr] train:54.4% val:98.1% test:97.9% maxVal:98.1% maxTest:97.9%\n",
      "[25/50] [Loss] train:-0.305(f:-0.491+r:0.073+k:0.001+l:0.112) val:-0.615 test:-0.613 [Accr] train:54.7% val:98.1% test:98.0% maxVal:98.1% maxTest:97.9%\n",
      "[30/50] [Loss] train:-0.312(f:-0.497+r:0.072+k:0.001+l:0.112) val:-0.620 test:-0.618 [Accr] train:54.8% val:98.1% test:98.0% maxVal:98.1% maxTest:97.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[../net/net_basic_choicenet_mnist.npz] Saved. Size is [16.2410]MB\n",
      "[35/50] [Loss] train:-0.319(f:-0.501+r:0.070+k:0.001+l:0.111) val:-0.627 test:-0.625 [Accr] train:54.9% val:98.2% test:97.9% maxVal:98.2% maxTest:97.9%\n",
      "[40/50] [Loss] train:-0.322(f:-0.503+r:0.069+k:0.001+l:0.111) val:-0.624 test:-0.623 [Accr] train:55.0% val:98.2% test:97.9% maxVal:98.2% maxTest:97.9%\n",
      "[45/50] [Loss] train:-0.324(f:-0.504+r:0.068+k:0.001+l:0.111) val:-0.626 test:-0.625 [Accr] train:55.0% val:98.1% test:97.9% maxVal:98.2% maxTest:97.9%\n",
      "[../net/net_basic_choicenet_mnist.npz] Saved. Size is [16.2456]MB\n",
      "[50/50] [Loss] train:-0.325(f:-0.505+r:0.068+k:0.001+l:0.111) val:-0.628 test:-0.627 [Accr] train:55.1% val:98.2% test:97.9% maxVal:98.2% maxTest:97.9%\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trainimg,trainlabel,testimg,testlabel,valimg,vallabel, \\\n",
    "    xdim,ydim,hdims,filterSizes,max_pools,feat_dim, \\\n",
    "    kmix,actv,bn,VERBOSE, \\\n",
    "    rho_ref_train,tau_inv,pi1_bias,logSigmaZval,logsumexp_coef,kl_reg_coef,l2_reg_coef, \\\n",
    "    USE_INPUT_BN,USE_RESNET,USE_GAP,USE_KENDALL_LOSS,USE_MIXUP = get_mnist_config()\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    CN = choiceNet_cls_class(_name='basic_choicenet_mnist'\n",
    "                          ,_xdim=xdim,_ydim=ydim,_hdims=hdims,_filterSizes=filterSizes\n",
    "                          ,_max_pools=max_pools,_feat_dim=feat_dim,_kmix=kmix,_actv=actv,_bn=slim.batch_norm\n",
    "                          ,_rho_ref_train=rho_ref_train,_tau_inv=tau_inv,_pi1_bias=pi1_bias,_logSigmaZval=logSigmaZval\n",
    "                          ,_logsumexp_coef=logsumexp_coef,_kl_reg_coef=kl_reg_coef,_l2_reg_coef=l2_reg_coef\n",
    "                          ,_USE_INPUT_BN=USE_INPUT_BN,_USE_RESNET=USE_RESNET,_USE_GAP=USE_GAP,_USE_KENDALL_LOSS=USE_KENDALL_LOSS\n",
    "                          ,_USE_MIXUP=USE_MIXUP,_GPU_ID=0,_VERBOSE=VERBOSE)\n",
    "    sess = gpusession(); sess.run(tf.global_variables_initializer()) \n",
    "    CN.train(_sess=sess,_trainimg=trainimg,_trainlabel=trainlabel\n",
    "               ,_testimg=testimg,_testlabel=testlabel,_valimg=valimg,_vallabel=vallabel\n",
    "               ,_maxEpoch=50,_batchSize=256,_lr=1e-5,_LR_SCHEDULE=True,_kp=0.95,_PRINT_EVERY=10,_SAVE_BEST=True)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore and Re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/t10k-labels-idx1-ubyte.gz\n",
      "[basic_choicenet_mnist] [Loss] train:-0.325 val:-0.625 test:-0.625 [Accr] train:55.042% val:98.199% test:97.862%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trainimg,trainlabel,testimg,testlabel,valimg,vallabel, \\\n",
    "    xdim,ydim,hdims,filterSizes,max_pools,feat_dim, \\\n",
    "    kmix,actv,bn,VERBOSE, \\\n",
    "    rho_ref_train,tau_inv,pi1_bias,logSigmaZval,logsumexp_coef,kl_reg_coef,l2_reg_coef, \\\n",
    "    USE_INPUT_BN,USE_RESNET,USE_GAP,USE_KENDALL_LOSS,USE_MIXUP = get_mnist_config()\n",
    "    tf.reset_default_graph(); tf.set_random_seed(0)\n",
    "    CN2 = choiceNet_cls_class(_name='basic_choicenet_mnist'\n",
    "                          ,_xdim=xdim,_ydim=ydim,_hdims=hdims,_filterSizes=filterSizes\n",
    "                          ,_max_pools=max_pools,_feat_dim=feat_dim,_kmix=kmix,_actv=actv,_bn=slim.batch_norm\n",
    "                          ,_rho_ref_train=rho_ref_train,_tau_inv=tau_inv,_pi1_bias=pi1_bias,_logSigmaZval=logSigmaZval\n",
    "                          ,_logsumexp_coef=logsumexp_coef,_kl_reg_coef=kl_reg_coef,_l2_reg_coef=l2_reg_coef\n",
    "                          ,_USE_INPUT_BN=USE_INPUT_BN,_USE_RESNET=USE_RESNET,_USE_GAP=USE_GAP,_USE_KENDALL_LOSS=USE_KENDALL_LOSS\n",
    "                          ,_USE_MIXUP=USE_MIXUP,_GPU_ID=0,_VERBOSE=False)\n",
    "    sess = gpusession(); sess.run(tf.global_variables_initializer()) \n",
    "    CN2.restore(sess) # Restore weights\n",
    "    CN2.test(sess,_trainimg=trainimg,_trainlabel=trainlabel\n",
    "             ,_testimg=testimg,_testlabel=testlabel,_valimg=valimg,_vallabel=vallabel)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
